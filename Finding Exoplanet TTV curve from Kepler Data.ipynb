{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b761c1c2-b4f7-4930-a115-e63a8b78f4de",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2420a-6c9d-4ed7-8c6b-03455d726020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "kepler_individ_lc.py\n",
    "\n",
    "Script to download, clean, and fit individual Kepler transits for a specified target,\n",
    "then create an O–C diagram using the Exotic Library.\n",
    "\n",
    "Important: NASA Exoplanet Archive typically provides T0 in BJD (~2454xxx),\n",
    "while Kepler data from lightkurve is in BKJD = BJD - 2454833.0.\n",
    "Hence, we subtract 2454833 from T0 before phase-folding and fitting.\n",
    "\"\"\"\n",
    "\n",
    "!pip install ultranest rebound\n",
    "!git clone https://github.com/rzellem/EXOTIC\n",
    "%cd EXOTIC\n",
    "!git checkout tess\n",
    "%cd ..\n",
    "import sys\n",
    "sys.path.append(\"F:/EXOTIC\")\n",
    "\n",
    "!git clone https://github.com/pearsonkyle/Nbody-AI\n",
    "sys.path.append('F:/Nbody-AI/nbody')\n",
    "\n",
    "sys.path.insert(0, 'F:/Nbody-AI')\n",
    "    \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "from pandas import read_csv\n",
    "from scipy.ndimage import binary_dilation, label\n",
    "from scipy.signal import savgol_filter, medfilt\n",
    "\n",
    "import lightkurve as lk\n",
    "from astropy import constants as const\n",
    "from astropy import units as u\n",
    "from wotan import flatten\n",
    "\n",
    "# EXOTIC imports (installed via \"pip install exotic\")\n",
    "from exotic.api.elca import lc_fitter\n",
    "from exotic.api.output_aavso import OutputFiles\n",
    "\n",
    "# For transit modeling\n",
    "from pylightcurve import exotethys\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Helper functions\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def tap_query(base_url, query, dataframe=True):\n",
    "    \"\"\"\n",
    "    Table Access Protocol (TAP) query to NASA Exoplanet Archive.\n",
    "    Builds a URL from 'base_url' and 'query', sends a GET request,\n",
    "    returns response as a DataFrame or raw text.\n",
    "    \"\"\"\n",
    "    uri_full = base_url\n",
    "    for k in query:\n",
    "        if k != \"format\":\n",
    "            uri_full += f\"{k} {query[k]} \"\n",
    "    uri_full = f\"{uri_full[:-1]} &format={query.get('format', 'csv')}\"\n",
    "    uri_full = uri_full.replace(' ', '+')\n",
    "    print(\"Query:\", uri_full)\n",
    "\n",
    "    response = requests.get(uri_full, timeout=300)\n",
    "    if dataframe:\n",
    "        return read_csv(StringIO(response.text))\n",
    "    else:\n",
    "        return response.text\n",
    "\n",
    "def nea_scrape(target=None):\n",
    "    \"\"\"\n",
    "    Use NASA Exoplanet Archive to fetch planet parameters for 'target'\n",
    "    (e.g., 'Kepler-18 b'). Returns a DataFrame with many columns\n",
    "    like pl_name, pl_orbper, st_rad, st_logg, etc.\n",
    "    \"\"\"\n",
    "    uri_ipac_base = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=\"\n",
    "    uri_ipac_query = {\n",
    "        \"select\": (\n",
    "            \"pl_name,hostname,pl_radj,pl_radjerr1,ra,dec,\"\n",
    "            \"pl_ratdor,pl_ratdorerr1,pl_ratdorerr2,pl_orbincl,pl_orbinclerr1,pl_orbinclerr2,\"\n",
    "            \"pl_orbper,pl_orbpererr1,pl_orbpererr2,pl_orbeccen,pl_orbsmax,pl_orbsmaxerr1,pl_orbsmaxerr2,\"\n",
    "            \"pl_orblper,pl_tranmid,pl_tranmiderr1,pl_tranmiderr2,\"\n",
    "            \"pl_ratror,pl_ratrorerr1,pl_ratrorerr2,\"\n",
    "            \"st_teff,st_tefferr1,st_tefferr2,st_met,st_meterr1,st_meterr2,\"\n",
    "            \"st_logg,st_loggerr1,st_loggerr2,st_mass,st_rad,st_raderr1\"\n",
    "        ),\n",
    "        \"from\": \"pscomppars\",\n",
    "        \"where\": \"tran_flag = 1\",\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    "    if target:\n",
    "        uri_ipac_query[\"where\"] += f\" and pl_name = '{target}'\"\n",
    "    return tap_query(uri_ipac_base, uri_ipac_query)\n",
    "\n",
    "def sigma_clip(ogdata, dt, iterations=1):\n",
    "    \"\"\"\n",
    "    Iterative sigma-clipping on 'ogdata' using a Savitzky-Golay filter\n",
    "    with window length 'dt' points. Outliers >3σ replaced by NaN.\n",
    "    \"\"\"\n",
    "    mask = np.ones(ogdata.shape, dtype=bool)\n",
    "    for _ in range(iterations):\n",
    "        mdata = savgol_filter(ogdata[mask], dt, 2)\n",
    "        res = ogdata[mask] - mdata\n",
    "        std = np.nanstd(res)\n",
    "        mask[mask] = np.abs(res) < 3*std\n",
    "\n",
    "    mdata = savgol_filter(ogdata[mask], dt, 2)\n",
    "    data = copy.deepcopy(ogdata)\n",
    "    data[~mask] = np.nan\n",
    "    return data, np.std(ogdata[mask] - mdata)\n",
    "\n",
    "def check_std(time, flux, dt=0.5):\n",
    "    \"\"\"\n",
    "    Sort by time, apply a Savitzky-Golay filter, return std of residuals.\n",
    "    dt in hours => affects smoothing window.\n",
    "    \"\"\"\n",
    "    tdt = np.diff(np.sort(time)).mean()\n",
    "    si = np.argsort(time)\n",
    "    # At least 15 points or so in the smoothing window\n",
    "    wsize = 1 + 2*int(max(15, dt/(24*tdt)))\n",
    "    sflux = savgol_filter(flux[si], wsize, 2)\n",
    "    return np.nanstd(flux - sflux)\n",
    "\n",
    "def stellar_mass(logg, rs):\n",
    "    \"\"\"\n",
    "    Estimate stellar mass (Msun) from logg (cgs) and radius (Rsun).\n",
    "    Msun ~ (R^2 * 10^logg / G).\n",
    "    \"\"\"\n",
    "    return ((rs*u.R_sun)**2 * 10**logg*(u.cm/u.s**2) / const.G).to(u.M_sun).value\n",
    "\n",
    "def sa(m, P):\n",
    "    \"\"\"\n",
    "    Semi-major axis (AU) from star mass m (Msun) and period P (days).\n",
    "    \"\"\"\n",
    "    return ((const.G*m*u.M_sun*P*u.day**2/(4*np.pi**2))**(1./3)).to(u.AU).value\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command-line arguments for analyzing Kepler data.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-t\", \"--target\", type=str, default=\"Kepler-1710 b\",\n",
    "                        help=\"Kepler target, e.g. 'Kepler-19 b'.\")\n",
    "    parser.add_argument(\"-o\", \"--output\", type=str, default=\"kepler_individ_lc_output\",\n",
    "                        help=\"Output directory for results.\")\n",
    "    parser.add_argument(\"--quarter\", type=int, default=0,\n",
    "                        help=\"Which Kepler quarter to process (0=all).\")\n",
    "    parser.add_argument(\"-r\", \"--reprocess\", action='store_true', default=False,\n",
    "                        help=\"Reprocess even if existing results found.\")\n",
    "    parser.add_argument(\"--ars\", action='store_true', default=False,\n",
    "                        help=\"Include a/R* in the fit bounds.\")\n",
    "    parser.add_argument(\"--tls\", action='store_true', default=False,\n",
    "                        help=\"Perform TLS search on final residuals (not fully used here).\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Main Script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    if not os.path.exists(args.output):\n",
    "        os.makedirs(args.output)\n",
    "\n",
    "    # Planet-specific directory\n",
    "    planetdir = os.path.join(args.output, args.target.replace(' ', '_').replace('-', '_'))\n",
    "    if not os.path.exists(planetdir):\n",
    "        os.mkdir(planetdir)\n",
    "    else:\n",
    "        # If there's a 'global_fit.png' and no --reprocess, skip\n",
    "        if os.path.exists(os.path.join(planetdir, \"global_fit.png\")) and not args.reprocess:\n",
    "            raise Exception(\"Target appears already processed. Use --reprocess to overwrite.\")\n",
    "\n",
    "    planetname = args.target.lower().replace(' ', '').replace('-', '')\n",
    "\n",
    "    # 1) Search for Kepler data\n",
    "    search_result = lk.search_targetpixelfile(args.target, mission='Kepler')\n",
    "    print(search_result)\n",
    "    if len(search_result) == 0:\n",
    "        raise Exception(f\"No Kepler data found for target: {args.target}\")\n",
    "\n",
    "    # 2) Load or scrape prior planet parameters\n",
    "    prior_path = os.path.join(planetdir, planetname + \"_prior.json\")\n",
    "    if os.path.exists(prior_path):\n",
    "        prior = json.load(open(prior_path, \"r\"))\n",
    "    else:\n",
    "        # Query NASA Exoplanet Archive\n",
    "        nea_df = nea_scrape(args.target)\n",
    "        if len(nea_df) == 0:\n",
    "            raise Exception(f\"No planet found in NASA Exoplanet Archive: {args.target}\")\n",
    "\n",
    "        prior = {}\n",
    "        for key in nea_df.columns:\n",
    "            prior[key] = nea_df[key].values[0]\n",
    "\n",
    "        # if pl_ratdor is NaN, compute from pl_orbsmax / st_rad\n",
    "        if np.isnan(prior.get('pl_ratdor', np.nan)) and not np.isnan(prior.get('pl_orbsmax', np.nan)):\n",
    "            prior['pl_ratdor'] = (\n",
    "                (prior['pl_orbsmax']*u.AU)/(prior['st_rad']*u.R_sun)\n",
    "            ).to(u.dimensionless_unscaled).value\n",
    "            prior['pl_ratdorerr1'] = 0.1 * prior['pl_ratdor']\n",
    "            prior['pl_ratdorerr2'] = -0.1 * prior['pl_ratdor']\n",
    "\n",
    "        # Save local prior\n",
    "        with open(prior_path, 'w', encoding='utf8') as jf:\n",
    "            json.dump(prior, jf, indent=4)\n",
    "\n",
    "    # 3) Limb darkening for Kepler band\n",
    "    #    If these are NaN, set defaults\n",
    "    st_logg = prior.get('st_logg', 4.4)\n",
    "    if np.isnan(st_logg): st_logg = 4.4\n",
    "    st_teff = prior.get('st_teff', 5700)\n",
    "    if np.isnan(st_teff): st_teff = 5700\n",
    "    st_met = prior.get('st_met', 0.0)\n",
    "    if np.isnan(st_met): st_met = 0.0\n",
    "\n",
    "    u0, u1, u2, u3 = exotethys(st_logg, st_teff, st_met,\n",
    "                               'Kepler', method='claret', stellar_model='phoenix')\n",
    "\n",
    "    # 4) Prepare structure to store data from each quarter\n",
    "    sv = {\n",
    "        'lightcurves': [],\n",
    "        'quarters': {},\n",
    "        'time': [],\n",
    "        'flux': [],\n",
    "        'flux_err': [],\n",
    "        'trend': [],\n",
    "        'quarter': []\n",
    "    }\n",
    "\n",
    "    # If there's a 'quarter' column in search_result, we can check it\n",
    "    if \"quarter\" in search_result.table.colnames:\n",
    "        unique_quarters = np.unique(search_result.table[\"quarter\"])\n",
    "    else:\n",
    "        # fallback: each row is \"quarterlike\"\n",
    "        unique_quarters = np.arange(len(search_result))\n",
    "\n",
    "    # If user wants only a specific quarter\n",
    "    if args.quarter != 0 and args.quarter not in unique_quarters:\n",
    "        raise Exception(f\"No data for quarter {args.quarter} in search_result.\")\n",
    "\n",
    "    # 5) Download & Clean each quarter\n",
    "    for idx in range(len(search_result)):\n",
    "\n",
    "        if \"quarter\" in search_result.table.colnames:\n",
    "            quarter = search_result.table[\"quarter\"][idx]\n",
    "            if args.quarter != 0 and quarter != args.quarter:\n",
    "                continue\n",
    "        else:\n",
    "            quarter = idx + 1  # fallback label\n",
    "\n",
    "        print(f\"Downloading Quarter {quarter} ...\")\n",
    "        try:\n",
    "            tpf = search_result[idx].download(quality_bitmask='hard')\n",
    "        except Exception as err:\n",
    "            print(f\"Failed to download quarter {quarter}: {err}\")\n",
    "            continue\n",
    "\n",
    "        # Aperture selection\n",
    "        lc = tpf.to_lightcurve(aperture_mask=tpf.pipeline_mask)\n",
    "        nmask = np.isnan(lc.flux.value)\n",
    "        lstd = check_std(lc.time.value[~nmask], lc.flux.value[~nmask])\n",
    "\n",
    "        aper_final = tpf.pipeline_mask\n",
    "        for it in [1, 2]:\n",
    "            bigger_mask = binary_dilation(tpf.pipeline_mask, iterations=it)\n",
    "            lcd = tpf.to_lightcurve(aperture_mask=bigger_mask)\n",
    "            nmaskd = np.isnan(lcd.flux.value)\n",
    "            lstdd = check_std(lcd.time.value[~nmaskd], lcd.flux.value[~nmaskd])\n",
    "            if lstdd < lstd:\n",
    "                aper_final = bigger_mask\n",
    "                lstd = lstdd\n",
    "\n",
    "        lc = tpf.to_lightcurve(aperture_mask=aper_final)\n",
    "        tpf.plot(aperture_mask=aper_final)\n",
    "        plt.savefig(os.path.join(planetdir, f\"{planetname}_quarter_{quarter}_aperture.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Remove first ~30 min after big gaps\n",
    "        time_arr = lc.time.value  # Kepler BKJD\n",
    "        flux_arr = lc.flux.value\n",
    "        tmask = np.ones_like(time_arr, dtype=bool)\n",
    "\n",
    "        srt = np.argsort(time_arr)\n",
    "        dt_ = np.diff(time_arr[srt])\n",
    "        if len(dt_) and not np.isnan(dt_).all():\n",
    "            median_dt = np.nanmedian(dt_)\n",
    "            # 30 min in days => 30./1440. => how many points\n",
    "            ndt = int(np.round((30./1440.) / median_dt))\n",
    "        else:\n",
    "            ndt = 30  # fallback\n",
    "\n",
    "        tmask[0:ndt] = False\n",
    "        big_gap = dt_ > (1./24.)  # 1 hour\n",
    "        biggap_idx = np.argwhere(big_gap).flatten()\n",
    "        for gap_idx in biggap_idx:\n",
    "            tmask[gap_idx:gap_idx+ndt] = False\n",
    "\n",
    "        nmask2 = ~np.isnan(flux_arr)\n",
    "        tmask = tmask & nmask2\n",
    "\n",
    "        time_c = time_arr[tmask]\n",
    "        flux_c = flux_arr[tmask]\n",
    "\n",
    "        # Remove outliers\n",
    "        mflux = medfilt(flux_c, kernel_size=15)\n",
    "        rflux = flux_c / mflux\n",
    "        newflux, std_ = sigma_clip(rflux, dt=15, iterations=1)\n",
    "        maskNaN = np.isnan(newflux)\n",
    "        time_c = time_c[~maskNaN]\n",
    "        flux_c = flux_c[~maskNaN]\n",
    "\n",
    "        # Flatten (remove stellar variability)\n",
    "        dflux = np.copy(flux_c)\n",
    "        dtrend = np.ones(len(time_c))\n",
    "\n",
    "        diff = np.diff(time_c)\n",
    "        if len(diff):\n",
    "            day_gaps = diff > 0.5\n",
    "            dmask = np.concatenate([[True], ~day_gaps])\n",
    "        else:\n",
    "            dmask = np.ones(len(time_c), dtype=bool)\n",
    "\n",
    "        seg_label, _ = label(dmask)\n",
    "        for seg_id in np.unique(seg_label):\n",
    "            if seg_id == 0:\n",
    "                continue\n",
    "            seg = (seg_label == seg_id)\n",
    "            if seg.sum() < 5:\n",
    "                continue\n",
    "            flc, tlc = flatten(time_c[seg], flux_c[seg],\n",
    "                               window_length=2.0, return_trend=True,\n",
    "                               method='biweight')\n",
    "            dflux[seg] = flc\n",
    "            dtrend[seg] = tlc\n",
    "\n",
    "        # Store in state\n",
    "        sv['quarters'][quarter] = True\n",
    "        sv['time'].append(time_c)\n",
    "        sv['flux'].append(dflux)\n",
    "        sv['flux_err'].append((dtrend**0.5)/np.nanmedian(dtrend))\n",
    "        sv['trend'].append(dtrend)\n",
    "        sv['quarter'].append(np.ones(len(time_c))*quarter)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure()\n",
    "        plt.plot(time_c, flux_c, 'k.', label='Flux')\n",
    "        plt.plot(time_c, dtrend, 'r--', label='Trend')\n",
    "        plt.title(f\"{args.target} - Quarter {quarter}\")\n",
    "        plt.xlabel(\"Time [BKJD]\")  # BKJD = BJD - 2454833\n",
    "        plt.ylabel(\"Flux\")\n",
    "        plt.legend()\n",
    "        outfig = os.path.join(planetdir, f\"{planetname}_quarter_{quarter}_trend.png\")\n",
    "        plt.savefig(outfig)\n",
    "        plt.close()\n",
    "\n",
    "    if not sv['time']:\n",
    "        raise Exception(\"No valid quarters processed. Check logs for errors.\")\n",
    "\n",
    "    # Concatenate all quarters\n",
    "    time_all = np.concatenate(sv['time'])\n",
    "    flux_all = np.concatenate(sv['flux'])\n",
    "    fluxerr_all = np.concatenate(sv['flux_err'])\n",
    "    trend_all = np.concatenate(sv['trend'])\n",
    "    quarter_all = np.concatenate(sv['quarter'])\n",
    "\n",
    "    # Remove nans\n",
    "    nanmask = np.isnan(flux_all) | np.isnan(fluxerr_all)\n",
    "    time_all = time_all[~nanmask]\n",
    "    flux_all = flux_all[~nanmask]\n",
    "    fluxerr_all = fluxerr_all[~nanmask]\n",
    "    quarter_all = quarter_all[~nanmask]\n",
    "    trend_all = trend_all[~nanmask]\n",
    "\n",
    "    # Save combined light curve to CSV\n",
    "    df = pd.DataFrame({\n",
    "        'time_bkjd': time_all,\n",
    "        'flux': flux_all * trend_all,\n",
    "        'flux_err': fluxerr_all * trend_all,\n",
    "        'quarter': quarter_all\n",
    "    })\n",
    "    outcsv = os.path.join(planetdir, f\"{planetname}_lightcurve.csv\")\n",
    "    df.to_csv(outcsv, index=False)\n",
    "    print(\"Saved combined lightcurve:\", outcsv)\n",
    "\n",
    "    # 6) Transit Fitting / O-C\n",
    "\n",
    "    # Convert T0 from BJD to BKJD by subtracting 2454833.0\n",
    "    # if we have a valid pl_tranmid\n",
    "    if not np.isnan(prior.get('pl_tranmid', np.nan)):\n",
    "        tmid_bjd = float(prior['pl_tranmid'])\n",
    "    else:\n",
    "        tmid_bjd = 2454950.0  # fallback guess\n",
    "\n",
    "    tmid_bkjd = tmid_bjd - 2454833.0  # *CRITICAL SHIFT*\n",
    "\n",
    "    # Orbital period\n",
    "    if not np.isnan(prior.get('pl_orbper', np.nan)):\n",
    "        period = float(prior['pl_orbper'])\n",
    "    else:\n",
    "        period = 10.0\n",
    "\n",
    "    # a/R*\n",
    "    if not np.isnan(prior.get('pl_ratdor', np.nan)):\n",
    "        ars = float(prior['pl_ratdor'])\n",
    "    else:\n",
    "        ars = 15.0\n",
    "\n",
    "    # rprs = (Rplanet/Rstar)\n",
    "    if not np.isnan(prior.get('pl_radj', np.nan)):\n",
    "        rprs = float((prior['pl_radj']*u.R_jup)/(prior['st_rad']*u.R_sun))\n",
    "    else:\n",
    "        rprs = 0.05\n",
    "\n",
    "    inc = prior.get('pl_orbincl', 88.0)\n",
    "    if np.isnan(inc): inc = 88.0\n",
    "\n",
    "    ecc = prior.get('pl_orbeccen', 0.0)\n",
    "    if np.isnan(ecc): ecc = 0.0\n",
    "\n",
    "    omega = prior.get('pl_orblper', 0.0)\n",
    "    if np.isnan(omega): omega = 0.0\n",
    "\n",
    "    tpars = {\n",
    "        'rprs': rprs,\n",
    "        'ars': ars,\n",
    "        'per': period,\n",
    "        'inc': inc,\n",
    "        'tmid': tmid_bkjd,\n",
    "        'omega': omega,\n",
    "        'ecc': ecc,\n",
    "        'a1': 0, 'a2': 0,\n",
    "        'u0': u0, 'u1': u1, 'u2': u2, 'u3': u3\n",
    "    }\n",
    "\n",
    "    # Quick flux_err guess\n",
    "    if len(flux_all):\n",
    "        phot_std = np.nanstd(flux_all[flux_all < 1.1])  # rough\n",
    "        fluxerr0 = phot_std / flux_all\n",
    "    else:\n",
    "        fluxerr0 = np.array([])\n",
    "\n",
    "    # Phase times\n",
    "    tphase = (time_all - tmid_bkjd)/period\n",
    "    # approximate transit duration\n",
    "    pdur = 2.0 * np.arctan(1./ars) / (2*np.pi) if ars > 0 else 0.05\n",
    "\n",
    "    # We'll define event epochs from floor(tphase)\n",
    "    events = np.unique(np.floor(tphase))\n",
    "\n",
    "    # Prepare arrays to store O-C\n",
    "    all_epochs = []\n",
    "    all_oc = []\n",
    "    all_oc_err = []\n",
    "\n",
    "    # Bounds\n",
    "    mybounds = {\n",
    "        'rprs': [0, 3*rprs],\n",
    "        'tmid': [tmid_bkjd - 0.3, tmid_bkjd + 0.3],\n",
    "        'inc': [70, 90]\n",
    "    }\n",
    "    if args.ars:\n",
    "        mybounds['ars'] = [ars*0.5, ars*2.0]\n",
    "\n",
    "    for e in events:\n",
    "        # mask within ~2 x pdur of the predicted transit\n",
    "        intrans = (tphase >= e - 2*pdur) & (tphase <= e + 2*pdur)\n",
    "        if intrans.sum() < 10:\n",
    "            continue\n",
    "\n",
    "        # update guess Tmid for that epoch\n",
    "        guess_tmid = tmid_bkjd + e*period\n",
    "        local_bounds = copy.deepcopy(mybounds)\n",
    "        # allow +/- 0.2 * (period*pdur) around guess\n",
    "        local_bounds['tmid'] = [guess_tmid - 0.2*period*pdur,\n",
    "                                guess_tmid + 0.2*period*pdur]\n",
    "        tpars['tmid'] = guess_tmid\n",
    "\n",
    "        airmass = np.zeros(intrans.sum())\n",
    "        try:\n",
    "            fit = lc_fitter(\n",
    "                time_all[intrans], flux_all[intrans],\n",
    "                fluxerr0[intrans],\n",
    "                airmass,\n",
    "                tpars,\n",
    "                local_bounds\n",
    "            )\n",
    "        except:\n",
    "            print(f\"Failed to fit transit at epoch {int(e)}.\")\n",
    "            continue\n",
    "\n",
    "        rprs2 = fit.parameters['rprs']**2\n",
    "        rprs2err = 2 * fit.parameters['rprs'] * fit.errors['rprs']\n",
    "        if (rprs2 - rprs2err <= 0):\n",
    "            print(f\"Skipping epoch {int(e)}: rprs^2 ~ 0 or negative.\")\n",
    "            continue\n",
    "\n",
    "        lcdata = {\n",
    "            'time': fit.time,           # BKJD\n",
    "            'flux': fit.data,\n",
    "            'residuals': fit.residuals,\n",
    "            'phase': fit.phase,\n",
    "            'pars': fit.parameters,\n",
    "            'errors': fit.errors,\n",
    "            'rchi2': fit.chi2/len(fit.time),\n",
    "            'epoch': int(e)\n",
    "        }\n",
    "        sv['lightcurves'].append(lcdata)\n",
    "\n",
    "        # Observed Tmid (in BKJD)\n",
    "        Tobs_bkjd = fit.parameters['tmid']\n",
    "        Tobs_err = fit.errors['tmid']\n",
    "\n",
    "        # Predicted Tmid for epoch e in BKJD\n",
    "        Tcalc_bkjd = tmid_bkjd + e*period\n",
    "        OC_bkjd = Tobs_bkjd - Tcalc_bkjd\n",
    "        OC_err = Tobs_err  # ignoring ephemeris uncertainty\n",
    "\n",
    "        all_epochs.append(e)\n",
    "        all_oc.append(OC_bkjd)\n",
    "        all_oc_err.append(OC_err)\n",
    "\n",
    "        # Plot bestfit\n",
    "        fig, ax = fit.plot_bestfit(title=f\"{args.target}, Quarter Fit, E={int(e)}\")\n",
    "        outname = f\"{planetname}_E{int(e)}_fit.png\"\n",
    "        plt.savefig(os.path.join(planetdir, outname))\n",
    "        plt.close()\n",
    "\n",
    "    # Dump pickled results\n",
    "    with open(os.path.join(planetdir, f\"{planetname}_data.pkl\"), 'wb') as pf:\n",
    "        pickle.dump(sv, pf)\n",
    "\n",
    "    # 7) O-C Diagram\n",
    "    all_epochs = np.array(all_epochs)\n",
    "    all_oc = np.array(all_oc)\n",
    "    all_oc_err = np.array(all_oc_err)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.errorbar(all_epochs, all_oc, yerr=all_oc_err, fmt='o',\n",
    "                 color='blue', ecolor='gray', capsize=3,\n",
    "                 label=\"O-C (BKJD)\")\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"O - C [days, BKJD]\")\n",
    "    plt.title(f\"O-C Diagram: {args.target}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    oc_plot_file = os.path.join(planetdir, f\"{planetname}_OC_diagram.png\")\n",
    "    plt.savefig(oc_plot_file)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Done! O-C diagram saved at:\", oc_plot_file)\n",
    "    print(\"Remember these times are in BKJD. (BJD - 2454833.0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc1dc9-4076-42a1-8848-eb02773bf8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "from astropy.timeseries import LombScargle\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "###############################################################################\n",
    "# LOMB–SCARGLE TTV ANALYSIS\n",
    "###############################################################################\n",
    "\n",
    "def run_lomb_scargle_TTV(all_epochs, all_oc, all_oc_err, show_plots=True):\n",
    "    \"\"\"\n",
    "    1) Convert O–C from days -> minutes\n",
    "    2) Filter points with O–C uncertainty < 5 min\n",
    "    3) (Optional) plot TTV curve vs epoch (now plotted in epochs with a 6th order polynomial fit)\n",
    "    4) Lomb–Scargle => find dominant & second-dominant peaks in \"epoch\" domain\n",
    "    5) Return amplitude_dom, dominant_period, power_dominant,\n",
    "             second_dominant_period, power_second_dominant, plus\n",
    "       also return the final mask for writing planet_name.csv\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to minutes\n",
    "    oc_min = all_oc * 24.0 * 60.0\n",
    "    oc_err_min = all_oc_err * 24.0 * 60.0\n",
    "\n",
    "    # Filter for uncertainties < 5 min (here threshold is 3 min as in original code)\n",
    "    mask = oc_err_min < 3.0\n",
    "    if np.sum(mask) == 0:\n",
    "        raise ValueError(\"No O–C points remain with uncertainties < 5 min!\")\n",
    "\n",
    "    epochs_f = all_epochs[mask]\n",
    "    ttv_f = oc_min[mask]\n",
    "    ttv_err_f = oc_err_min[mask]\n",
    "\n",
    "    # Optional: plot TTV curve in epochs with a 6th order polynomial fit\n",
    "    if show_plots:\n",
    "        plt.figure(figsize=(8,5))\n",
    "        # Plot the errorbar data (x-axis is epochs)\n",
    "        plt.errorbar(epochs_f, ttv_f, yerr=ttv_err_f, fmt='o', color='black', ecolor='gray', capsize=3)\n",
    "        \n",
    "        # Fit a 6th order polynomial to the TTV data\n",
    "        poly_coeffs = np.polyfit(epochs_f, ttv_f, 19)\n",
    "        # Create a smooth set of x-values over the range of epochs for plotting the curve\n",
    "        x_fit = np.linspace(np.min(epochs_f), np.max(epochs_f), 200)\n",
    "        y_fit = np.polyval(poly_coeffs, x_fit)\n",
    "        plt.plot(x_fit, y_fit, color='black', linewidth=2)\n",
    "        \n",
    "        plt.title(\"Figure 4a: Kepler-1710b Observed-Calculated (O-C) Plot\", fontsize=20, pad=20)\n",
    "        plt.xlabel(\"Epochs\", fontsize=18)\n",
    "        plt.ylabel(\"TTV Amplitude (minutes)\", fontsize=18)\n",
    "        plt.xticks(fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Mean-center TTV data for Lomb–Scargle\n",
    "    y_data = ttv_f - np.mean(ttv_f)\n",
    "\n",
    "    # Weighted Lomb–Scargle\n",
    "    ls = LombScargle(epochs_f, y_data, ttv_err_f)\n",
    "\n",
    "    # Frequency range => periods ~2..200 epochs\n",
    "    frequency, power = ls.autopower(\n",
    "        minimum_frequency=1.0/200.0,\n",
    "        maximum_frequency=0.5,\n",
    "        samples_per_peak=10\n",
    "    )\n",
    "    periods = 1.0 / frequency  # in epochs\n",
    "\n",
    "    # Identify top peaks\n",
    "    peaks, _ = find_peaks(power, height=0.01, distance=5)\n",
    "    if len(peaks) == 0:\n",
    "        # fallback: top 2 powers\n",
    "        sorted_indices = np.argsort(power)[::-1]\n",
    "        peaks = sorted_indices[:2]\n",
    "\n",
    "    peak_periods = periods[peaks]\n",
    "    peak_powers = power[peaks]\n",
    "\n",
    "    # Dominant & second-dominant\n",
    "    if len(peak_periods) == 1:\n",
    "        dominant_period = peak_periods[0]\n",
    "        power_dominant = peak_powers[0]\n",
    "        second_dominant_period = peak_periods[0]\n",
    "        power_second_dominant = peak_powers[0]\n",
    "    else:\n",
    "        sort_idx = np.argsort(peak_powers)[::-1]\n",
    "        dominant_period = peak_periods[sort_idx[0]]\n",
    "        power_dominant = peak_powers[sort_idx[0]]\n",
    "\n",
    "        second_dominant_period = None\n",
    "        power_second_dominant = None\n",
    "        for idx in sort_idx[1:]:\n",
    "            if abs(peak_periods[idx] - dominant_period) > 0.05 * dominant_period:\n",
    "                second_dominant_period = peak_periods[idx]\n",
    "                power_second_dominant = peak_powers[idx]\n",
    "                break\n",
    "        if second_dominant_period is None and len(sort_idx) > 1:\n",
    "            second_dominant_period = peak_periods[sort_idx[1]]\n",
    "            power_second_dominant = peak_powers[sort_idx[1]]\n",
    "        elif second_dominant_period is None:\n",
    "            second_dominant_period = dominant_period\n",
    "            power_second_dominant = power_dominant\n",
    "\n",
    "    # amplitude ~ 2 * sqrt(power * var(y_data)), in minutes\n",
    "    var_y = np.var(y_data)\n",
    "    amplitude_dom = 2.0 * np.sqrt(power_dominant * var_y) if power_dominant > 0 else 0.0\n",
    "    amplitude_2nd = 2.0 * np.sqrt(power_second_dominant * var_y) if power_second_dominant > 0 else 0.0\n",
    "\n",
    "    print(\"\\n===== LOMB-SCARGLE RESULTS =====\")\n",
    "    print(f\"Dominant Period (epochs)       = {dominant_period:.3f}\")\n",
    "    print(f\"Dominant Power                 = {power_dominant:.4f}\")\n",
    "    print(f\"Amplitude (Dominant) [min]     = {amplitude_dom:.3f}\")\n",
    "    print(f\"Second Dominant Period (epochs)= {second_dominant_period:.3f}\")\n",
    "    print(f\"Second Dominant Power          = {power_second_dominant:.4f}\")\n",
    "    print(f\"Amplitude (Second) [min]       = {amplitude_2nd:.3f}\")\n",
    "\n",
    "    # Optional FAP\n",
    "    try:\n",
    "        fap_val = ls.false_alarm_probability(power_dominant)\n",
    "        print(f\"FAP of top peak: {fap_val:.4g}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Optional periodogram plot: highlight the dominant (highest) peak\n",
    "    if show_plots:\n",
    "        plt.figure(figsize=(8,5))\n",
    "        # Plot periods directly in epochs (instead of converting to days)\n",
    "        plt.plot(periods, power)\n",
    "        # Zoom limit example: show up to 50 epochs\n",
    "        plt.xlim(0, 50)\n",
    "\n",
    "        # Plot only the highest peak\n",
    "        plt.scatter(\n",
    "            dominant_period, \n",
    "            power_dominant, \n",
    "            color='red', \n",
    "            zorder=3, \n",
    "            label=f\"Dominant Period ({dominant_period:.2f} epochs)\"\n",
    "        )\n",
    "\n",
    "        plt.xlabel(\"Epochs\", fontsize=18)\n",
    "        plt.ylabel(\"Power\", fontsize=18)\n",
    "        plt.title(\"Figure 4b: Kepler-1710b O-C Lomb-Scargle Periodogram\", fontsize=20, pad=20)\n",
    "        plt.xticks(fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        #plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Return main results + final mask\n",
    "    return (amplitude_dom, dominant_period, power_dominant,\n",
    "            second_dominant_period, power_second_dominant, mask)\n",
    "\n",
    "###############################################################################\n",
    "# NASA EXOPLANET ARCHIVE QUERY\n",
    "###############################################################################\n",
    "\n",
    "def tap_query(base_url, query):\n",
    "    uri_full = base_url\n",
    "    for k in query:\n",
    "        if k != \"format\":\n",
    "            uri_full += f\"{k} {query[k]} \"\n",
    "    uri_full = f\"{uri_full[:-1]} &format={query.get('format', 'csv')}\"\n",
    "    uri_full = uri_full.replace(' ', '+')\n",
    "    print(\"Query URL:\", uri_full)\n",
    "\n",
    "    response = requests.get(uri_full, timeout=60)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Query failed: status {response.status_code}\\n{response.text}\")\n",
    "\n",
    "    df = pd.read_csv(StringIO(response.text))\n",
    "    return df\n",
    "\n",
    "def query_inner_params(target_name):\n",
    "    \"\"\"\n",
    "    Query NASA Exoplanet Archive for:\n",
    "      - st_mass (Msun)\n",
    "      - pl_bmasse (Earth masses)\n",
    "      - pl_orbper (days)\n",
    "      - pl_orbeccen (eccentricity)\n",
    "      - pl_orblper (omega)\n",
    "      - pl_orbincl (inclination, deg)\n",
    "    Return the row with the most non-null among these columns.\n",
    "    \"\"\"\n",
    "    base_url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=\"\n",
    "    query_dict = {\n",
    "        \"select\": \"st_mass, pl_bmasse, pl_orbper, pl_orbeccen, pl_orblper, pl_orbincl\",\n",
    "        \"from\": \"ps\",\n",
    "        \"where\": f\"pl_name = '{target_name}'\",\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    "    df = tap_query(base_url, query_dict)\n",
    "    if len(df) == 0:\n",
    "        print(f\"No rows returned for {target_name}. Possibly unrecognized planet name.\")\n",
    "        return {}\n",
    "\n",
    "    cols = [\"st_mass\",\"pl_bmasse\",\"pl_orbper\",\"pl_orbeccen\",\"pl_orblper\",\"pl_orbincl\"]\n",
    "    best_idx = None\n",
    "    best_count = -1\n",
    "    for i in range(len(df)):\n",
    "        count_valid = df.iloc[i][cols].notna().sum()\n",
    "        if count_valid > best_count:\n",
    "            best_count = count_valid\n",
    "            best_idx = i\n",
    "    if best_idx is None:\n",
    "        return {}\n",
    "\n",
    "    row = df.iloc[best_idx]\n",
    "    out = {\n",
    "        \"st_mass_msun\":    row.get(\"st_mass\", np.nan),\n",
    "        \"pl_bmasse\":       row.get(\"pl_bmasse\", np.nan),\n",
    "        \"pl_orbper_days\":  row.get(\"pl_orbper\", np.nan),\n",
    "        \"pl_orbeccen\":     row.get(\"pl_orbeccen\", np.nan),\n",
    "        \"pl_orblper\":      row.get(\"pl_orblper\", np.nan),\n",
    "        \"pl_orbincl_deg\":  row.get(\"pl_orbincl\", np.nan)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "###############################################################################\n",
    "# APPEND RESULTS TO ocean.csv\n",
    "###############################################################################\n",
    "\n",
    "def write_to_ocean_csv(\n",
    "    params,\n",
    "    amplitude_dom,\n",
    "    dominant_period,\n",
    "    power_dominant,\n",
    "    second_dominant_period,\n",
    "    power_second_dominant,\n",
    "    filename=\"ocean.csv\"\n",
    "):\n",
    "    columns = [\n",
    "        \"Stellar Mass (Msun)\",\n",
    "        \"Inner Mass (Mearth)\",\n",
    "        \"Inner Period (days)\",\n",
    "        \"Inner Eccentricity\",\n",
    "        \"Inner Inclination\",\n",
    "        \"Inner Omega\",\n",
    "        \"Amplitude of Dominant Period Test (P1)\",\n",
    "        \"Dominant Period Planet 1\",\n",
    "        \"Dominant Period Power Planet 1\",\n",
    "        \"Second Dominant Period Planet 1\",\n",
    "        \"Second Dominant Period Power Planet 1\"\n",
    "    ]\n",
    "\n",
    "    new_row = {\n",
    "        \"Stellar Mass (Msun)\":   params.get(\"st_mass_msun\", np.nan),\n",
    "        \"Inner Mass (Mearth)\":   params.get(\"pl_bmasse\", np.nan),\n",
    "        \"Inner Period (days)\":   params.get(\"pl_orbper_days\", np.nan),\n",
    "        \"Inner Eccentricity\":    params.get(\"pl_orbeccen\", np.nan),\n",
    "        \"Inner Inclination\":     params.get(\"pl_orbincl_deg\", np.nan),\n",
    "        \"Inner Omega\":           params.get(\"pl_orblper\", np.nan),\n",
    "        \"Amplitude of Dominant Period Test (P1)\": amplitude_dom,\n",
    "        \"Dominant Period Planet 1\": dominant_period,\n",
    "        \"Dominant Period Power Planet 1\": power_dominant,\n",
    "        \"Second Dominant Period Planet 1\": second_dominant_period,\n",
    "        \"Second Dominant Period Power Planet 1\": power_second_dominant\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Created {filename} and wrote first row.\")\n",
    "    else:\n",
    "        df = pd.read_csv(filename)\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Appended a new row to existing {filename}.\")\n",
    "\n",
    "###############################################################################\n",
    "# WRITE planet_name.csv with O-C vs EPOCH (2 columns)\n",
    "###############################################################################\n",
    "\n",
    "def write_planet_csv(target_name, all_epochs, all_oc, all_oc_err):\n",
    "    \"\"\"\n",
    "    Creates a CSV named \"<target_name>.csv\" with columns:\n",
    "      Epoch, O-C (days)\n",
    "    for the points with uncertainties < 5 min.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to minutes\n",
    "    oc_min = all_oc * 24.0 * 60.0\n",
    "    oc_err_min = all_oc_err * 24.0 * 60.0\n",
    "\n",
    "    mask = oc_err_min < 15.0\n",
    "    if np.sum(mask) == 0:\n",
    "        raise ValueError(\"No O–C points remain with uncertainties < 5 min, so no CSV created.\")\n",
    "    # Filter\n",
    "    epochs_f = all_epochs[mask]\n",
    "    oc_f = all_oc[mask]  # still in days\n",
    "\n",
    "    # Build filename from planet name\n",
    "    planet_csv_filename = f\"E:/{target_name.replace(' ', '_').replace('-', '_')}.csv\"\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Epoch\": epochs_f,\n",
    "        \"O-C (days)\": oc_f\n",
    "    })\n",
    "\n",
    "    df.to_csv(planet_csv_filename, index=False)\n",
    "    print(f\"Wrote {len(df)} rows of O-C data to {planet_csv_filename}\")\n",
    "\n",
    "###############################################################################\n",
    "# EXAMPLE MAIN: use your real data here\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose your real data arrays are named exactly:\n",
    "    # all_epochs, all_oc, all_oc_err\n",
    "    # (They must be defined in the kernel or code prior to running.)\n",
    "\n",
    "    target_name = \"Kepler-1710 b\"  # or whichever planet name you want\n",
    "\n",
    "    # 1) Lomb–Scargle\n",
    "    (amplitude_dom,\n",
    "     dom_period,\n",
    "     power_dom,\n",
    "     sec_period,\n",
    "     power_sec,\n",
    "     final_mask) = run_lomb_scargle_TTV(all_epochs, all_oc, all_oc_err, show_plots=True)\n",
    "\n",
    "    # 2) Query NASA Exoplanet Archive\n",
    "    params = query_inner_params(target_name)\n",
    "\n",
    "    # 3) Append results to ocean.csv\n",
    "    write_to_ocean_csv(\n",
    "        params,\n",
    "        amplitude_dom,\n",
    "        dom_period,\n",
    "        power_dom,\n",
    "        sec_period,\n",
    "        power_sec,\n",
    "        filename=\"E:/ocean.csv\"\n",
    "    )\n",
    "\n",
    "    # 4) Also write planet_name.csv with \"Epoch\" and \"O-C (days)\" (filtered data)\n",
    "    write_planet_csv(target_name, all_epochs, all_oc, all_oc_err)\n",
    "\n",
    "    print(\"\\nAll tasks complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
