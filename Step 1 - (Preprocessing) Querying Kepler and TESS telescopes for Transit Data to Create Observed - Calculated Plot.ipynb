{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b761c1c2-b4f7-4930-a115-e63a8b78f4de",
   "metadata": {},
   "source": [
    "# Querying Kepler and TESS for Transit Data\n",
    "\n",
    "This notebook walks through the complete workflow of downloading Kepler (or TESS) data, cleaning light curves, fitting individual transits, and finally building an Observed minus Calculated (O\u2013C) diagram.\n",
    "\n",
    "Each code block below now contains extensive comments so that it can serve as a step-by-step training module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "d2fe786b-c021-4422-b9c9-20fd409dcace",
   "source": [
    "## 1. Environment Setup and Helper Functions\n\nThis section installs required packages, clones repositories, and defines utility functions used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2420a-6c9d-4ed7-8c6b-03455d726020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "kepler_individ_lc.py\n",
    "\n",
    "Script to download, clean, and fit individual Kepler transits for a specified target,\n",
    "then create an O\u2013C diagram. Notebooks or other scripts can be similarly adapted.\n",
    "\n",
    "Important: NASA Exoplanet Archive typically provides T0 in BJD (~2454xxx),\n",
    "while Kepler data from lightkurve is in BKJD = BJD - 2454833.0.\n",
    "Hence, we subtract 2454833 from T0 before phase-folding and fitting.\n",
    "\"\"\"\n",
    "\n",
    "# --- Environment setup: install and clone required packages ---\n",
    "!pip install ultranest rebound\n",
    "!git clone https://github.com/rzellem/EXOTIC\n",
    "%cd EXOTIC\n",
    "!git checkout tess\n",
    "%cd ..\n",
    "import sys\n",
    "sys.path.append(\"F:/EXOTIC\")\n",
    "\n",
    "!git clone https://github.com/pearsonkyle/Nbody-AI\n",
    "sys.path.append('F:/Nbody-AI/nbody')\n",
    "# Add the cloned repository to the Python path so we can import it\n",
    "sys.path.insert(0, 'F:/Nbody-AI')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "from pandas import read_csv\n",
    "from scipy.ndimage import binary_dilation, label\n",
    "from scipy.signal import savgol_filter, medfilt\n",
    "\n",
    "import lightkurve as lk\n",
    "from astropy import constants as const\n",
    "from astropy import units as u\n",
    "from wotan import flatten\n",
    "\n",
    "# EXOTIC imports (installed via \"pip install exotic\")\n",
    "from exotic.api.elca import lc_fitter\n",
    "from exotic.api.output_aavso import OutputFiles\n",
    "\n",
    "# For transit modeling\n",
    "from pylightcurve import exotethys\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Helper functions\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def tap_query(base_url, query, dataframe=True):\n",
    "    \"\"\"\n",
    "    Table Access Protocol (TAP) query to NASA Exoplanet Archive.\n",
    "    Builds a URL from 'base_url' and 'query', sends a GET request,\n",
    "    returns response as a DataFrame or raw text.\n",
    "    \"\"\"\n",
    "    uri_full = base_url\n",
    "    for k in query:\n",
    "        if k != \"format\":\n",
    "            uri_full += f\"{k} {query[k]} \"\n",
    "    uri_full = f\"{uri_full[:-1]} &format={query.get('format', 'csv')}\"\n",
    "    uri_full = uri_full.replace(' ', '+')\n",
    "    print(\"Query:\", uri_full)\n",
    "\n",
    "    response = requests.get(uri_full, timeout=300)\n",
    "    if dataframe:\n",
    "        return read_csv(StringIO(response.text))\n",
    "    else:\n",
    "        return response.text\n",
    "\n",
    "def nea_scrape(target=None):\n",
    "    \"\"\"\n",
    "    Use NASA Exoplanet Archive to fetch planet parameters for 'target'\n",
    "    (e.g., 'Kepler-18 b'). Returns a DataFrame with many columns\n",
    "    like pl_name, pl_orbper, st_rad, st_logg, etc.\n",
    "    \"\"\"\n",
    "    uri_ipac_base = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=\"\n",
    "    uri_ipac_query = {\n",
    "        \"select\": (\n",
    "            \"pl_name,hostname,pl_radj,pl_radjerr1,ra,dec,\"\n",
    "            \"pl_ratdor,pl_ratdorerr1,pl_ratdorerr2,pl_orbincl,pl_orbinclerr1,pl_orbinclerr2,\"\n",
    "            \"pl_orbper,pl_orbpererr1,pl_orbpererr2,pl_orbeccen,pl_orbsmax,pl_orbsmaxerr1,pl_orbsmaxerr2,\"\n",
    "            \"pl_orblper,pl_tranmid,pl_tranmiderr1,pl_tranmiderr2,\"\n",
    "            \"pl_ratror,pl_ratrorerr1,pl_ratrorerr2,\"\n",
    "            \"st_teff,st_tefferr1,st_tefferr2,st_met,st_meterr1,st_meterr2,\"\n",
    "            \"st_logg,st_loggerr1,st_loggerr2,st_mass,st_rad,st_raderr1\"\n",
    "        ),\n",
    "        \"from\": \"pscomppars\",\n",
    "        \"where\": \"tran_flag = 1\",\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    "    if target:\n",
    "        uri_ipac_query[\"where\"] += f\" and pl_name = '{target}'\"\n",
    "    return tap_query(uri_ipac_base, uri_ipac_query)\n",
    "\n",
    "def sigma_clip(ogdata, dt, iterations=1):\n",
    "    \"\"\"\n",
    "    Iterative sigma-clipping on 'ogdata' using a Savitzky-Golay filter\n",
    "    with window length 'dt' points. Outliers >3\u03c3 replaced by NaN.\n",
    "    \"\"\"\n",
    "    mask = np.ones(ogdata.shape, dtype=bool)\n",
    "    for _ in range(iterations):\n",
    "        mdata = savgol_filter(ogdata[mask], dt, 2)\n",
    "        res = ogdata[mask] - mdata\n",
    "        std = np.nanstd(res)\n",
    "        mask[mask] = np.abs(res) < 3*std\n",
    "\n",
    "    mdata = savgol_filter(ogdata[mask], dt, 2)\n",
    "    data = copy.deepcopy(ogdata)\n",
    "    data[~mask] = np.nan\n",
    "    return data, np.std(ogdata[mask] - mdata)\n",
    "\n",
    "def check_std(time, flux, dt=0.5):\n",
    "    \"\"\"\n",
    "    Sort by time, apply a Savitzky-Golay filter, return std of residuals.\n",
    "    dt in hours => affects smoothing window.\n",
    "    \"\"\"\n",
    "    tdt = np.diff(np.sort(time)).mean()\n",
    "    si = np.argsort(time)\n",
    "    # At least 15 points or so in the smoothing window\n",
    "    wsize = 1 + 2*int(max(15, dt/(24*tdt)))\n",
    "    sflux = savgol_filter(flux[si], wsize, 2)\n",
    "    return np.nanstd(flux - sflux)\n",
    "\n",
    "def stellar_mass(logg, rs):\n",
    "    \"\"\"\n",
    "    Estimate stellar mass (Msun) from logg (cgs) and radius (Rsun).\n",
    "    Msun ~ (R^2 * 10^logg / G).\n",
    "    \"\"\"\n",
    "    return ((rs*u.R_sun)**2 * 10**logg*(u.cm/u.s**2) / const.G).to(u.M_sun).value\n",
    "\n",
    "def sa(m, P):\n",
    "    \"\"\"\n",
    "    Semi-major axis (AU) from star mass m (Msun) and period P (days).\n",
    "    \"\"\"\n",
    "    return ((const.G*m*u.M_sun*P*u.day**2/(4*np.pi**2))**(1./3)).to(u.AU).value\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command-line arguments for analyzing Kepler data.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-t\", \"--target\", type=str, default=\"Kepler-1710 b\",\n",
    "                        help=\"Kepler target, e.g. 'Kepler-19 b'.\")\n",
    "    parser.add_argument(\"-o\", \"--output\", type=str, default=\"kepler_individ_lc_output\",\n",
    "                        help=\"Output directory for results.\")\n",
    "    parser.add_argument(\"--quarter\", type=int, default=0,\n",
    "                        help=\"Which Kepler quarter to process (0=all).\")\n",
    "    parser.add_argument(\"-r\", \"--reprocess\", action='store_true', default=False,\n",
    "                        help=\"Reprocess even if existing results found.\")\n",
    "    parser.add_argument(\"--ars\", action='store_true', default=False,\n",
    "                        help=\"Include a/R* in the fit bounds.\")\n",
    "    parser.add_argument(\"--tls\", action='store_true', default=False,\n",
    "                        help=\"Perform TLS search on final residuals (not fully used here).\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Main Script\n",
    "# ---------------------------------------------------------------------\n",
    "# The following block ties everything together:\n",
    "#   1. Search for public light curve data.\n",
    "#   2. Retrieve or estimate stellar/planet parameters.\n",
    "#   3. Clean and flatten each quarter of data.\n",
    "#   4. Fit individual transits and compute timing residuals.\n",
    "#   5. Produce an O--C diagram summarizing all epochs.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    if not os.path.exists(args.output):\n",
    "        os.makedirs(args.output)\n",
    "\n",
    "    # Planet-specific directory\n",
    "    planetdir = os.path.join(args.output, args.target.replace(' ', '_').replace('-', '_'))\n",
    "    if not os.path.exists(planetdir):\n",
    "        os.mkdir(planetdir)\n",
    "    else:\n",
    "        # If there's a 'global_fit.png' and no --reprocess, skip\n",
    "        if os.path.exists(os.path.join(planetdir, \"global_fit.png\")) and not args.reprocess:\n",
    "            raise Exception(\"Target appears already processed. Use --reprocess to overwrite.\")\n",
    "\n",
    "    planetname = args.target.lower().replace(' ', '').replace('-', '')\n",
    "\n",
    "    # 1) Search for Kepler data\n",
    "    search_result = lk.search_targetpixelfile(args.target, mission='Kepler')\n",
    "    print(search_result)\n",
    "    if len(search_result) == 0:\n",
    "        raise Exception(f\"No Kepler data found for target: {args.target}\")\n",
    "\n",
    "    # 2) Load or scrape prior planet parameters\n",
    "    prior_path = os.path.join(planetdir, planetname + \"_prior.json\")\n",
    "    if os.path.exists(prior_path):\n",
    "        prior = json.load(open(prior_path, \"r\"))\n",
    "    else:\n",
    "        # Query NASA Exoplanet Archive\n",
    "        nea_df = nea_scrape(args.target)\n",
    "        if len(nea_df) == 0:\n",
    "            raise Exception(f\"No planet found in NASA Exoplanet Archive: {args.target}\")\n",
    "\n",
    "        prior = {}\n",
    "        for key in nea_df.columns:\n",
    "            prior[key] = nea_df[key].values[0]\n",
    "\n",
    "        # if pl_ratdor is NaN, compute from pl_orbsmax / st_rad\n",
    "        if np.isnan(prior.get('pl_ratdor', np.nan)) and not np.isnan(prior.get('pl_orbsmax', np.nan)):\n",
    "            prior['pl_ratdor'] = (\n",
    "                (prior['pl_orbsmax']*u.AU)/(prior['st_rad']*u.R_sun)\n",
    "            ).to(u.dimensionless_unscaled).value\n",
    "            prior['pl_ratdorerr1'] = 0.1 * prior['pl_ratdor']\n",
    "            prior['pl_ratdorerr2'] = -0.1 * prior['pl_ratdor']\n",
    "\n",
    "        # Save local prior\n",
    "        with open(prior_path, 'w', encoding='utf8') as jf:\n",
    "            json.dump(prior, jf, indent=4)\n",
    "\n",
    "    # 3) Limb darkening for Kepler band\n",
    "    #    If these are NaN, set defaults\n",
    "    st_logg = prior.get('st_logg', 4.4)\n",
    "    if np.isnan(st_logg): st_logg = 4.4\n",
    "    st_teff = prior.get('st_teff', 5700)\n",
    "    if np.isnan(st_teff): st_teff = 5700\n",
    "    st_met = prior.get('st_met', 0.0)\n",
    "    if np.isnan(st_met): st_met = 0.0\n",
    "\n",
    "    u0, u1, u2, u3 = exotethys(st_logg, st_teff, st_met,\n",
    "                               'Kepler', method='claret', stellar_model='phoenix')\n",
    "\n",
    "    # 4) Prepare structure to store data from each quarter\n",
    "    sv = {\n",
    "        'lightcurves': [],\n",
    "        'quarters': {},\n",
    "        'time': [],\n",
    "        'flux': [],\n",
    "        'flux_err': [],\n",
    "        'trend': [],\n",
    "        'quarter': []\n",
    "    }\n",
    "\n",
    "    # If there's a 'quarter' column in search_result, we can check it\n",
    "    if \"quarter\" in search_result.table.colnames:\n",
    "        unique_quarters = np.unique(search_result.table[\"quarter\"])\n",
    "    else:\n",
    "        # fallback: each row is \"quarterlike\"\n",
    "        unique_quarters = np.arange(len(search_result))\n",
    "\n",
    "    # If user wants only a specific quarter\n",
    "    if args.quarter != 0 and args.quarter not in unique_quarters:\n",
    "        raise Exception(f\"No data for quarter {args.quarter} in search_result.\")\n",
    "\n",
    "    # 5) Download & Clean each quarter\n",
    "    for idx in range(len(search_result)):\n",
    "\n",
    "        if \"quarter\" in search_result.table.colnames:\n",
    "            quarter = search_result.table[\"quarter\"][idx]\n",
    "            if args.quarter != 0 and quarter != args.quarter:\n",
    "                continue\n",
    "        else:\n",
    "            quarter = idx + 1  # fallback label\n",
    "\n",
    "        print(f\"Downloading Quarter {quarter} ...\")\n",
    "        try:\n",
    "            tpf = search_result[idx].download(quality_bitmask='hard')\n",
    "        except Exception as err:\n",
    "            print(f\"Failed to download quarter {quarter}: {err}\")\n",
    "            continue\n",
    "\n",
    "        # Aperture selection\n",
    "        lc = tpf.to_lightcurve(aperture_mask=tpf.pipeline_mask)\n",
    "        nmask = np.isnan(lc.flux.value)\n",
    "        lstd = check_std(lc.time.value[~nmask], lc.flux.value[~nmask])\n",
    "\n",
    "        aper_final = tpf.pipeline_mask\n",
    "        for it in [1, 2]:\n",
    "            bigger_mask = binary_dilation(tpf.pipeline_mask, iterations=it)\n",
    "            lcd = tpf.to_lightcurve(aperture_mask=bigger_mask)\n",
    "            nmaskd = np.isnan(lcd.flux.value)\n",
    "            lstdd = check_std(lcd.time.value[~nmaskd], lcd.flux.value[~nmaskd])\n",
    "            if lstdd < lstd:\n",
    "                aper_final = bigger_mask\n",
    "                lstd = lstdd\n",
    "\n",
    "        lc = tpf.to_lightcurve(aperture_mask=aper_final)\n",
    "        tpf.plot(aperture_mask=aper_final)\n",
    "        plt.savefig(os.path.join(planetdir, f\"{planetname}_quarter_{quarter}_aperture.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Remove first ~30 min after big gaps\n",
    "        time_arr = lc.time.value  # Kepler BKJD\n",
    "        flux_arr = lc.flux.value\n",
    "        tmask = np.ones_like(time_arr, dtype=bool)\n",
    "\n",
    "        srt = np.argsort(time_arr)\n",
    "        dt_ = np.diff(time_arr[srt])\n",
    "        if len(dt_) and not np.isnan(dt_).all():\n",
    "            median_dt = np.nanmedian(dt_)\n",
    "            # 30 min in days => 30./1440. => how many points\n",
    "            ndt = int(np.round((30./1440.) / median_dt))\n",
    "        else:\n",
    "            ndt = 30  # fallback\n",
    "\n",
    "        tmask[0:ndt] = False\n",
    "        big_gap = dt_ > (1./24.)  # 1 hour\n",
    "        biggap_idx = np.argwhere(big_gap).flatten()\n",
    "        for gap_idx in biggap_idx:\n",
    "            tmask[gap_idx:gap_idx+ndt] = False\n",
    "\n",
    "        nmask2 = ~np.isnan(flux_arr)\n",
    "        tmask = tmask & nmask2\n",
    "\n",
    "        time_c = time_arr[tmask]\n",
    "        flux_c = flux_arr[tmask]\n",
    "\n",
    "        # Remove outliers\n",
    "        mflux = medfilt(flux_c, kernel_size=15)\n",
    "        rflux = flux_c / mflux\n",
    "        newflux, std_ = sigma_clip(rflux, dt=15, iterations=1)\n",
    "        maskNaN = np.isnan(newflux)\n",
    "        time_c = time_c[~maskNaN]\n",
    "        flux_c = flux_c[~maskNaN]\n",
    "\n",
    "        # Flatten (remove stellar variability)\n",
    "        dflux = np.copy(flux_c)\n",
    "        dtrend = np.ones(len(time_c))\n",
    "\n",
    "        diff = np.diff(time_c)\n",
    "        if len(diff):\n",
    "            day_gaps = diff > 0.5\n",
    "            dmask = np.concatenate([[True], ~day_gaps])\n",
    "        else:\n",
    "            dmask = np.ones(len(time_c), dtype=bool)\n",
    "\n",
    "        seg_label, _ = label(dmask)\n",
    "        for seg_id in np.unique(seg_label):\n",
    "            if seg_id == 0:\n",
    "                continue\n",
    "            seg = (seg_label == seg_id)\n",
    "            if seg.sum() < 5:\n",
    "                continue\n",
    "            flc, tlc = flatten(time_c[seg], flux_c[seg],\n",
    "                               window_length=2.0, return_trend=True,\n",
    "                               method='biweight')\n",
    "            dflux[seg] = flc\n",
    "            dtrend[seg] = tlc\n",
    "\n",
    "        # Store in state\n",
    "        sv['quarters'][quarter] = True\n",
    "        sv['time'].append(time_c)\n",
    "        sv['flux'].append(dflux)\n",
    "        sv['flux_err'].append((dtrend**0.5)/np.nanmedian(dtrend))\n",
    "        sv['trend'].append(dtrend)\n",
    "        sv['quarter'].append(np.ones(len(time_c))*quarter)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure()\n",
    "        plt.plot(time_c, flux_c, 'k.', label='Flux')\n",
    "        plt.plot(time_c, dtrend, 'r--', label='Trend')\n",
    "        plt.title(f\"{args.target} - Quarter {quarter}\")\n",
    "        plt.xlabel(\"Time [BKJD]\")  # BKJD = BJD - 2454833\n",
    "        plt.ylabel(\"Flux\")\n",
    "        plt.legend()\n",
    "        outfig = os.path.join(planetdir, f\"{planetname}_quarter_{quarter}_trend.png\")\n",
    "        plt.savefig(outfig)\n",
    "        plt.close()\n",
    "\n",
    "    if not sv['time']:\n",
    "        raise Exception(\"No valid quarters processed. Check logs for errors.\")\n",
    "\n",
    "    # Concatenate all quarters\n",
    "    time_all = np.concatenate(sv['time'])\n",
    "    flux_all = np.concatenate(sv['flux'])\n",
    "    fluxerr_all = np.concatenate(sv['flux_err'])\n",
    "    trend_all = np.concatenate(sv['trend'])\n",
    "    quarter_all = np.concatenate(sv['quarter'])\n",
    "\n",
    "    # Remove nans\n",
    "    nanmask = np.isnan(flux_all) | np.isnan(fluxerr_all)\n",
    "    time_all = time_all[~nanmask]\n",
    "    flux_all = flux_all[~nanmask]\n",
    "    fluxerr_all = fluxerr_all[~nanmask]\n",
    "    quarter_all = quarter_all[~nanmask]\n",
    "    trend_all = trend_all[~nanmask]\n",
    "\n",
    "    # Save combined light curve to CSV\n",
    "    df = pd.DataFrame({\n",
    "        'time_bkjd': time_all,\n",
    "        'flux': flux_all * trend_all,\n",
    "        'flux_err': fluxerr_all * trend_all,\n",
    "        'quarter': quarter_all\n",
    "    })\n",
    "    outcsv = os.path.join(planetdir, f\"{planetname}_lightcurve.csv\")\n",
    "    df.to_csv(outcsv, index=False)\n",
    "    print(\"Saved combined lightcurve:\", outcsv)\n",
    "\n",
    "    # 6) Transit Fitting / O-C\n",
    "\n",
    "    # Convert T0 from BJD to BKJD by subtracting 2454833.0\n",
    "    # if we have a valid pl_tranmid\n",
    "    if not np.isnan(prior.get('pl_tranmid', np.nan)):\n",
    "        tmid_bjd = float(prior['pl_tranmid'])\n",
    "    else:\n",
    "        tmid_bjd = 2454950.0  # fallback guess\n",
    "\n",
    "    tmid_bkjd = tmid_bjd - 2454833.0  # *CRITICAL SHIFT*\n",
    "\n",
    "    # Orbital period\n",
    "    if not np.isnan(prior.get('pl_orbper', np.nan)):\n",
    "        period = float(prior['pl_orbper'])\n",
    "    else:\n",
    "        period = 10.0\n",
    "\n",
    "    # a/R*\n",
    "    if not np.isnan(prior.get('pl_ratdor', np.nan)):\n",
    "        ars = float(prior['pl_ratdor'])\n",
    "    else:\n",
    "        ars = 15.0\n",
    "\n",
    "    # rprs = (Rplanet/Rstar)\n",
    "    if not np.isnan(prior.get('pl_radj', np.nan)):\n",
    "        rprs = float((prior['pl_radj']*u.R_jup)/(prior['st_rad']*u.R_sun))\n",
    "    else:\n",
    "        rprs = 0.05\n",
    "\n",
    "    inc = prior.get('pl_orbincl', 88.0)\n",
    "    if np.isnan(inc): inc = 88.0\n",
    "\n",
    "    ecc = prior.get('pl_orbeccen', 0.0)\n",
    "    if np.isnan(ecc): ecc = 0.0\n",
    "\n",
    "    omega = prior.get('pl_orblper', 0.0)\n",
    "    if np.isnan(omega): omega = 0.0\n",
    "\n",
    "    tpars = {\n",
    "        'rprs': rprs,\n",
    "        'ars': ars,\n",
    "        'per': period,\n",
    "        'inc': inc,\n",
    "        'tmid': tmid_bkjd,\n",
    "        'omega': omega,\n",
    "        'ecc': ecc,\n",
    "        'a1': 0, 'a2': 0,\n",
    "        'u0': u0, 'u1': u1, 'u2': u2, 'u3': u3\n",
    "    }\n",
    "\n",
    "    # Quick flux_err guess\n",
    "    if len(flux_all):\n",
    "        phot_std = np.nanstd(flux_all[flux_all < 1.1])  # rough\n",
    "        fluxerr0 = phot_std / flux_all\n",
    "    else:\n",
    "        fluxerr0 = np.array([])\n",
    "\n",
    "    # Phase times\n",
    "    tphase = (time_all - tmid_bkjd)/period\n",
    "    # approximate transit duration\n",
    "    pdur = 2.0 * np.arctan(1./ars) / (2*np.pi) if ars > 0 else 0.05\n",
    "\n",
    "    # We'll define event epochs from floor(tphase)\n",
    "    events = np.unique(np.floor(tphase))\n",
    "\n",
    "    # Prepare arrays to store O-C\n",
    "    all_epochs = []\n",
    "    all_oc = []\n",
    "    all_oc_err = []\n",
    "\n",
    "    # Bounds\n",
    "    mybounds = {\n",
    "        'rprs': [0, 3*rprs],\n",
    "        'tmid': [tmid_bkjd - 0.3, tmid_bkjd + 0.3],\n",
    "        'inc': [70, 90]\n",
    "    }\n",
    "    if args.ars:\n",
    "        mybounds['ars'] = [ars*0.5, ars*2.0]\n",
    "\n",
    "    # Iterate through each expected transit epoch and\n",
    "    # fit a local light curve model to refine the mid-transit time.\n",
    "    for e in events:\n",
    "        # mask within ~2 x pdur of the predicted transit\n",
    "        intrans = (tphase >= e - 2*pdur) & (tphase <= e + 2*pdur)\n",
    "        if intrans.sum() < 10:\n",
    "            continue\n",
    "\n",
    "        # update guess Tmid for that epoch\n",
    "        guess_tmid = tmid_bkjd + e*period\n",
    "        local_bounds = copy.deepcopy(mybounds)\n",
    "        # allow +/- 0.2 * (period*pdur) around guess\n",
    "        local_bounds['tmid'] = [guess_tmid - 0.2*period*pdur,\n",
    "                                guess_tmid + 0.2*period*pdur]\n",
    "        tpars['tmid'] = guess_tmid\n",
    "\n",
    "        airmass = np.zeros(intrans.sum())\n",
    "        try:\n",
    "            fit = lc_fitter(\n",
    "                time_all[intrans], flux_all[intrans],\n",
    "                fluxerr0[intrans],\n",
    "                airmass,\n",
    "                tpars,\n",
    "                local_bounds\n",
    "            )\n",
    "        except:\n",
    "            print(f\"Failed to fit transit at epoch {int(e)}.\")\n",
    "            continue\n",
    "\n",
    "        rprs2 = fit.parameters['rprs']**2\n",
    "        rprs2err = 2 * fit.parameters['rprs'] * fit.errors['rprs']\n",
    "        if (rprs2 - rprs2err <= 0):\n",
    "            print(f\"Skipping epoch {int(e)}: rprs^2 ~ 0 or negative.\")\n",
    "            continue\n",
    "\n",
    "        lcdata = {\n",
    "            'time': fit.time,           # BKJD\n",
    "            'flux': fit.data,\n",
    "            'residuals': fit.residuals,\n",
    "            'phase': fit.phase,\n",
    "            'pars': fit.parameters,\n",
    "            'errors': fit.errors,\n",
    "            'rchi2': fit.chi2/len(fit.time),\n",
    "            'epoch': int(e)\n",
    "        }\n",
    "        sv['lightcurves'].append(lcdata)\n",
    "\n",
    "        # Observed Tmid (in BKJD)\n",
    "        Tobs_bkjd = fit.parameters['tmid']\n",
    "        Tobs_err = fit.errors['tmid']\n",
    "\n",
    "        # Predicted Tmid for epoch e in BKJD\n",
    "        Tcalc_bkjd = tmid_bkjd + e*period\n",
    "        OC_bkjd = Tobs_bkjd - Tcalc_bkjd\n",
    "        OC_err = Tobs_err  # ignoring ephemeris uncertainty\n",
    "\n",
    "        all_epochs.append(e)\n",
    "        all_oc.append(OC_bkjd)\n",
    "        all_oc_err.append(OC_err)\n",
    "\n",
    "        # Plot bestfit\n",
    "        fig, ax = fit.plot_bestfit(title=f\"{args.target}, Quarter Fit, E={int(e)}\")\n",
    "        outname = f\"{planetname}_E{int(e)}_fit.png\"\n",
    "        plt.savefig(os.path.join(planetdir, outname))\n",
    "        plt.close()\n",
    "\n",
    "    # Dump pickled results\n",
    "    with open(os.path.join(planetdir, f\"{planetname}_data.pkl\"), 'wb') as pf:\n",
    "        pickle.dump(sv, pf)\n",
    "\n",
    "    # 7) O-C Diagram\n",
    "    # Compile all of the measured transit times and compare them\n",
    "    # to the predictions from the ephemeris. The result is a classic\n",
    "    # Observed minus Calculated (O-C) plot showing timing variations.\n",
    "    all_epochs = np.array(all_epochs)\n",
    "    all_oc = np.array(all_oc)\n",
    "    all_oc_err = np.array(all_oc_err)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.errorbar(all_epochs, all_oc, yerr=all_oc_err, fmt='o',\n",
    "                 color='blue', ecolor='gray', capsize=3,\n",
    "                 label=\"O-C (BKJD)\")\n",
    "    plt.axhline(0, color='k', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"O - C [days, BKJD]\")\n",
    "    plt.title(f\"O-C Diagram: {args.target}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    oc_plot_file = os.path.join(planetdir, f\"{planetname}_OC_diagram.png\")\n",
    "    plt.savefig(oc_plot_file)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Done! O-C diagram saved at:\", oc_plot_file)\n",
    "    print(\"Remember these times are in BKJD. (BJD - 2454833.0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "f4ce29c3-e336-4ded-85fa-aa17235de91c",
   "source": [
    "## 2. Lomb\u2013Scargle Transit Timing Analysis\n\nHere we implement an iterative Lomb\u2013Scargle procedure to identify periodic signals in transit timing variations and helper routines for querying the NASA Exoplanet Archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f706ec-7100-4539-8185-4eab97ea34d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "from astropy.timeseries import LombScargle\n",
    "\n",
    "# Set plot tick label sizes for consistency\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "\n",
    "###############################################################################\n",
    "# LOMB\u2013SCARGLE TTV ANALYSIS (Iterative Removal of Dominant Periods)\n",
    "###############################################################################\n",
    "# This cell contains a helper function that applies a weighted\n",
    "# Lomb--Scargle periodogram multiple times, removing the strongest\n",
    "# signal after each iteration. The goal is to identify periodic\n",
    "# signatures in the transit timing variations.\n",
    "\n",
    "def run_lomb_scargle_TTV(all_epochs, all_oc, all_oc_err, period, show_plots=True):\n",
    "    \"\"\"\n",
    "    Tutorial function: given arrays of epochs and O--C values this routine\n",
    "    demonstrates how to clean the timing data and perform an iterative\n",
    "    Lomb--Scargle period search.\n",
    "    1) Converts O\u2013C from days to minutes.\n",
    "    2) Filters points with O\u2013C uncertainty < 3 min.\n",
    "    3) Optionally plots the TTV curve (with an 8th-order polynomial fit) vs shifted epoch.\n",
    "    4) Runs weighted Lomb\u2013Scargle iteratively:\n",
    "         - In each iteration, considers only periods < 40 epochs.\n",
    "         - Finds the dominant peak, computes its amplitude and FAP.\n",
    "         - Subtracts the best-fit sinusoid from the current signal.\n",
    "         - Repeats for a total of 10 iterations.\n",
    "    5) Returns a list of results (one per iteration) and the final mask used.\n",
    "    \"\"\"\n",
    "    # Convert O\u2013C from days to minutes\n",
    "    oc_min = all_oc * 24.0 * 60.0\n",
    "    oc_err_min = all_oc_err * 24.0 * 60.0\n",
    "\n",
    "    # Filter for uncertainties less than 3 minutes\n",
    "    mask = oc_err_min < 10\n",
    "    if np.sum(mask) == 0:\n",
    "        raise ValueError(\"No O\u2013C points remain with uncertainties < 3 min!\")\n",
    "\n",
    "    epochs_f = all_epochs[mask]\n",
    "    ttv_f = oc_min[mask]\n",
    "    ttv_err_f = oc_err_min[mask]\n",
    "\n",
    "    # Create a shifted epoch axis for plotting (shift so minimum epoch is zero)\n",
    "    x_data = epochs_f\n",
    "    x_shifted = x_data - np.min(x_data)\n",
    "\n",
    "    # Plot the TTV curve with error bars and an 8th-order polynomial fit\n",
    "    if show_plots:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.errorbar(x_shifted, ttv_f, yerr=ttv_err_f, fmt='o', color='black',\n",
    "                     ecolor='gray', capsize=3)\n",
    "        plt.title(\"Figure 1a: Observed-Calculated (O\u2013C) Plot\")\n",
    "        plt.xlabel(\"Epochs (shifted)\")\n",
    "        plt.ylabel(\"TTV Amplitude (minutes)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Polynomial fit for visualization (8th order)\n",
    "        coeffs = np.polyfit(x_shifted, ttv_f, 8)\n",
    "        poly_fit = np.poly1d(coeffs)\n",
    "        x_fit = np.linspace(np.min(x_shifted), np.max(x_shifted), 500)\n",
    "        y_fit = poly_fit(x_fit)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.errorbar(x_shifted, ttv_f, yerr=ttv_err_f, fmt='o', color='black',\n",
    "                     ecolor='gray', capsize=3, label='Data')\n",
    "        plt.plot(x_fit, y_fit, color='black', label='8th Order Fit')\n",
    "        plt.title(\"Figure 1b: O\u2013C Plot with Polynomial Fit\")\n",
    "        plt.xlabel(\"Epochs (shifted)\")\n",
    "        plt.ylabel(\"TTV Amplitude (minutes)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Mean-center the TTV data for Lomb\u2013Scargle\n",
    "    y_data = ttv_f - np.mean(ttv_f)\n",
    "\n",
    "    # Set the current signal to be iteratively cleaned\n",
    "    current_signal = y_data.copy()\n",
    "    results = []\n",
    "\n",
    "    # Iteratively remove the dominant periodic signal 10 times\n",
    "    for i in range(10):\n",
    "        ls_current = LombScargle(epochs_f, current_signal, ttv_err_f)\n",
    "        frequency, power = ls_current.autopower(\n",
    "            minimum_frequency=1.0/50.0,\n",
    "            maximum_frequency=0.5,\n",
    "            samples_per_peak=10\n",
    "        )\n",
    "        periods = 1.0 / frequency  # Periods in epochs\n",
    "\n",
    "        # Limit to periods less than 40 epochs\n",
    "        valid = periods < 40\n",
    "        if not np.any(valid):\n",
    "            print(f\"No valid periods (periods < 40 epochs) found in iteration {i+1}.\")\n",
    "            break\n",
    "\n",
    "        periods_valid = periods[valid]\n",
    "        power_valid = power[valid]\n",
    "\n",
    "        idx_max = np.argmax(power_valid)\n",
    "        dom_period = periods_valid[idx_max]\n",
    "        dom_power = power_valid[idx_max]\n",
    "\n",
    "        var_current = np.var(current_signal)\n",
    "        amplitude = 2.0 * np.sqrt(dom_power * var_current) if dom_power > 0 else 0.0\n",
    "        fap = ls_current.false_alarm_probability(dom_power)\n",
    "\n",
    "        results.append({\n",
    "            'iteration': i + 1,\n",
    "            'dominant_period': dom_period,\n",
    "            'power': dom_power,\n",
    "            'amplitude': amplitude,\n",
    "            'fap': fap\n",
    "        })\n",
    "\n",
    "        print(f\"\\n===== Iteration {i+1} =====\")\n",
    "        print(f\"Dominant Period (epochs) = {dom_period:.3f}\")\n",
    "        print(f\"Power                  = {dom_power:.4f}\")\n",
    "        print(f\"Amplitude (min)        = {amplitude:.3f}\")\n",
    "        print(f\"FAP                    = {fap:.4g}\")\n",
    "\n",
    "        if show_plots:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(periods, power, label=f\"Iteration {i+1} Power Spectrum\")\n",
    "            plt.scatter(dom_period, dom_power, color='red', zorder=3,\n",
    "                        label=f\"Dominant Period ({dom_period:.2f} epochs)\")\n",
    "            plt.xlabel(\"Period (epochs)\")\n",
    "            plt.ylabel(\"Power\")\n",
    "            plt.title(f\"Lomb\u2013Scargle Periodogram (Iteration {i+1})\")\n",
    "            plt.xlim(2, 10)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Subtract the best\u2013fit sinusoid for the dominant period from the current signal\n",
    "        model = ls_current.model(epochs_f, frequency=1.0/dom_period)\n",
    "        current_signal = current_signal - model\n",
    "\n",
    "    return results, mask\n",
    "\n",
    "###############################################################################\n",
    "# NASA EXOPLANET ARCHIVE QUERY FUNCTIONS\n",
    "###############################################################################\n",
    "\n",
    "def tap_query(base_url, query):\n",
    "    uri_full = base_url\n",
    "    for k in query:\n",
    "        if k != \"format\":\n",
    "            uri_full += f\"{k} {query[k]} \"\n",
    "    uri_full = f\"{uri_full[:-1]} &format={query.get('format', 'csv')}\"\n",
    "    uri_full = uri_full.replace(' ', '+')\n",
    "    print(\"Query URL:\", uri_full)\n",
    "\n",
    "    response = requests.get(uri_full, timeout=60)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Query failed: status {response.status_code}\\n{response.text}\")\n",
    "\n",
    "    df = pd.read_csv(StringIO(response.text))\n",
    "    return df\n",
    "\n",
    "def query_inner_params(target_name):\n",
    "    \"\"\"\n",
    "    Query NASA Exoplanet Archive for parameters:\n",
    "      st_mass, pl_bmasse, pl_orbper, pl_orbeccen, pl_orblper, pl_orbincl.\n",
    "    The helper returns the row with the most non-null values among\n",
    "    these columns so that we have the best available parameters for\n",
    "    subsequent analysis.\n",
    "    \"\"\"\n",
    "    base_url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=\"\n",
    "    query_dict = {\n",
    "        \"select\": \"st_mass, pl_bmasse, pl_orbper, pl_orbeccen, pl_orblper, pl_orbincl\",\n",
    "        \"from\": \"ps\",\n",
    "        \"where\": f\"pl_name = '{target_name}'\",\n",
    "        \"format\": \"csv\"\n",
    "    }\n",
    "    df = tap_query(base_url, query_dict)\n",
    "    if len(df) == 0:\n",
    "        print(f\"No rows returned for {target_name}. Possibly unrecognized planet name.\")\n",
    "        return {}\n",
    "\n",
    "    cols = [\"st_mass\", \"pl_bmasse\", \"pl_orbper\", \"pl_orbeccen\", \"pl_orblper\", \"pl_orbincl\"]\n",
    "    best_idx = None\n",
    "    best_count = -1\n",
    "    for i in range(len(df)):\n",
    "        count_valid = df.iloc[i][cols].notna().sum()\n",
    "        if count_valid > best_count:\n",
    "            best_count = count_valid\n",
    "            best_idx = i\n",
    "    if best_idx is None:\n",
    "        return {}\n",
    "\n",
    "    row = df.iloc[best_idx]\n",
    "    out = {\n",
    "        \"st_mass_msun\":   row.get(\"st_mass\", np.nan),\n",
    "        \"pl_bmasse\":      row.get(\"pl_bmasse\", np.nan),\n",
    "        \"pl_orbper_days\": row.get(\"pl_orbper\", np.nan),\n",
    "        \"pl_orbeccen\":    row.get(\"pl_orbeccen\", np.nan),\n",
    "        \"pl_orblper\":     row.get(\"pl_orblper\", np.nan),\n",
    "        \"pl_orbincl_deg\": row.get(\"pl_orbincl\", np.nan)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "###############################################################################\n",
    "# Append results to ocean.csv\n",
    "###############################################################################\n",
    "\n",
    "def write_to_ocean_csv(params, amplitude_dom, dominant_period, power_dominant,\n",
    "                       second_dominant_period, power_second_dominant, filename=\"ocean.csv\"):\n",
    "    columns = [\n",
    "        \"Stellar Mass (Msun)\",\n",
    "        \"Inner Mass (Mearth)\",\n",
    "        \"Inner Period (days)\",\n",
    "        \"Inner Eccentricity\",\n",
    "        \"Inner Inclination\",\n",
    "        \"Inner Omega\",\n",
    "        \"Amplitude of Dominant Period Test (P1)\",\n",
    "        \"Dominant Period Planet 1\",\n",
    "        \"Dominant Period Power Planet 1\",\n",
    "        \"Second Dominant Period Planet 1\",\n",
    "        \"Second Dominant Period Power Planet 1\"\n",
    "    ]\n",
    "    new_row = {\n",
    "        \"Stellar Mass (Msun)\":   params.get(\"st_mass_msun\", np.nan),\n",
    "        \"Inner Mass (Mearth)\":   params.get(\"pl_bmasse\", np.nan),\n",
    "        \"Inner Period (days)\":   params.get(\"pl_orbper_days\", np.nan),\n",
    "        \"Inner Eccentricity\":    params.get(\"pl_orbeccen\", np.nan),\n",
    "        \"Inner Inclination\":     params.get(\"pl_orbincl_deg\", np.nan),\n",
    "        \"Inner Omega\":           params.get(\"pl_orblper\", np.nan),\n",
    "        \"Amplitude of Dominant Period Test (P1)\": amplitude_dom,\n",
    "        \"Dominant Period Planet 1\": dominant_period,\n",
    "        \"Dominant Period Power Planet 1\": power_dominant,\n",
    "        \"Second Dominant Period Planet 1\": second_dominant_period,\n",
    "        \"Second Dominant Period Power Planet 1\": power_second_dominant\n",
    "    }\n",
    "    if not os.path.exists(filename):\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Created {filename} and wrote first row.\")\n",
    "    else:\n",
    "        df = pd.read_csv(filename)\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Appended a new row to existing {filename}.\")\n",
    "\n",
    "###############################################################################\n",
    "# Write planet_name.csv with Epoch and O\u2013C (days)\n",
    "###############################################################################\n",
    "\n",
    "def write_planet_csv(target_name, all_epochs, all_oc, all_oc_err):\n",
    "    \"\"\"\n",
    "    Creates a CSV file named \"<target_name>.csv\" (spaces and dashes replaced by underscores)\n",
    "    with columns: Epoch, O\u2013C (days), and O\u2013C Error (days) for data points\n",
    "    with timing uncertainties below 15 minutes. This allows the timing\n",
    "    data to be imported into other analysis tools.\n",
    "    \"\"\"\n",
    "    # Convert O\u2013C from days to minutes for uncertainty filtering\n",
    "    oc_min = all_oc * 24.0 * 60.0\n",
    "    oc_err_min = all_oc_err * 24.0 * 60.0\n",
    "\n",
    "    mask = oc_err_min < 20.0\n",
    "    if np.sum(mask) == 0:\n",
    "        raise ValueError(\"No O\u2013C points remain with uncertainties < 15 min, so no CSV created.\")\n",
    "\n",
    "    epochs_f = all_epochs[mask]\n",
    "    oc_f = all_oc[mask]        # Keep O\u2013C in days\n",
    "    oc_err_f = all_oc_err[mask]  # Keep O\u2013C uncertainty in days\n",
    "\n",
    "    # Construct a filename based on the target name\n",
    "    planet_csv_filename = f\"E:/{target_name.replace(' ', '_').replace('-', '_')}.csv\"\n",
    "    df = pd.DataFrame({\n",
    "        \"Epoch\": epochs_f,\n",
    "        \"O-C (days)\": oc_f,\n",
    "        \"O-C Error (days)\": oc_err_f\n",
    "    })\n",
    "    df.to_csv(planet_csv_filename, index=False)\n",
    "    print(f\"Wrote {len(df)} rows of O\u2013C data with errors to {planet_csv_filename}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# MAIN: Example usage (ensure your data arrays are defined)\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # The following variables (all_epochs, all_oc, all_oc_err) must be defined before running.\n",
    "    # For example, they might come from a file or earlier processing step.\n",
    "    #\n",
    "    # all_epochs : numpy array of epochs (in epochs)\n",
    "    # all_oc     : numpy array of O\u2013C values (in days)\n",
    "    # all_oc_err : numpy array of uncertainties in O\u2013C (in days)\n",
    "\n",
    "    # Set your target planet name (adjust as needed)\n",
    "    target_name = \"Kepler-1710 b\"\n",
    "    # 'period' is provided for compatibility, although it is not directly used in the iterative analysis.\n",
    "    period = 7.64159\n",
    "\n",
    "    # --- Run iterative Lomb\u2013Scargle analysis ---\n",
    "    results, final_mask = run_lomb_scargle_TTV(all_epochs, all_oc, all_oc_err, period, show_plots=True)\n",
    "\n",
    "    # Find the first iteration with FAP below 0.1\n",
    "    dominant_result = None\n",
    "    for res in results:\n",
    "        if res['fap'] < 0.1:\n",
    "            dominant_result = res\n",
    "            break\n",
    "\n",
    "    if dominant_result is not None:\n",
    "        amplitude_dom = dominant_result['amplitude']\n",
    "        dom_period = dominant_result['dominant_period']\n",
    "        power_dom = dominant_result['power']\n",
    "        # For compatibility we set second dominant period values equal to the first\n",
    "        sec_period = dom_period\n",
    "        power_sec = power_dom\n",
    "\n",
    "        # --- Query NASA Exoplanet Archive for additional parameters ---\n",
    "        params = query_inner_params(target_name)\n",
    "\n",
    "        # --- Append results to ocean.csv ---\n",
    "        write_to_ocean_csv(params, amplitude_dom, dom_period, power_dom,\n",
    "                           sec_period, power_sec, filename=\"E:/ocean.csv\")\n",
    "    else:\n",
    "        print(\"No dominant period with FAP below 0.1 was found; nothing appended to ocean.csv.\")\n",
    "\n",
    "    # --- Write CSV with Epoch and O\u2013C data ---\n",
    "    # Saving the final timing data allows you to explore the\n",
    "    # TTV signal in external programs or share it with collaborators.\n",
    "    write_planet_csv(target_name, all_epochs, all_oc, all_oc_err)\n",
    "\n",
    "    print(\"\\nAll tasks complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "4e438ee1-6552-4aa9-9a8f-c1440c40e89d",
   "source": [
    "## 3. Visualizing Harmonics of the O\u2013C Curve\n\nThis cell demonstrates how to combine the dominant Lomb\u2013Scargle signals and overlay them on the observed minus calculated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855c56e-c24a-4ab3-b7e7-1da9fe4a7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.timeseries import LombScargle\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Visualise the harmonic content found by the\n",
    "# Lomb--Scargle analysis. This cell reconstructs\n",
    "# the dominant sinusoids and plots them together\n",
    "# with the measured TTV data.\n",
    "# -----------------------------------------------\n",
    "# Suppose you already have:\n",
    "#   all_epochs, all_oc, all_oc_err\n",
    "#   final_mask from run_lomb_scargle_TTV\n",
    "#   results (list of dictionaries) from the 10 iterations\n",
    "\n",
    "# Convert O\u2013C to minutes\n",
    "oc_min = all_oc * 24 * 60\n",
    "oc_err_min = all_oc_err * 24 * 60\n",
    "\n",
    "# Filtered data using final_mask\n",
    "epochs_f = all_epochs[final_mask]\n",
    "ttv_f = oc_min[final_mask]\n",
    "ttv_err_f = oc_err_min[final_mask]\n",
    "\n",
    "# Shift epochs for plotting (optional)\n",
    "x_shifted = epochs_f - np.min(epochs_f)\n",
    "\n",
    "# Mean-center TTV data (as done in the LS analysis)\n",
    "mean_ttv = np.mean(ttv_f)\n",
    "y_data = ttv_f - mean_ttv\n",
    "\n",
    "# Create a Lomb-Scargle object for the entire dataset\n",
    "ls_orig = LombScargle(epochs_f, y_data, ttv_err_f)\n",
    "\n",
    "# We will evaluate each harmonic on a *dense* grid of epochs:\n",
    "num_points = 500\n",
    "epochs_dense = np.linspace(np.min(epochs_f), np.max(epochs_f), num_points)\n",
    "x_dense_shifted = epochs_dense - np.min(epochs_f)\n",
    "\n",
    "# Initialize composite model on the dense grid\n",
    "composite_dense = np.zeros(num_points)\n",
    "\n",
    "# Plot original data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.errorbar(x_shifted, ttv_f, yerr=ttv_err_f, fmt='o', color='black',\n",
    "             ecolor='gray', capsize=3)\n",
    "\n",
    "# Number of harmonics to show\n",
    "num_harmonics = min(2, len(results))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_harmonics))\n",
    "\n",
    "for i in range(num_harmonics):\n",
    "    period_i = results[i]['dominant_period']\n",
    "    freq_i = 1.0 / period_i\n",
    "\n",
    "    # Evaluate the sinusoidal model on the *dense* grid\n",
    "    model_dense_i = ls_orig.model(epochs_dense, frequency=freq_i)\n",
    "    # Add this component to the composite\n",
    "    composite_dense += model_dense_i\n",
    "\n",
    "    # Plot the individual harmonic as a dashed line\n",
    "    # Shift upward by the mean so it aligns with the data's absolute scale\n",
    "    plt.plot(x_dense_shifted, model_dense_i + mean_ttv,\n",
    "             linestyle='--', color=colors[i],\n",
    "             label=f'Dominant Signal {i+1} (Period = {period_i:.2f} epochs)')\n",
    "\n",
    "# After summing up all components, shift the total back up by the mean\n",
    "composite_dense += mean_ttv\n",
    "\n",
    "# Plot the composite model as a solid line\n",
    "#plt.plot(x_dense_shifted, composite_dense, color='blue', lw=2,\n",
    "#         label='Composite 2-Harmonic Fit')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"TTV Amplitude (minutes)\", fontsize=14)\n",
    "plt.title(\"Composite 2-Harmonic Curve Fit\", fontsize=16)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cb1be2ec-26f7-4ea2-b229-fc59d4bccca8",
   "source": [
    "## 4. Chi-Squared Diagnostics for Model Fits\n\nFinally we compute goodness-of-fit statistics for models derived from the Lomb\u2013Scargle analysis and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788e938-1d68-470d-b200-308bdd932ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.timeseries import LombScargle\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Assess the quality of the harmonic fit using\n",
    "# reduced chi-square statistics and visualise the\n",
    "# model together with the raw TTV measurements.\n",
    "# -----------------------------------------------\n",
    "# Suppose you already have:\n",
    "#   all_epochs, all_oc, all_oc_err\n",
    "#   final_mask from run_lomb_scargle_TTV\n",
    "#   results (list of dictionaries) from the 10 iterations\n",
    "\n",
    "# 1) Convert O\u2013C to minutes\n",
    "oc_min     = all_oc     * 24 * 60\n",
    "oc_err_min = all_oc_err * 24 * 60\n",
    "\n",
    "# 2) Filter data using final_mask\n",
    "epochs_f  = all_epochs[final_mask]\n",
    "ttv_f     = oc_min[final_mask]\n",
    "ttv_err_f = oc_err_min[final_mask]\n",
    "\n",
    "# 3) Shift epochs for plotting (optional)\n",
    "x_shifted_epochs = epochs_f - np.min(epochs_f)\n",
    "x_shifted_days   = x_shifted_epochs * 33.6    # multiply by 33.6 to turn epochs into days\n",
    "\n",
    "# 4) Mean\u2010center the TTV data\n",
    "mean_ttv = np.mean(ttv_f)\n",
    "y_data   = ttv_f - mean_ttv\n",
    "\n",
    "# 5) Create Lomb-Scargle object\n",
    "ls_orig = LombScargle(epochs_f, y_data, ttv_err_f)\n",
    "\n",
    "# 6) Compute \u03c7\u00b2 for the null model (y=0)\n",
    "chi2_null  = np.sum((y_data / ttv_err_f)**2)\n",
    "ndof_null  = len(y_data) - 1               # one parameter (the mean)\n",
    "chi2r_null = chi2_null / ndof_null\n",
    "print(f\"Chi\u00b2 (null model y=0):           {chi2_null:.2f}\")\n",
    "print(f\"Reduced \u03c7\u00b2 (dof={ndof_null}):    {chi2r_null:.2f}\")\n",
    "\n",
    "# 7) Compute \u03c7\u00b2 for the dominant-period fit\n",
    "P1            = results[0]['dominant_period']\n",
    "f1            = 1.0 / P1\n",
    "model1_epochs = ls_orig.model(epochs_f, frequency=f1)\n",
    "resid1        = y_data - model1_epochs\n",
    "chi2_dom      = np.sum((resid1 / ttv_err_f)**2)\n",
    "ndof_dom      = len(y_data) - 2           # two fitted parameters (amp & phase)\n",
    "chi2r_dom     = chi2_dom / ndof_dom\n",
    "print(f\"Chi\u00b2 (dominant P={P1:.2f} epochs):    {chi2_dom:.2f}\")\n",
    "print(f\"Reduced \u03c7\u00b2 (dof={ndof_dom}):    {chi2r_dom:.2f}\")\n",
    "\n",
    "# 8) Compute \u03c7\u00b2 for the two\u2010period composite (dominant + second\u2010dominant)\n",
    "if len(results) > 1:\n",
    "    P2            = results[1]['dominant_period']\n",
    "    f2            = 1.0 / P2\n",
    "    model2_raw    = ls_orig.model(epochs_f, frequency=f2)\n",
    "\n",
    "    amp2_raw      = 0.5 * (model2_raw.max() - model2_raw.min())\n",
    "    scale2        = amp2_raw\n",
    "    model2_epochs = model2_raw * scale2\n",
    "\n",
    "    composite_epochs = model1_epochs + model2_epochs\n",
    "    resid_comp       = y_data - composite_epochs\n",
    "    chi2_comp        = np.sum((resid_comp / ttv_err_f)**2)\n",
    "    ndof_comp        = len(y_data) - 4       # 4 fitted params: amp & phase \u00d7 2\n",
    "    chi2r_comp       = chi2_comp / ndof_comp\n",
    "    print(f\"Chi\u00b2 (P\u2081={P1:.2f}, P\u2082={P2:.2f} epochs):  {chi2_comp:.2f}\")\n",
    "    print(f\"Reduced \u03c7\u00b2 (dof={ndof_comp}):           {chi2r_comp:.2f}\")\n",
    "\n",
    "# 9) Plot data and up to two harmonics\n",
    "num_points     = 500\n",
    "epochs_dense   = np.linspace(np.min(epochs_f), np.max(epochs_f), num_points)\n",
    "x_dense_shift  = (epochs_dense - np.min(epochs_f)) * 33.6\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.errorbar(\n",
    "    x_shifted_days,\n",
    "    ttv_f,\n",
    "    yerr=ttv_err_f,\n",
    "    fmt='o', color='black', ecolor='gray', capsize=3\n",
    ")\n",
    "\n",
    "num_harmonics = min(2, len(results))\n",
    "colors        = ['blue', 'green'][:num_harmonics]\n",
    "composite_dense = np.zeros(num_points)\n",
    "\n",
    "for i in range(num_harmonics):\n",
    "    P_i = results[i]['dominant_period']\n",
    "    f_i = 1.0 / P_i\n",
    "    model_dense = ls_orig.model(epochs_dense, frequency=f_i)\n",
    "\n",
    "    if i == 1:\n",
    "        # scale second harmonic to 20.25 min\n",
    "        amp_raw     = 0.5 * (model_dense.max() - model_dense.min())\n",
    "        scale       = amp_raw\n",
    "        model_dense *= scale\n",
    "\n",
    "    composite_dense += model_dense\n",
    "\n",
    "    plt.plot(\n",
    "        x_dense_shift,\n",
    "        model_dense + mean_ttv,\n",
    "        linestyle='--',\n",
    "        color=colors[i],\n",
    "        alpha=0.5,\n",
    "        label=f\"Signal {i+1} (P={P_i:.2f} epochs)\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Days\", fontsize=14)\n",
    "plt.ylabel(\"TTV Amplitude (minutes)\", fontsize=14)\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
