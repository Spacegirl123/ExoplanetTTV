{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEPTUNE Step 3 Training Module\n",
    "This notebook walks through using machine learning to generate informative priors and then applying Bayesian inference with MCMC. Each section below explains the purpose of the code so you can follow along and adapt it to your own data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "The code relies on `emcee`, `corner`, `plotly`, and `hdbscan`. If these packages are not installed in your environment, run the following cell to install them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Global Variables\n",
    "This section imports the required Python libraries and defines convenient global constants used throughout the training module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in scalar subtract\")\n",
    "from emcee.moves import StretchMove\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import hdbscan\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "import corner\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import Axes3D  # for 3D plotting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.cluster import DBSCAN  # for clustering the MCMC samples\n",
    "import plotly.express as px\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import Normalize, ListedColormap\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "multiprocessing.set_start_method('fork', force=True)\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Create a \"clipped\" version of inferno using only the 0.98 \u2192 1.0 segment\n",
    "inferno_limited = ListedColormap(plt.cm.inferno(np.linspace(0.7, 1.0, 256)))\n",
    "# Global placeholders to avoid pickling large models\n",
    "_MCMC_BIN = None\n",
    "_MCMC_PARAM = None\n",
    "_MCMC_MODEL = None\n",
    "_MCMC_SCALER = None\n",
    "_MCMC_AMP = None\n",
    "_MCMC_FREQ = None\n",
    "\n",
    "from emcee.moves import StretchMove\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom StretchMove and Clustering Utilities\n",
    "Here we implement custom sampling moves and clustering helpers for the MCMC algorithm, allowing efficient exploration and grouping of candidate solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClippedStretchMove(StretchMove):\n",
    "    def __init__(self, a, min_p, max_p):\n",
    "        super().__init__(a=a)\n",
    "        self.min_p = min_p\n",
    "        self.max_p = max_p\n",
    "\n",
    "    def propose(self, ensemble, state):\n",
    "        # emcee now returns (new_state, accepted)\n",
    "        new_state, accepted = super().propose(ensemble, state)\n",
    "\n",
    "        # Clip the period (parameter 0) directly in the coords array:\n",
    "        new_state.coords[..., 0] = np.clip(\n",
    "            new_state.coords[..., 0],\n",
    "            self.min_p,\n",
    "            self.max_p\n",
    "        )\n",
    "\n",
    "        return new_state, accepted\n",
    "\n",
    "\n",
    "\n",
    "def cluster_hdbscan(samples, min_cluster_size=30):\n",
    "    \"\"\"\n",
    "    samples : (N,3) array, already scaled to [0,1] or z-score\n",
    "    returns: (clusters, clusterer) where\n",
    "      clusters: list of dict {label, indices, mean, std, n_samples}\n",
    "      clusterer: the fitted HDBSCAN object (has .outlier_scores_, etc)\n",
    "    \"\"\"\n",
    "    model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                            cluster_selection_method='eom',\n",
    "                            prediction_data=True)\n",
    "    labels = model.fit_predict(samples)\n",
    "    clusters = []\n",
    "    for lab in np.unique(labels):\n",
    "        if lab == -1:\n",
    "            continue\n",
    "        idx = np.flatnonzero(labels == lab)\n",
    "        pts = samples[idx]\n",
    "        clusters.append({\n",
    "            \"label\":      int(lab),\n",
    "            \"indices\":    idx,\n",
    "            \"mean\":       pts.mean(axis=0),\n",
    "            \"std\":        pts.std(axis=0),\n",
    "            \"n_samples\":  len(idx)\n",
    "        })\n",
    "    return clusters, model\n",
    "\n",
    "def init_mcmc_globals(bin_name, pf, modelA, scA, actual_amp, actual_freq):\n",
    "    \"\"\"\n",
    "    Initialize globals so that the worker processes can access them without pickling.\n",
    "    \"\"\"\n",
    "    global _MCMC_BIN, _MCMC_PARAM, _MCMC_MODEL, _MCMC_SCALER, _MCMC_AMP, _MCMC_FREQ\n",
    "    _MCMC_BIN = bin_name\n",
    "    _MCMC_PARAM = pf\n",
    "    _MCMC_MODEL = modelA\n",
    "    _MCMC_SCALER = scA\n",
    "    _MCMC_AMP = actual_amp\n",
    "    _MCMC_FREQ = actual_freq\n",
    "\n",
    "\n",
    "def _log_prob_fast(theta):\n",
    "    \"\"\"\n",
    "    A thin wrapper that only takes theta and uses global variables to evaluate log-prob.\n",
    "    \"\"\"\n",
    "    return log_prob_3params_layer1(theta,\n",
    "                                   _MCMC_BIN,\n",
    "                                   _MCMC_PARAM,\n",
    "                                   _MCMC_MODEL,\n",
    "                                   _MCMC_SCALER,\n",
    "                                   _MCMC_AMP,\n",
    "                                   _MCMC_FREQ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentile Corner Plot\n",
    "Utility functions for producing corner plots with percentile labels are defined so we can visualize posterior distributions at a glance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentile_corner(chain_flat_post, log_probs, labels, bins=50):\n",
    "    \"\"\"\n",
    "    Generates a corner plot where the color of the data points is based on fit percentiles.\n",
    "    Uses only the 0.98 \u2192 1.0 segment of the inferno colormap (bright top).\n",
    "    The colorbar is displayed from 0.0 \u2192 1.0 (where lower percentiles = better fits).\n",
    "    \n",
    "    Parameters:\n",
    "    - chain_flat_post: (N, ndim) array of post burn-in MCMC samples.\n",
    "    - log_probs: Log-probabilities of the samples (higher = better fit).\n",
    "    - labels: List of parameter names.\n",
    "    - bins: Number of bins for the corner plot.\n",
    "    \"\"\"\n",
    "    ndim = len(labels)\n",
    "    import numpy as np\n",
    "\n",
    "    # Convert eccentricity from log-scale to linear for visualization (if needed)\n",
    "    chain_for_plot = chain_flat_post.copy()\n",
    "    chain_for_plot[:, 1] = chain_for_plot[:, 1]  # Example placeholder if you want log10(e) \u2192 e\n",
    "\n",
    "    # Convert log-probabilities into percentiles [0, 1] (0 = best fit, 1 = worst fit)\n",
    "    # Higher log_prob \u2192 better fit \u2192 lower percentile\n",
    "    # np.argsort(-log_probs) ranks best log_prob as index 0\n",
    "    ranks = np.argsort(np.argsort(-log_probs))\n",
    "    percentiles = ranks / (len(ranks) - 1)\n",
    "    # compute the histogram\u2010mode\n",
    "    mass_vals = chain_for_plot[:, 2]  \n",
    "    counts, edges = np.histogram(mass_vals, bins=100)\n",
    "\n",
    "    i_mode = np.argmax(counts)\n",
    "    mode_mass = 0.5 * (edges[i_mode] + edges[i_mode+1])\n",
    "    \n",
    "\n",
    "    # Create the corner plot without scatter first\n",
    "    fig_percentile = corner.corner(\n",
    "        chain_for_plot,\n",
    "        labels=labels,\n",
    "        #truths=[None, None, mode_mass],\n",
    "        #truth_kwargs={'linewidth': 0},\n",
    "        bins=bins,\n",
    "        plot_contours=False,\n",
    "        plot_density=False\n",
    "    )\n",
    "\n",
    "    # Now overplot scatter with percentile-based coloring\n",
    "    axes = np.array(fig_percentile.axes).reshape((ndim, ndim))\n",
    "    norm = Normalize(vmin=0, vmax=1)  # colorbar range 0 \u2192 1\n",
    "    for i in range(ndim):\n",
    "        for j in range(i):\n",
    "            ax = axes[i, j]\n",
    "            scatter = ax.scatter(\n",
    "                chain_for_plot[:, j],\n",
    "                chain_for_plot[:, i],\n",
    "                c=percentiles,\n",
    "                cmap=inferno_limited,  # Use the clipped inferno\n",
    "                norm=norm,\n",
    "                s=4,\n",
    "                alpha=0.8\n",
    "            )\n",
    "\n",
    "    # Move colorbar outside the plot\n",
    "    fig_percentile.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig_percentile.add_axes([0.85, 0.3, 0.02, 0.4])\n",
    "    cb = fig_percentile.colorbar(scatter, cax=cbar_ax)\n",
    "    cb.set_label(\"Fit Percentile (0 = Best)\")\n",
    "\n",
    "    plt.title(\"Percentile-Based Corner Plot (Using 0.98 \u2192 1.0 of Inferno)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import Normalize, ListedColormap\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import Normalize, ListedColormap\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import Normalize, ListedColormap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Plot\n",
    "This portion adds plotting tools to track convergence of the MCMC chains and confirm that the sampler has stabilized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_corner(chain_flat_post, labels, bins=50):\n",
    "    \"\"\"\n",
    "    Demonstration of 'darker = more convergence' with extra axis margin.\n",
    "    We explicitly set the corner plot range to include 30% more space\n",
    "    around the min/max of the data in each dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Build custom colormap for top 10% of inferno, reversed ---\n",
    "    top_vals = np.linspace(0.7, 1.0, 256)\n",
    "    inferno_slice = plt.cm.inferno(top_vals)\n",
    "    inferno_top10_r = ListedColormap(inferno_slice[::-1])\n",
    "\n",
    "    ndim = len(labels)\n",
    "    chain_for_plot = chain_flat_post.copy()\n",
    "\n",
    "    # Example: convert log10(e) \u2192 e\n",
    "    chain_for_plot[:, 1] = 10 ** chain_for_plot[:, 1]\n",
    "\n",
    "    # Avoid singular KDE issues\n",
    "    variances = np.var(chain_for_plot, axis=0)\n",
    "    low_variance_mask = variances < 1e-10\n",
    "    chain_for_plot[:, low_variance_mask] += np.random.normal(\n",
    "        0, 1e-6, size=chain_for_plot[:, low_variance_mask].shape\n",
    "    )\n",
    "\n",
    "    # --- KDE for densities ---\n",
    "    kde = gaussian_kde(chain_for_plot.T, bw_method='scott')\n",
    "    densities = kde(chain_for_plot.T)\n",
    "\n",
    "    # Example: log-scale densities, then keep top 70%\n",
    "    densities = np.log1p(densities)\n",
    "    density_threshold = np.percentile(densities, 70)\n",
    "    mask = densities > density_threshold\n",
    "    chain_filtered = chain_for_plot[mask]\n",
    "    densities_filtered = densities[mask]\n",
    "\n",
    "    # =================================================\n",
    "    #  A) Compute per-dimension ranges with +30% margin\n",
    "    # =================================================\n",
    "    data_min = chain_for_plot.min(axis=0)\n",
    "    data_max = chain_for_plot.max(axis=0)\n",
    "\n",
    "    ranges = []\n",
    "    for i in range(ndim):\n",
    "        span = data_max[i] - data_min[i]\n",
    "        if span <= 0:\n",
    "            # If dimension is nearly constant, just pick a small fixed range\n",
    "            pad = 1e-3\n",
    "        else:\n",
    "            # 30% extra space on each side\n",
    "            pad = 0\n",
    "        low = data_min[i] - pad\n",
    "        high = data_max[i] + pad\n",
    "        ranges.append((low, high))\n",
    "\n",
    "    # --- Heatmap-based corner plot (optional) ---\n",
    "\n",
    "    # compute the histogram\u2010mode\n",
    "    counts, edges = np.histogram(mass, bins=100)\n",
    "    i_mode = np.argmax(counts)\n",
    "    mode_mass = 0.5 * (edges[i_mode] + edges[i_mode+1])\n",
    "\n",
    "    fig_heat = corner.corner(\n",
    "        chain_for_plot,\n",
    "        bins=50,\n",
    "        range=full_range,\n",
    "        plot_density=True,\n",
    "        plot_datapoints=False,\n",
    "        smooth=1,\n",
    "        smooth1d=1,\n",
    "        fill_contours=True,\n",
    "        contour_kwargs={},\n",
    "        color=None,\n",
    "        show_titles=True,\n",
    "        title_fmt=\".3f\"\n",
    "    )\n",
    "    plt.title(\"Heatmap-Based Corner Plot\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Scatter-based corner plot ---\n",
    "    fig_scatter = corner.corner(\n",
    "        chain_filtered,\n",
    "        labels=labels,\n",
    "        #truths=[None, None, mode_mass],\n",
    "        #truth_kwargs={'linewidths': 0},\n",
    "        bins=bins,\n",
    "        range=ranges,      # <-- pass same expanded ranges\n",
    "        plot_contours=False,\n",
    "        plot_density=False\n",
    "    )\n",
    "\n",
    "    # Normalize so min density -> colormap(0.0) (bright) and max density -> colormap(1.0) (dark)\n",
    "    norm = Normalize(vmin=densities_filtered.min(), vmax=densities_filtered.max())\n",
    "\n",
    "    axes = np.array(fig_scatter.axes).reshape((ndim, ndim))\n",
    "    for i in range(ndim):\n",
    "        for j in range(i):\n",
    "            ax = axes[i, j]\n",
    "            scatter = ax.scatter(\n",
    "                chain_filtered[:, j],\n",
    "                chain_filtered[:, i],\n",
    "                c=densities_filtered,\n",
    "                cmap=inferno_top10_r,\n",
    "                norm=norm,\n",
    "                s=4,\n",
    "                alpha=0.8\n",
    "            )\n",
    "\n",
    "    # Move colorbar\n",
    "    fig_scatter.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig_scatter.add_axes([0.85, 0.3, 0.02, 0.4])\n",
    "    cb = fig_scatter.colorbar(scatter, cax=cbar_ax)\n",
    "    cb.set_label(\"Convergence Intensity\")\n",
    "\n",
    "    plt.title(\"Scatter-Based Corner Plot\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# DBSCAN cluster function (unchanged)\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Diagnostics\n",
    "After sampling, diagnostic functions report statistics like acceptance rates and autocorrelation lengths to evaluate performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_mcmc_samples(chain_samples, eps=0.1, min_samples=30):\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(chain_samples)\n",
    "    labels = clustering.labels_\n",
    "    clusters = []\n",
    "    for label in set(labels):\n",
    "        if label == -1:  # ignore noise points\n",
    "            continue\n",
    "        indices = np.where(labels == label)[0]\n",
    "        cluster_samples = chain_samples[indices]\n",
    "        mean = np.mean(cluster_samples, axis=0)\n",
    "        std = np.std(cluster_samples, axis=0)\n",
    "        clusters.append({\"mean\": mean, \"std\": std, \"n_samples\": len(cluster_samples), \"indices\": indices})\n",
    "    return clusters\n",
    "\n",
    "###############################################################################\n",
    "# Standard Gelman-Rubin convergence diagnostic for a set of chains.\n",
    "###############################################################################\n",
    "def gelman_rubin(chains):\n",
    "    \"\"\"\n",
    "    Computes the Gelman-Rubin statistic for a set of chains.\n",
    "    Input:\n",
    "      chains: numpy array with shape (n_steps, n_walkers, n_dim)\n",
    "    Returns:\n",
    "      rhat: numpy array of length n_dim with the R\u0302 values.\n",
    "    \"\"\"\n",
    "    n_steps, n_walkers, n_dim = chains.shape\n",
    "    rhat = np.empty(n_dim)\n",
    "    for d in range(n_dim):\n",
    "        # Get chain for parameter d: shape (n_steps, n_walkers)\n",
    "        chain_d = chains[:, :, d]\n",
    "        chain_means = np.mean(chain_d, axis=0)\n",
    "        overall_mean = np.mean(chain_means)\n",
    "        B = n_steps * np.var(chain_means, ddof=1)\n",
    "        W = np.mean(np.var(chain_d, axis=0, ddof=1))\n",
    "        Var_hat = ((n_steps - 1) / n_steps) * W + (B / n_steps)\n",
    "        rhat[d] = np.sqrt(Var_hat / W)\n",
    "    return rhat\n",
    "\n",
    "###############################################################################\n",
    "# Gelman-Rubin diagnostic computed on samples in one cluster\n",
    "###############################################################################\n",
    "def gelman_rubin_cluster(chain_post, cluster_indices, n_walkers):\n",
    "    \"\"\"\n",
    "    Compute Rhat for a given cluster. The cluster_indices are indices from the \n",
    "    flattened chain (from chain_post.reshape(-1, ndim)). This function re-groups \n",
    "    the samples by walker.\n",
    "\n",
    "    Parameters:\n",
    "      chain_post: numpy array of shape (n_steps, n_walkers, n_dim)\n",
    "      cluster_indices: 1D array of flattened indices (from DBSCAN clustering)\n",
    "      n_walkers: number of walkers (as in chain_post.shape[1])\n",
    "    \n",
    "    Returns:\n",
    "      rhat: R\u0302 values for this cluster (numpy array of length n_dim),\n",
    "            or None if insufficient samples/walkers.\n",
    "    \"\"\"\n",
    "    n_steps, n_walkers, n_dim = chain_post.shape\n",
    "\n",
    "    # Build a dictionary: for each walker, store a list of samples (in order of appearance).\n",
    "    samples_by_walker = {w: [] for w in range(n_walkers)}\n",
    "    \n",
    "    # Because the flattened chain was obtained with row-major order,\n",
    "    # each index i corresponds to: step = i // n_walkers, walker = i % n_walkers.\n",
    "    for idx in cluster_indices:\n",
    "        step = idx // n_walkers\n",
    "        walker = idx % n_walkers\n",
    "        samples_by_walker[walker].append(chain_post[step, walker, :])\n",
    "    \n",
    "    # Only keep walkers with at least 2 samples\n",
    "    valid_walkers = {w: np.array(samples) for w, samples in samples_by_walker.items() if len(samples) >= 2}\n",
    "    \n",
    "    if len(valid_walkers) < 2:\n",
    "        # Not enough chains in this cluster to compute Rhat\n",
    "        return None\n",
    "\n",
    "    # Truncate each walker's chain to the minimum number of samples found\n",
    "    min_samples = min(len(samples) for samples in valid_walkers.values())\n",
    "    # Build an array of shape (min_samples, n_valid_walkers, n_dim)\n",
    "    chains_cluster = []\n",
    "    for w in valid_walkers:\n",
    "        chain_w = valid_walkers[w][:min_samples]\n",
    "        chains_cluster.append(chain_w)\n",
    "    chains_cluster = np.stack(chains_cluster, axis=1)  # shape: (min_samples, n_valid_walkers, n_dim)\n",
    "    rhat = gelman_rubin(chains_cluster)\n",
    "    return rhat\n",
    "\n",
    "###############################################################################\n",
    "# Plot predicted vs. actual\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building Functions\n",
    "Functions in this section train random forest models on different data bins to provide informed starting points for the Bayesian inference step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_actual(y_true, y_pred, model_name, target_names):\n",
    "    for i, target in enumerate(target_names):\n",
    "        plt.figure()\n",
    "        plt.scatter(y_true[:, i], y_pred[:, i], alpha=0.7)\n",
    "        plt.plot([min(y_true[:, i]), max(y_true[:, i])],\n",
    "                 [min(y_true[:, i]), max(y_true[:, i])], 'r--')\n",
    "        plt.xlabel(f\"Actual {target}\")\n",
    "        plt.ylabel(f\"Predicted {target}\")\n",
    "        plt.title(f\"{model_name} - Predicted vs. Actual: {target}\")\n",
    "        plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# Get bin mask\n",
    "###############################################################################\n",
    "def get_bin_mask(df_all, period_bin, ecc_bin):\n",
    "    if period_bin == \"p1\":\n",
    "        period_mask = (df_all[\"ratio\"] >= 1.9) & (df_all[\"ratio\"] <= 1.99)\n",
    "    elif period_bin == \"p2\":\n",
    "        period_mask = (df_all[\"ratio\"] >= 2.01) & (df_all[\"ratio\"] <= 2.1)\n",
    "    elif period_bin == \"p3\":\n",
    "        period_mask = (df_all[\"ratio\"] >= 1.4) & (df_all[\"ratio\"] <= 1.49)\n",
    "    elif period_bin == \"p4\":\n",
    "        period_mask = (df_all[\"ratio\"] >= 1.51) & (df_all[\"ratio\"] <= 1.6)\n",
    "    elif period_bin == \"p5\":\n",
    "        period_mask = (df_all[\"ratio\"] >= 2.2) & (df_all[\"ratio\"] <= 2.5)\n",
    "    else:\n",
    "        period_mask = (df_all[\"ratio\"] > 2.5) & (df_all[\"ratio\"] <= 2.8)\n",
    "\n",
    "    if ecc_bin == \"e1\":\n",
    "        ecc_mask = (df_all[\"out_ecc\"] <= -2.0)\n",
    "    elif ecc_bin == \"e2\":\n",
    "        ecc_mask = (df_all[\"out_ecc\"] > -2.0) & (df_all[\"out_ecc\"] <= -1.5228)\n",
    "    else:\n",
    "        ecc_mask = (df_all[\"out_ecc\"] > -1.5228) & (df_all[\"out_ecc\"] <= np.log10(0.08))\n",
    "    return period_mask & ecc_mask\n",
    "\n",
    "###############################################################################\n",
    "# Build ModelB for a bin\n",
    "###############################################################################\n",
    "def build_modelB_for_bin(df_all, period_bin, ecc_bin):\n",
    "    print(f\"\\n=== build_modelB_for_bin => period='{period_bin}', eccentricity='{ecc_bin}'\")\n",
    "    mask_bin = get_bin_mask(df_all, period_bin, ecc_bin)\n",
    "    mask_amp = df_all[\"AmpP1\"] #<= 100\n",
    "    df = df_all[mask_bin & mask_amp].copy()\n",
    "    if df.empty:\n",
    "        print(\"No data for this bin.\")\n",
    "        return None, None, None\n",
    "    featB = [\"star_m\", \"inn_p\", \"inn_m\", \"inn_ecc\", \"inn_inc\", \"inn_omega\", \"AmpP1\", \"DomP1\"]\n",
    "    targB = [\"out_m\", \"out_p\"]\n",
    "    df.dropna(subset=(featB + targB), inplace=True)\n",
    "    #if period_bin in (\"p3\", \"p4\"):\n",
    "    #    df = df[df[\"DomP1\"] < 10].copy()\n",
    "    if df.empty or len(df) < 5:\n",
    "        print(\"Not enough data after dropping NA for this bin.\")\n",
    "        return None, None, None\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(df[featB], df[targB],\n",
    "                                                test_size=0.2, random_state=42)\n",
    "    scB = StandardScaler()\n",
    "    X_tr_sc = scB.fit_transform(X_tr)\n",
    "    X_val_sc = scB.transform(X_val)\n",
    "    rfB = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rfB.fit(X_tr_sc, y_tr)\n",
    "    y_pred_val = rfB.predict(X_val_sc)\n",
    "    plot_predicted_vs_actual(y_val.values, y_pred_val,\n",
    "                             f\"Model B ({period_bin}, {ecc_bin})\", targB)\n",
    "    print(f\"Number of data points in this bin after final filtering: {len(df)}\")\n",
    "\n",
    "    mae_vals = []\n",
    "    rmse_vals = []\n",
    "    mafe_vals = []\n",
    "    r2_vals = []\n",
    "    for i, col_ in enumerate(targB):\n",
    "        mae_ = mean_absolute_error(y_val[col_], y_pred_val[:, i])\n",
    "        r2_  = r2_score(y_val[col_], y_pred_val[:, i])\n",
    "        rmse_ = np.sqrt(mean_squared_error(y_val[col_], y_pred_val[:, i]))\n",
    "        mafe_ = np.mean(np.abs((y_val[col_] - y_pred_val[:, i]) / (y_val[col_] + 1e-8)))\n",
    "        print(f\"ModelB({period_bin}, {ecc_bin}) - {col_}: MAE={mae_:.4f}, RMSE={rmse_:.4f}, MAFE={mafe_:.4f}, R2={r2_:.4f}\")\n",
    "        mae_vals.append(mae_)\n",
    "        rmse_vals.append(rmse_)\n",
    "        mafe_vals.append(mafe_)\n",
    "        r2_vals.append(r2_)\n",
    "    error_metrics_B = {\"MAE\": np.mean(mae_vals),\n",
    "                       \"RMSE\": np.mean(rmse_vals),\n",
    "                       \"MAFE\": np.mean(mafe_vals),\n",
    "                       \"R2\": np.mean(r2_vals)}\n",
    "    return rfB, scB, error_metrics_B\n",
    "\n",
    "###############################################################################\n",
    "# Build ModelA for a bin\n",
    "###############################################################################\n",
    "def build_modelA_for_bin(df_all, period_bin, ecc_bin):\n",
    "    print(f\"\\n=== build_modelA_for_bin => period='{period_bin}', eccentricity='{ecc_bin}' ===\")\n",
    "    featA = [\"star_m\", \"inn_p\", \"inn_m\", \"inn_ecc\", \"inn_inc\", \"inn_omega\",\n",
    "             \"out_p\", \"out_m\", \"out_ecc\", \"out_inc\", \"out_omega\"]\n",
    "    targA = [\"AmpP1\", \"DomP1\"]\n",
    "    mask_bin = get_bin_mask(df_all, period_bin, ecc_bin)\n",
    "    mask_amp = df_all[\"AmpP1\"] #<= 100\n",
    "    \n",
    "    df = df_all[mask_bin & mask_amp].copy()\n",
    "    \n",
    "\n",
    "    #if period_bin in (\"p3\", \"p4\"):\n",
    "    #    df = df[df[\"DomP1\"] < 10].copy()\n",
    "        \n",
    "    if df.empty or len(df) < 5:\n",
    "        print(\"Not enough data for this bin.\")\n",
    "        return None, None, None\n",
    "    df.dropna(subset=(featA + targA), inplace=True)\n",
    "    if df.empty:\n",
    "        print(\"Not enough data after dropping NA for this bin.\")\n",
    "        return None, None, None\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(df[featA], df[targA],\n",
    "                                                test_size=0.2, random_state=42)\n",
    "    scA = StandardScaler()\n",
    "    X_tr_sc = scA.fit_transform(X_tr)\n",
    "\n",
    "    # \u2014\u2014 new: prevent divide-by-zero on constant features \u2014\u2014 \n",
    "    # any scale_ that\u2019s zero becomes 1.0\n",
    "    zero_scale = scA.scale_ == 0\n",
    "    if zero_scale.any():\n",
    "        scA.scale_[zero_scale] = 1.0\n",
    "\n",
    "    X_val_sc = scA.transform(X_val)\n",
    "    rfA = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    rfA.fit(X_tr_sc, y_tr)\n",
    "\n",
    "    y_pred_val = rfA.predict(X_val_sc)\n",
    "    plot_predicted_vs_actual(y_val.values, y_pred_val,\n",
    "                             f\"Model A ({period_bin}, {ecc_bin})\", targA)\n",
    "    print(f\"Number of data points in this bin after final filtering: {len(df)}\")\n",
    "\n",
    "\n",
    "    mae_vals = []\n",
    "    rmse_vals = []\n",
    "    mafe_vals = []\n",
    "    r2_vals = []\n",
    "    for i, col_ in enumerate(targA):\n",
    "        mae_ = mean_absolute_error(y_val[col_], y_pred_val[:, i])\n",
    "        r2_  = r2_score(y_val[col_], y_pred_val[:, i])\n",
    "        rmse_ = np.sqrt(mean_squared_error(y_val[col_], y_pred_val[:, i]))\n",
    "        mafe_ = np.mean(np.abs((y_val[col_] - y_pred_val[:, i]) / (y_val[col_] + 1e-8)))\n",
    "        print(f\"ModelA({period_bin}, {ecc_bin}) - {col_}: MAE={mae_:.4f}, RMSE={rmse_:.4f}, MAFE={mafe_:.4f}, R2={r2_:.4f}\")\n",
    "        mae_vals.append(mae_)\n",
    "        rmse_vals.append(rmse_)\n",
    "        mafe_vals.append(mafe_)\n",
    "        r2_vals.append(r2_)\n",
    "    error_metrics_A = {\"MAE\": np.mean(mae_vals),\n",
    "                       \"RMSE\": np.mean(rmse_vals),\n",
    "                       \"MAFE\": np.mean(mafe_vals),\n",
    "                       \"R2\": np.mean(r2_vals)}\n",
    "    return rfA, scA, error_metrics_A\n",
    "\n",
    "###############################################################################\n",
    "# LOG-PROB for MCMC\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC Sampling Routines\n",
    "Here we collect routines that execute the MCMC sampler itself, drawing from the posterior using priors informed by the machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_3params_layer1(theta, bin_name, pf, modelA, scA, actual_amp, actual_freq):\n",
    "    newD = copy.deepcopy(pf)\n",
    "    newD['out_p'], newD['out_ecc'], newD['out_m'] = theta\n",
    "\n",
    "    # Period prior\n",
    "    if newD['out_p'] < pf['period_prior_min'] or newD['out_p'] > pf['period_prior_max']:\n",
    "        return -np.inf\n",
    "    # Ratio in correct bin\n",
    "    ratio = newD['out_p'] / newD['inn_p']\n",
    "    if ratio < pf['period_bin_min'] or ratio > pf['period_bin_max']:\n",
    "        return -np.inf\n",
    "    # Eccentricity prior\n",
    "    if newD['out_ecc'] > 0:\n",
    "        return -np.inf\n",
    "    if bin_name == 'e1' and newD['out_ecc'] >= -2:\n",
    "        return -np.inf\n",
    "    if bin_name == 'e2' and (newD['out_ecc'] < -2 or newD['out_ecc'] > -1.5228):\n",
    "        return -np.inf\n",
    "    if bin_name == 'e3' and (newD['out_ecc'] <= -1.5228 or newD['out_ecc'] > np.log10(0.08)):\n",
    "        return -np.inf\n",
    "    # Mass prior\n",
    "    if newD['out_m'] < pf['mass_prior_min'] or newD['out_m'] > pf['mass_prior_max']:\n",
    "        return -np.inf\n",
    "\n",
    "    # Likelihood via Model A (SSE)\n",
    "    feats = [\"star_m\",\"inn_p\",\"inn_m\",\"inn_ecc\",\"inn_inc\",\"inn_omega\",\n",
    "             \"out_p\",\"out_m\",\"out_ecc\",\"out_inc\",\"out_omega\"]\n",
    "    dfA = pd.DataFrame([[newD[f] for f in feats]], columns=feats)\n",
    "    try:\n",
    "        X_scA = scA.transform(dfA)\n",
    "        pred = modelA.predict(X_scA)[0]\n",
    "    except:\n",
    "        return -np.inf\n",
    "\n",
    "    da = pred[0] - actual_amp\n",
    "    dfv = pred[1] - actual_freq\n",
    "    return -0.5 * (da*da + dfv*dfv)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# MCMC function with per-cluster Gelman-Rubin convergence diagnostics and\n",
    "# computation of an evidence estimate (lnZ) for Bayes factor comparisons.\n",
    "###############################################################################\n",
    "def _log_prob_wrapper(theta, bin_name, pf, modelA, scA, actual_amp, actual_freq):\n",
    "    return log_prob_3params_layer1(\n",
    "        theta, bin_name, pf, modelA, scA, actual_amp, actual_freq\n",
    "    )\n",
    "\n",
    "def do_mcmc_3param_layer1(period_bin, bin_name, old_param,\n",
    "                          start_p, p_std, start_m, start_e,\n",
    "                          modelA, scA,\n",
    "                          actual_amp, actual_freq,\n",
    "                          nsteps=400, nwalkers=30):\n",
    "    # Set priors\n",
    "    pf=copy.deepcopy(old_param)\n",
    "    bins={'p1':(1.9,1.99),'p2':(2.01,2.1),'p3':(1.4,1.49),'p4':(1.51,1.6),'p5':(2.2,2.5)}\n",
    "    pf['period_bin_min'],pf['period_bin_max']=bins.get(period_bin,(2.5,2.8))\n",
    "    pf['period_prior_min'],pf['period_prior_max']=start_p-3*p_std,start_p+3*p_std\n",
    "    pf['mass_prior_min'],pf['mass_prior_max']=max(start_m/3,0),min(3*start_m,200)\n",
    "    # init walkers\n",
    "    ndim=3; nc=int(np.ceil(nwalkers*0.7)); nu=nwalkers-nc\n",
    "    lp, hp=old_param['inn_p']*pf['period_bin_min'],old_param['inn_p']*pf['period_bin_max']\n",
    "    \n",
    "    if bin_name=='e1': le,he=-9,-2.001\n",
    "    elif bin_name=='e2': le,he=-2,-1.5228\n",
    "    else: le,he=-1.5228,np.log10(0.08)\n",
    "        # \u2500\u2500\u2500 after \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # cluster walkers near the guess\n",
    "    p_c = np.clip(start_p + 0.1*np.random.randn(nc), lp, hp)\n",
    "    e_c = np.clip(start_e + 0.1*np.random.randn(nc), le, he)\n",
    "\n",
    "    # include exactly two walkers at the extremes\n",
    "    if nu >= 2:\n",
    "        p_u = np.concatenate([[lp, hp],\n",
    "                              np.random.uniform(lp, hp, nu-2)])\n",
    "        e_u = np.concatenate([[le, he],\n",
    "                              np.random.uniform(le, he, nu-2)])\n",
    "    else:\n",
    "        p_u = np.random.uniform(lp, hp, nu)\n",
    "        e_u = np.random.uniform(le, he, nu)\n",
    "\n",
    "    # mix and shuffle\n",
    "    p0 = np.concatenate([p_c, p_u]); np.random.shuffle(p0)\n",
    "    e0 = np.concatenate([e_c, e_u]); np.random.shuffle(e0)\n",
    "\n",
    "    # leave mass initialization as before (or do same trick if you like)\n",
    "    m_c = np.clip(start_m + 4*(np.random.rand(nc)-0.5),\n",
    "                  pf['mass_prior_min'], pf['mass_prior_max'])\n",
    "    m_u = np.random.uniform(pf['mass_prior_min'],\n",
    "                            pf['mass_prior_max'], nu)\n",
    "    m0  = np.concatenate([m_c, m_u]); np.random.shuffle(m0)\n",
    "\n",
    "    pos0 = np.column_stack([p0, e0, m0])\n",
    "    # \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "    # run MCMC\n",
    "    init_mcmc_globals(bin_name,pf,modelA,scA,actual_amp,actual_freq)\n",
    "    pool=multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "    pmin = start_p - 3 * p_std\n",
    "    pmax = start_p + 3 * p_std\n",
    "    #move = ClippedStretchMove(a=2, min_p=pmin, max_p=pmax)\n",
    "    \n",
    "   # sampler = emcee.EnsembleSampler(nwalkers, ndim, _log_prob_fast,\n",
    "   #                                 moves=move,\n",
    "   #                                 pool=pool)\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, _log_prob_fast, a=2, pool=pool)\n",
    "\n",
    "    sampler.run_mcmc(pos0,nsteps,progress=True)\n",
    "    \n",
    "    from emcee.autocorr import integrated_time, AutocorrError\n",
    "\n",
    "    # 1) set burn-in and thinning\n",
    "    burn = nsteps // 4\n",
    "    thin = 1\n",
    "    \n",
    "    # 2) grab the post-burn chain\n",
    "    chain_array = sampler.get_chain(discard=burn, thin=thin)  \n",
    "    #    shape = (n_steps_post, n_walkers, ndim)\n",
    "    \n",
    "    # 3) jitter zero-variance dims to avoid divide-by-zero\n",
    "    variances = np.var(chain_array, axis=(0,1))\n",
    "    for dim in np.where(variances < 1e-8)[0]:\n",
    "        chain_array[:,:,dim] += np.random.normal(0, 1e-6,\n",
    "                                                  size=chain_array[:,:,dim].shape)\n",
    "    \n",
    "    # 4) compute autocorr time with fallback\n",
    "    try:\n",
    "        tau = integrated_time(chain_array, quiet=True)\n",
    "    except AutocorrError:\n",
    "        tau = np.ones(chain_array.shape[2])  # fallback to \u03c4=1\n",
    "    \n",
    "    print(\"Autocorrelation times (\u03c4) for [out_p, log10(e), out_m]:\", tau)\n",
    "    \n",
    "    # 5) effective sample size and convergence\n",
    "    n_steps_post = chain_array.shape[0]\n",
    "    n_walkers   = chain_array.shape[1]\n",
    "    neff = n_steps_post * n_walkers / tau\n",
    "    print(\"Approximate effective sample size:\", neff)\n",
    "    conv = np.all(n_steps_post >= 50 * tau)\n",
    "    print(\"Converged (50\u03c4 per walker)?\", conv)\n",
    "        \n",
    "    pool.close(); pool.join()\n",
    "    # trace\n",
    "    chain_all=sampler.get_chain()\n",
    "    cp=chain_all.copy(); cp[:,:,1]=10**cp[:,:,1]\n",
    "    fig,ax=plt.subplots(ndim,1,sharex=True,figsize=(6,6))\n",
    "    labs=['out_p','out_e','out_m']\n",
    "    for i in range(ndim): ax[i].plot(cp[:,:,i],alpha=0.5); ax[i].set_ylabel(labs[i])\n",
    "    ax[-1].set_xlabel('Step'); plt.tight_layout(); plt.show()\n",
    "    # post burn-in\n",
    "    chp=sampler.get_chain(discard=nsteps//3,thin=2)\n",
    "    flat=chp.reshape(-1,ndim); lin_flat=flat.copy(); lin_flat[:,1]=10**lin_flat[:,1]\n",
    "    # clustering\n",
    "    # ---- after: no normalization ----\n",
    "    idx_sub = np.random.choice(len(lin_flat),\n",
    "                                size=min(3000, len(lin_flat)),\n",
    "                                replace=False)\n",
    "    lin_sub = lin_flat[idx_sub]\n",
    "    raw_clusters, hdb = cluster_hdbscan(lin_sub, min_cluster_size=500)\n",
    "\n",
    "\n",
    "    # map only the *actual* HDBSCAN clusters back to full\u2010chain indices\n",
    "    clusters = []\n",
    "    for rc in raw_clusters:\n",
    "        clusters.append({\n",
    "            \"mean\":      rc[\"mean\"],\n",
    "            \"std\":       rc[\"std\"],\n",
    "            \"indices\":   idx_sub[rc[\"indices\"]],   # safe: rc[\"indices\"] < len(idx_sub)\n",
    "            \"n_samples\": rc[\"n_samples\"]\n",
    "        })\n",
    "\n",
    "    # if HDBSCAN found nothing, fall back to the entire chain\n",
    "    if not clusters:\n",
    "        clusters = [{\n",
    "            \"mean\":      lin_flat.mean(0),\n",
    "            \"std\":       lin_flat.std(0),\n",
    "            \"indices\":   np.arange(len(lin_flat)),  # full array\n",
    "            \"n_samples\": len(lin_flat)\n",
    "        }]\n",
    "\n",
    "    print(\"Cluster labels:\", np.unique(hdb.labels_))\n",
    "    print(\"Outlier scores:\", hdb.outlier_scores_[:10])\n",
    "    \n",
    "    feats = [\n",
    "    'star_m','inn_p','inn_m','inn_ecc','inn_inc','inn_omega',\n",
    "    'out_p','out_m','out_ecc','out_inc','out_omega'\n",
    "    ]\n",
    "\n",
    "    solutions=[]\n",
    "\n",
    "    for c in clusters:\n",
    "        mean_lin=c['mean']; std_lin=c['std']; idx=c['indices']\n",
    "        p_m, p_s = mean_lin[0],std_lin[0]\n",
    "        m_m, m_s = mean_lin[2],std_lin[2]\n",
    "        e_lin, e_s_lin = mean_lin[1],std_lin[1]\n",
    "        e_m=np.log10(e_lin) if e_lin>1e-12 else -9; e_s=e_s_lin/(e_lin*np.log(10)) if e_lin>0 else 0\n",
    "        # SSE and TTV preds\n",
    "        ttv=[]\n",
    "        # use the original 'flat' chain (which is already in log\u2010ecc) instead of lin_flat\n",
    "\n",
    "        d = copy.deepcopy(pf)\n",
    "        d[\"out_p\"], d[\"out_ecc\"], d[\"out_m\"] = c[\"mean\"]\n",
    "        Xm = pd.DataFrame([[d[f] for f in feats]], columns=feats)\n",
    "        ttv_mean = modelA.predict(scA.transform(Xm))[0]\n",
    "        ttv = np.array([ttv_mean])\n",
    "\n",
    "        ttv=np.array(ttv); ap_m,ap_s=ttv[:,0].mean(),ttv[:,0].std(); dp_m,dp_s=ttv[:,1].mean(),ttv[:,1].std()\n",
    "        solutions.append({'p_mean':(p_m,p_s),'mass_mean':(m_m,m_s),'e_mean':(e_m,e_s),'ttvAmp':(ap_m,ap_s),'ttvDomP':(dp_m,dp_s)})\n",
    "    # plots\n",
    "    \n",
    "   # Get post burn-in chain in 3D: shape (n_steps_post, n_walkers, ndim)\n",
    "    chain_post = sampler.get_chain(discard=nsteps//3, thin=2)  # shape (n_steps_post, n_walkers, ndim)\n",
    "    n_walkers = chain_post.shape[1]\n",
    "    flat_full = chain_post.reshape(-1, ndim)                  # true full flattened samples\n",
    "    chain_flat = flat_full\n",
    "\n",
    "    # you already selected a subset idx_sub for HDBSCAN:\n",
    "    #   lin_sub = flat_full[idx_sub]; clusters = cluster_hdbscan(...)\n",
    "    # now map each cluster\u2019s local indices back to flat_full\n",
    "   \n",
    "\n",
    "    print(\"\\n=== DBSCAN clustering results ===\")\n",
    "    for iC, c_ in enumerate(clusters):\n",
    "        print(f\"Cluster {iC}: {c_['n_samples']} samples\")\n",
    "        # pass the full 3D chain_post into the diagnostic\n",
    "        rhat_cluster = gelman_rubin_cluster(chain_post, c_[\"indices\"], n_walkers)\n",
    "        if rhat_cluster is not None:\n",
    "            print(f\"  Cluster {iC} Gelman-Rubin R\u0302: {rhat_cluster}\")\n",
    "        else:\n",
    "            print(f\"  Cluster {iC}: Not enough samples per walker to compute R\u0302 reliably.\")\n",
    "    if not clusters:\n",
    "        print(\"No clusters => fallback to overall chain mean.\")\n",
    "        # use the full 3D post\u2013burn-in chain for R\u0302\n",
    "        #chain_post = sampler.get_chain(discard=nsteps//2, thin=2)  # (n_steps_post, n_walkers, ndim)\n",
    "        rhat_all = gelman_rubin(chain_post)\n",
    "        print(f\"Overall chain Gelman-Rubin R\u0302: {rhat_all}\")\n",
    "        # now still summarize your flat array for the \u201csingle solution\u201d fallback\n",
    "        chain_flat = chain_post.reshape(-1, ndim).copy()\n",
    "        mean_ = np.mean(chain_flat, axis=0)\n",
    "        std_  = np.std(chain_flat, axis=0)\n",
    "        clusters = [{\"mean\": mean_, \"std\": std_, \"n_samples\": len(chain_flat),\n",
    "                     \"indices\": np.arange(len(chain_flat))}]\n",
    "    \n",
    "    solutions = []\n",
    "    for c_ in clusters:\n",
    "        p_mean = c_[\"mean\"][0]\n",
    "        p_std_ = c_[\"std\"][0]\n",
    "        m_mean = c_[\"mean\"][2]\n",
    "        m_std_ = c_[\"std\"][2]\n",
    "        mean_lin_e = c_[\"mean\"][1]\n",
    "        std_lin_e  = c_[\"std\"][1]\n",
    "        if mean_lin_e<=1e-12:\n",
    "            mean_log_e = -9.0\n",
    "            std_log_e  = 0.0\n",
    "        else:\n",
    "            mean_log_e = np.log10(mean_lin_e)\n",
    "            std_log_e  = std_lin_e / (mean_lin_e*np.log(10)) if mean_lin_e>0 else 0.0\n",
    "\n",
    "        # TTV predictions for each sample in cluster\n",
    "        indices_ = c_[\"indices\"]\n",
    "        cluster_samples_log = chain_flat[indices_]\n",
    "        ttv_preds = []\n",
    "        for samp_ in cluster_samples_log:\n",
    "            newD_ = copy.deepcopy(pf)\n",
    "            newD_[\"out_p\"]   = samp_[0]\n",
    "            newD_[\"out_ecc\"] = samp_[1]\n",
    "            newD_[\"out_m\"]   = samp_[2]\n",
    "            featA = [\"star_m\",\"inn_p\",\"inn_m\",\"inn_ecc\",\"inn_inc\",\"inn_omega\",\n",
    "                     \"out_p\",\"out_m\",\"out_ecc\",\"out_inc\",\"out_omega\"]\n",
    "            rowA = [[ newD_[\"star_m\"], newD_[\"inn_p\"], newD_[\"inn_m\"], newD_[\"inn_ecc\"],\n",
    "                      newD_[\"inn_inc\"], newD_[\"inn_omega\"],\n",
    "                      newD_[\"out_p\"], newD_[\"out_m\"], newD_[\"out_ecc\"],\n",
    "                      newD_[\"out_inc\"], newD_[\"out_omega\"] ]]\n",
    "            X_dfA = pd.DataFrame(rowA, columns=featA)\n",
    "            X_scA = scA.transform(X_dfA)\n",
    "            pred_ = modelA.predict(X_scA)[0]\n",
    "            ttv_preds.append(pred_)\n",
    "        ttv_preds = np.array(ttv_preds)\n",
    "        predAmp_mean = np.mean(ttv_preds[:,0])\n",
    "        predAmp_std  = np.std(ttv_preds[:,0])\n",
    "        predDomP_mean = np.mean(ttv_preds[:,1])\n",
    "        predDomP_std = np.std(ttv_preds[:,1])\n",
    "\n",
    "        newD_final = copy.deepcopy(pf)\n",
    "        newD_final[\"out_p\"]   = p_mean\n",
    "        newD_final[\"out_ecc\"] = mean_log_e\n",
    "        newD_final[\"out_m\"]   = m_mean\n",
    "        newD_final[\"predAmp\"] = predAmp_mean\n",
    "        newD_final[\"predDomP\"] = predDomP_mean\n",
    "\n",
    "        # Compute SSE\n",
    "        featA = [\"star_m\",\"inn_p\",\"inn_m\",\"inn_ecc\",\"inn_inc\",\"inn_omega\",\n",
    "                 \"out_p\",\"out_m\",\"out_ecc\",\"out_inc\",\"out_omega\"]\n",
    "        rowA_ = [[ newD_final[\"star_m\"], newD_final[\"inn_p\"], newD_final[\"inn_m\"], newD_final[\"inn_ecc\"],\n",
    "                   newD_final[\"inn_inc\"], newD_final[\"inn_omega\"],\n",
    "                   newD_final[\"out_p\"], newD_final[\"out_m\"], newD_final[\"out_ecc\"],\n",
    "                   newD_final[\"out_inc\"], newD_final[\"out_omega\"] ]]\n",
    "        X_dfA_ = pd.DataFrame(rowA_, columns=featA)\n",
    "        X_scA_ = scA.transform(X_dfA_)\n",
    "        pred__ = modelA.predict(X_scA_)[0]\n",
    "        da_ = pred__[0] - actual_amp\n",
    "        df_ = pred__[1] - actual_freq\n",
    "        sse_ = da_**2 + df_**2\n",
    "\n",
    "        solutions.append({\"final_param_dict\": newD_final,\n",
    "                          \"p_mean\": (p_mean, p_std_),\n",
    "                          \"e_mean\": (mean_log_e, std_log_e),\n",
    "                          \"mass_mean\": (m_mean, m_std_),\n",
    "                          \"ttvAmp\": (predAmp_mean, predAmp_std),\n",
    "                          \"ttvDomP\": (predDomP_mean, predDomP_std),\n",
    "                          \"SSE\": sse_})\n",
    "    print(\"\\nConvergence solutions for this bin combo:\")\n",
    "    for iSol, sol_ in enumerate(solutions):\n",
    "        fd = sol_[\"final_param_dict\"]\n",
    "        p_val, p_err = sol_[\"p_mean\"]\n",
    "        m_val, m_err = sol_[\"mass_mean\"]\n",
    "        e_log, e_err = sol_[\"e_mean\"]\n",
    "        e_val = 10**(e_log)\n",
    "        e_err_lin = 10**(e_log + e_err) - 10**(e_log) if e_val>1e-12 else 0.0\n",
    "        ttv_amp, ttv_amp_err = sol_[\"ttvAmp\"]\n",
    "        ttv_domP, ttv_domP_err = sol_[\"ttvDomP\"]\n",
    "        sse_ = sol_[\"SSE\"]\n",
    "        print(f\"  Solution {iSol}:\")\n",
    "        print(f\"    Outer Period = {p_val:.6f} \u00b1 {p_err:.6f}\")\n",
    "        print(f\"    Outer Mass   = {m_val:.6f} \u00b1 {m_err:.6f}\")\n",
    "        print(f\"    Outer Ecc (linear) = {e_val:.6f} \u00b1 {e_err_lin:.6f}\")\n",
    "        print(f\"    Predicted TTV Amp   = {ttv_amp:.6f} \u00b1 {ttv_amp_err:.6f}\")\n",
    "        print(f\"    Predicted TTV DomP  = {ttv_domP:.6f} \u00b1 {ttv_domP_err:.6f}\")\n",
    "        print(f\"    SSE = {sse_:.6f}\\n\")\n",
    "\n",
    "    # --- NEW: Compute Bayesian evidence using a harmonic mean estimator ---\n",
    "    log_prob_post = sampler.get_log_prob(discard=nsteps//3, thin=2, flat=True)\n",
    "    log_prob_post = log_prob_post[np.isfinite(log_prob_post)]\n",
    "    if len(log_prob_post) > 0:\n",
    "        L_values = np.exp(log_prob_post)\n",
    "        Z_est = 1.0 / np.mean(1.0 / L_values)\n",
    "        lnZ = np.log(Z_est)\n",
    "    else:\n",
    "        lnZ = -np.inf\n",
    "    print(f\"Estimated log evidence (lnZ) = {lnZ:.6f}\")\n",
    "    # --- END NEW ---\n",
    "\n",
    "    # Corner plots for visualization (unchanged)\n",
    "    # Labels for corner plot\n",
    "    labels_3 = [\"out_p\", \"out_e\", \"out_m\"]\n",
    "    \n",
    "    # Call the function to plot chain_flat_post and scatter-based corner plots\n",
    "    #plot_convergence_corner(chain_flat, labels_3)\n",
    "\n",
    "    \n",
    "\n",
    "    p_min, p_max = np.min(chain_flat[:,0]), np.max(chain_flat[:,0])\n",
    "    m_min, m_max = np.min(chain_flat[:,2]), np.max(chain_flat[:,2])\n",
    "    pad_p = 0.1*(p_max-p_min)\n",
    "    pad_m = 0.1*(m_max-m_min)\n",
    "    full_range = [(p_min-pad_p, p_max+pad_p),\n",
    "                  (np.min(10**(chain_flat[:,1])), np.max(10**(chain_flat[:,1]))),\n",
    "                  (m_min-pad_m, m_max+pad_m)]\n",
    "    chain_flat_for_corner = chain_flat.copy()\n",
    "    chain_flat_for_corner[:,1] = 10**(chain_flat_for_corner[:,1])\n",
    "    labels_3 = [\"out_p\",\"out_e\",\"out_m\"]\n",
    "    n_plot = min(300, len(chain_flat_for_corner))\n",
    "    idx    = np.random.choice(len(chain_flat_for_corner), size=n_plot, replace=False)\n",
    "    small_all = chain_flat_for_corner[idx]\n",
    "    mass_vals = small_all[:, 2]  \n",
    "    counts, edges = np.histogram(mass_vals, bins=100)\n",
    "    i_mode = np.argmax(counts)\n",
    "    mode_mass = 0.5 * (edges[i_mode] + edges[i_mode+1])\n",
    "\n",
    "    fig_corner_all = corner.corner(small_all,\n",
    "                                   labels=labels_3,\n",
    "                                   #truths=[None, None, mode_mass],\n",
    "                                   #truth_kwargs={'linewidths': 0},\n",
    "                                   bins=100,\n",
    "                                   range=full_range,\n",
    "                                   color='red',\n",
    "                                   show_titles=True,\n",
    "                                   title_fmt=\".3f\")\n",
    "    plt.title(\"Corner Plot: All Samples\")\n",
    "    plt.show()\n",
    "\n",
    "    fig_heat = corner.corner(chain_flat_for_corner,\n",
    "                             labels=labels_3,\n",
    "                             #truths=[None, None, mode_mass],\n",
    "                             #truth_kwargs={'linewidths': 0},\n",
    "                             bins=100,\n",
    "                             range=full_range,\n",
    "                             plot_density=True,\n",
    "                             plot_datapoints=False,\n",
    "                             smooth=1,\n",
    "                             smooth1d=1,\n",
    "                             fill_contours=True,\n",
    "                             contour_kwargs={},\n",
    "                             color=None,\n",
    "                             show_titles=True,\n",
    "                             title_fmt=\".3f\")\n",
    "    #plt.title(\"Corner Plot\")\n",
    "    plt.show()\n",
    "\n",
    "    chain_flat_post = sampler.get_chain(discard=nsteps//3, thin=2, flat=True).copy()\n",
    "    idx = np.random.choice(len(chain_flat_post), size=300, replace=False)\n",
    "    chain_flat_post = chain_flat_post[idx]\n",
    "   \n",
    "\n",
    "    n_plot = min(300, len(chain_flat_post))\n",
    "    idx    = np.random.choice(len(chain_flat_post), size=n_plot, replace=False)\n",
    "    chain_flat_post = chain_flat_post[idx]\n",
    "    #log_probs_post  = log_probs_post[idx]\n",
    "    \n",
    "    labels_3 = [\"out_p\", \"out_e\", \"out_m\"]\n",
    "    chain_flat_post[:,1] = 10**(chain_flat_post[:,1])\n",
    "\n",
    "\n",
    "    #plot_percentile_corner(chain_flat_post, log_probs_post, labels_3)\n",
    "\n",
    "    n_post = min(300, len(chain_flat_post))\n",
    "    idx_post = np.random.choice(len(chain_flat_post), size=n_post, replace=False)\n",
    "    small_post = chain_flat_post[idx_post]\n",
    "\n",
    "    \n",
    "    #fig_corner_post = corner.corner(small_post,\n",
    "    #                                labels=labels_3,\n",
    "    #                                bins=50,\n",
    "    #                                range=full_range,\n",
    "    #                                color='blue',\n",
    "    #                                show_titles=True,\n",
    "   #                                 title_fmt=\".3f\")\n",
    "   # plt.title(\"Corner Plot: Post Burn-In\")\n",
    "   # plt.show()\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    # Extract log-probabilities of post burn-in samples\n",
    "    \n",
    "    \n",
    "    # Call function to generate percentile-based corner plot\n",
    "    return solutions, lnZ\n",
    "###############################################################################\n",
    "# SINGLE-LAYER pipeline\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline and Visualization Helpers\n",
    "Helper utilities are provided to run the full pipeline and generate summary plots, streamlining repeated analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_singlelayer_pipeline_for_system(sysdict, modelB_map, modelA_map):\n",
    "    baseParam = copy.deepcopy(sysdict)\n",
    "    combos_e = {}\n",
    "    period_bins = [\"p4\",\"p3\",\"p1\",\"p2\",\"p5\",\"p6\"]\n",
    "    ecc_bins = [\"e1\",\"e2\",\"e3\"]\n",
    "    for pbin in period_bins:\n",
    "        for ebin in ecc_bins:\n",
    "            (rfB, scB, errB) = modelB_map.get((pbin, ebin),(None,None,None))\n",
    "            if rfB is None:\n",
    "                if pbin==\"p1\":\n",
    "                    guess_p = baseParam[\"inn_p\"]*1.95\n",
    "                elif pbin==\"p2\":\n",
    "                    guess_p = baseParam[\"inn_p\"]*2.05\n",
    "                elif pbin==\"p3\":\n",
    "                    guess_p = baseParam[\"inn_p\"]*1.45\n",
    "                elif pbin==\"p4\":\n",
    "                    guess_p = baseParam[\"inn_p\"]*1.44\n",
    "                elif pbin==\"p5\":\n",
    "                    guess_p = baseParam[\"inn_p\"]*2.35\n",
    "                else:\n",
    "                    guess_p = baseParam[\"inn_p\"]*2.65\n",
    "                guess_m = 10.0\n",
    "            else:\n",
    "                featB = [\"star_m\",\"inn_p\",\"inn_m\",\"inn_ecc\",\"inn_inc\",\"inn_omega\",\"AmpP1\",\"DomP1\"]\n",
    "                rowB = [[ baseParam[\"star_m\"], baseParam[\"inn_p\"], baseParam[\"inn_m\"],\n",
    "                          baseParam[\"inn_ecc\"], baseParam[\"inn_inc\"], baseParam[\"inn_omega\"],\n",
    "                          baseParam[\"AmpP1\"], baseParam[\"DomP1\"] ]]\n",
    "                X_dfB = pd.DataFrame(rowB, columns=featB)\n",
    "                X_scB = scB.transform(X_dfB)\n",
    "                outB_ = rfB.predict(X_scB)[0]\n",
    "                guess_m = outB_[0]\n",
    "                guess_p = outB_[1]\n",
    "            \n",
    "            # Force guess_p to be within the allowed period bin limits\n",
    "            if pbin==\"p1\":\n",
    "                allowed_low = baseParam[\"inn_p\"] * 1.9\n",
    "                allowed_high = baseParam[\"inn_p\"] * 1.99\n",
    "            elif pbin==\"p2\":\n",
    "                allowed_low = baseParam[\"inn_p\"] * 2.01\n",
    "                allowed_high = baseParam[\"inn_p\"] * 2.1\n",
    "            elif pbin==\"p3\":\n",
    "                allowed_low = baseParam[\"inn_p\"] * 1.56666666667\n",
    "                allowed_high = baseParam[\"inn_p\"] * 1.65666666667\n",
    "            elif pbin==\"p4\":\n",
    "                allowed_low = baseParam[\"inn_p\"] * 1.45\n",
    "                allowed_high = baseParam[\"inn_p\"] * 1.55\n",
    "            elif pbin==\"p5\":\n",
    "                allowed_low = baseParam[\"inn_p\"] * 2.2\n",
    "                allowed_high = baseParam[\"inn_p\"] * 2.5\n",
    "            else:\n",
    "                allowed_low = baseParam[\"inn_p\"] * 2.5\n",
    "                allowed_high = baseParam[\"inn_p\"] * 2.8\n",
    "            \n",
    "            guess_p = np.clip(guess_p, allowed_low, allowed_high)\n",
    "\n",
    "            p_std_for_mcmc = 0.05*guess_p\n",
    "            if p_std_for_mcmc<1e-6:\n",
    "                p_std_for_mcmc = max(0.5, 0.05*baseParam[\"inn_p\"])\n",
    "            if ebin==\"e1\":\n",
    "                guess_e = -2.5\n",
    "            elif ebin==\"e2\":\n",
    "                guess_e = -1.8\n",
    "            else:\n",
    "                guess_e = -1.3\n",
    "            (rfA, scA, errA) = modelA_map.get((pbin, ebin),(None,None,None))\n",
    "            sol_list, lnZ = do_mcmc_3param_layer1(period_bin=pbin, bin_name=ebin, old_param=baseParam,\n",
    "                start_p=guess_p, p_std=p_std_for_mcmc,\n",
    "                start_m=guess_m, start_e=guess_e,\n",
    "                modelA=rfA, scA=scA,\n",
    "                actual_amp=baseParam[\"AmpP1\"], actual_freq=baseParam[\"DomP1\"],\n",
    "                nsteps=20000, nwalkers=200\n",
    "                )\n",
    "            combos_e[(pbin,ebin)] = {\"solutions\": sol_list, \"lnZ\": lnZ, \"errA\": errA, \"errB\": errB}\n",
    "    combos_final=[]\n",
    "    for (pbin,ebin), results in combos_e.items():\n",
    "        combos_final.append({\"bins\":(pbin,ebin), \"solutions\": results[\"solutions\"], \"lnZ\": results[\"lnZ\"],\n",
    "                             \"errA\": results[\"errA\"], \"errB\": results[\"errB\"]})\n",
    "    return combos_final\n",
    "\n",
    "###############################################################################\n",
    "# 3D / scatter plot helpers (unchanged)\n",
    "###############################################################################\n",
    "def set_3d_axes_and_reverse_mass(ax):\n",
    "    x1,x2= ax.get_xlim3d()\n",
    "    y1,y2= ax.get_ylim3d()\n",
    "    z1,z2= ax.get_zlim3d()\n",
    "    ax.set_xlim3d(x1,x2)\n",
    "    ax.set_ylim3d(y2,y1)\n",
    "    ax.set_zlim3d(z1,z2)\n",
    "\n",
    "def reverse_mass_axis_plotly(fig):\n",
    "    fig.update_layout(scene=dict(yaxis=dict(autorange=\"reversed\")))\n",
    "\n",
    "def three_3d_diff_plots_with_mass_reversed(X,Y,Z, title_extra=\"\"):\n",
    "    fig1= plt.figure()\n",
    "    ax1= fig1.add_subplot(111, projection='3d')\n",
    "    ax1.scatter(X,Y,Z, c='b', marker='o')\n",
    "    ax1.set_xlabel(\"X\")\n",
    "    ax1.set_ylabel(\"Y (mass reversed)\")\n",
    "    ax1.set_zlabel(\"Z\")\n",
    "    ax1.set_title(f\"3D Scatter {title_extra}\")\n",
    "    plt.draw()\n",
    "    set_3d_axes_and_reverse_mass(ax1)\n",
    "    plt.show()\n",
    "    if len(X)>=3:\n",
    "        fig2= plt.figure()\n",
    "        ax2= fig2.add_subplot(111, projection='3d')\n",
    "        ax2.plot_trisurf(X,Y,Z, edgecolor='gray', linewidth=0.2, alpha=0.5)\n",
    "        ax2.set_xlabel(\"X\")\n",
    "        ax2.set_ylabel(\"Y (mass reversed)\")\n",
    "        ax2.set_zlabel(\"Z\")\n",
    "        ax2.set_title(f\"3D Wire/TriSurf {title_extra}\")\n",
    "        plt.draw()\n",
    "        set_3d_axes_and_reverse_mass(ax2)\n",
    "        plt.show()\n",
    "    fig3= px.scatter_3d(x=X, y=Y, z=Z, title=f\"Interactive 3D {title_extra}\")\n",
    "    reverse_mass_axis_plotly(fig3)\n",
    "    fig3.show()\n",
    "\n",
    "def plot_3d_actual_pred_rev(p_act, m_act, e_act, p_pred, m_pred, e_pred, sys_idx):\n",
    "    X= [p_act, p_pred]\n",
    "    Y= [m_act, m_pred]\n",
    "    Z= [e_act, e_pred]\n",
    "    figA= plt.figure()\n",
    "    axA= figA.add_subplot(111, projection='3d')\n",
    "    axA.scatter([p_act],[m_act],[e_act], c='b', s=50, label='Actual')\n",
    "    axA.scatter([p_pred],[m_pred],[e_pred], c='r', s=50, label='Predicted')\n",
    "    axA.set_xlabel(\"Period\")\n",
    "    axA.set_ylabel(\"Mass (reversed)\")\n",
    "    axA.set_zlabel(\"Ecc\")\n",
    "    axA.set_title(f\"System {sys_idx} => 3D scatter reversed mass\")\n",
    "    plt.draw()\n",
    "    set_3d_axes_and_reverse_mass(axA)\n",
    "    axA.legend()\n",
    "    plt.show()\n",
    "\n",
    "    figB= plt.figure()\n",
    "    axB= figB.add_subplot(111, projection='3d')\n",
    "    axB.plot(X,Y,Z, c='gray')\n",
    "    axB.scatter([p_act],[m_act],[e_act], c='b', s=50, label='Actual')\n",
    "    axB.scatter([p_pred],[m_pred],[e_pred], c='r', s=50, label='Predicted')\n",
    "    axB.set_xlabel(\"Period\")\n",
    "    axB.set_ylabel(\"Mass (reversed)\")\n",
    "    axB.set_zlabel(\"Ecc\")\n",
    "    axB.set_title(f\"System {sys_idx} => wire reversed mass\")\n",
    "    plt.draw()\n",
    "    set_3d_axes_and_reverse_mass(axB)\n",
    "    axB.legend()\n",
    "    plt.show()\n",
    "\n",
    "    df_plot= pd.DataFrame({\"Period\":X,\"Mass\":Y,\"Ecc\":Z,\"Type\":[\"Actual\",\"Predicted\"]})\n",
    "    figC= px.scatter_3d(df_plot, x=\"Period\", y=\"Mass\", z=\"Ecc\", color=\"Type\",\n",
    "                        symbol=\"Type\", title=f\"System {sys_idx} => interactive reversed mass\")\n",
    "    reverse_mass_axis_plotly(figC)\n",
    "    figC.show()\n",
    "\n",
    "def single_scatter(paramName, dataList):\n",
    "    if not dataList:\n",
    "        return\n",
    "    x_ = [d[0] for d in dataList]\n",
    "    y_ = [d[1] for d in dataList]\n",
    "    e_ = [d[2] for d in dataList]\n",
    "    plt.figure()\n",
    "    plt.errorbar(x_, y_, yerr=e_, fmt='o', color='k', ecolor='red', capsize=3)\n",
    "    mn = min(min(x_), min(y_))\n",
    "    mx = max(max(x_), max(y_))\n",
    "    plt.plot([mn,mx],[mn,mx],'r--')\n",
    "    plt.xlabel(f\"Actual {paramName}\")\n",
    "    plt.ylabel(f\"Pred {paramName}\")\n",
    "    plt.title(f\"{paramName} => so far: {len(dataList)} systems\")\n",
    "    plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# MAIN\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution Logic\n",
    "This cell ties all components together, demonstrating how to run the workflow on a sample planetary system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=== LOADING & PREPARING MAIN DATA ===\")\n",
    "    df_all = pd.read_csv(r\"/home/ubuntu/MCMC/ttv_dataset_multiple_params.csv\")\n",
    "\n",
    "    rename_map = {\n",
    "        \"Stellar Mass (Msun)\": \"star_m\",\n",
    "        \"Inner Period (days)\": \"inn_p\",\n",
    "        \"Inner Mass (Mearth)\": \"inn_m\",\n",
    "        \"Inner Eccentricity\": \"inn_ecc\",\n",
    "        \"Inner Inclination\": \"inn_inc\",\n",
    "        \"Inner Omega\": \"inn_omega\",\n",
    "        \"Outer Mass (Mearth)\": \"out_m\",\n",
    "        \"Outer Period (days)\": \"out_p\",\n",
    "        \"Outer Eccentricity\": \"out_ecc\",\n",
    "        \"Outer Inclination\": \"out_inc\",\n",
    "        \"Outer Omega\": \"out_omega\",\n",
    "        \"Amplitude of Dominant Period Test (P1)\": \"AmpP1\",\n",
    "        \"Dominant Period Planet 1\": \"DomP1\"\n",
    "    }\n",
    "    df_all.rename(columns=rename_map, inplace=True, errors='ignore')\n",
    "\n",
    "    # Filter out large masses, clamp ecc, log-transform\n",
    "    df_all = df_all[df_all[\"out_m\"] <= 150].copy()\n",
    "    df_all[\"inn_ecc\"] = df_all[\"inn_ecc\"].clip(lower=0, upper=1.0)\n",
    "    df_all[\"out_ecc\"] = df_all[\"out_ecc\"].clip(lower=0, upper=1.0)\n",
    "    df_all[\"inn_ecc\"] = np.log10(df_all[\"inn_ecc\"].replace(0, 1e-9))\n",
    "    df_all[\"out_ecc\"] = np.log10(df_all[\"out_ecc\"].replace(0, 1e-9))\n",
    "\n",
    "    df_all[\"ratio\"] = df_all[\"out_p\"] / df_all[\"inn_p\"]\n",
    "\n",
    "    def ratio_in_any_period_bin(r):\n",
    "        return ((1.9<=r<=1.99) or (2.01<=r<=2.1) or\n",
    "                (1.4<=r<=1.49) or (1.51<=r<=1.6) or\n",
    "                (2.2<=r<=2.5) or (2.5<r<=2.8))\n",
    "    def ecc_in_any_bin(e_log):\n",
    "        e_lin=10**(e_log)\n",
    "        return ((0.0<=e_lin<=0.01) or (0.01<e_lin<=0.03) or (0.03< e_lin<=1.0))\n",
    "    mask_ratio = df_all[\"ratio\"].apply(ratio_in_any_period_bin)\n",
    "    mask_ecc   = df_all[\"out_ecc\"].apply(ecc_in_any_bin)\n",
    "    df_filtered= df_all[mask_ratio & mask_ecc].copy()\n",
    "\n",
    "    essential_cols= [\"star_m\",\"inn_p\",\"inn_m\",\"inn_ecc\",\"inn_inc\",\"inn_omega\",\n",
    "                     \"out_m\",\"out_p\",\"out_ecc\",\"out_inc\",\"out_omega\",\"AmpP1\",\"DomP1\",\"ratio\"]\n",
    "    df_filtered.dropna(subset=essential_cols, inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    if len(df_filtered)==0:\n",
    "        print(\"No data remains after filtering => abort.\")\n",
    "        return\n",
    "\n",
    "    df_all= df_filtered\n",
    "    print(f\"Data filtered: {len(df_all)} rows remain.\\n\")\n",
    "\n",
    "    # Build the 18 models\n",
    "    df_tmp = df_all.copy()\n",
    "    period_bins = [\"p4\",\"p1\",\"p2\",\"p3\",\"p5\",\"p6\"]\n",
    "    ecc_bins    = [\"e1\",\"e2\",\"e3\"]\n",
    "    print(\"\\n=== Building ModelB for period and eccentricity bins ===\")\n",
    "    modelB_map={}\n",
    "    modelB_errors = []\n",
    "    for pb in period_bins:\n",
    "        for eb in ecc_bins:\n",
    "            rfB, scB, errB = build_modelB_for_bin(df_tmp, pb, eb)\n",
    "            modelB_map[(pb, eb)] = (rfB, scB, errB)\n",
    "            if errB is not None:\n",
    "                modelB_errors.append(errB)\n",
    "    print(\"\\n=== Building ModelA for period and eccentricity bins ===\")\n",
    "    modelA_map={}\n",
    "    modelA_errors = []\n",
    "    for pb in period_bins:\n",
    "        for eb in ecc_bins:\n",
    "            rfA, scA, errA = build_modelA_for_bin(df_tmp, pb, eb)\n",
    "            modelA_map[(pb, eb)] = (rfA, scA, errA)\n",
    "            if errA is not None:\n",
    "                modelA_errors.append(errA)\n",
    "\n",
    "    # Print overall aggregated error metrics for ModelA and ModelB (all bins)\n",
    "    if modelA_errors:\n",
    "        overall_A = {\"MAE\": np.mean([d[\"MAE\"] for d in modelA_errors]),\n",
    "                     \"RMSE\": np.mean([d[\"RMSE\"] for d in modelA_errors]),\n",
    "                     \"MAFE\": np.mean([d[\"MAFE\"] for d in modelA_errors]),\n",
    "                     \"R2\": np.mean([d[\"R2\"] for d in modelA_errors])}\n",
    "        print(\"\\nOverall aggregated errors for Model A (all bins):\")\n",
    "        print(overall_A)\n",
    "    if modelB_errors:\n",
    "        overall_B = {\"MAE\": np.mean([d[\"MAE\"] for d in modelB_errors]),\n",
    "                     \"RMSE\": np.mean([d[\"RMSE\"] for d in modelB_errors]),\n",
    "                     \"MAFE\": np.mean([d[\"MAFE\"] for d in modelB_errors]),\n",
    "                     \"R2\": np.mean([d[\"R2\"] for d in modelB_errors])}\n",
    "        print(\"\\nOverall aggregated errors for Model B (all bins):\")\n",
    "        print(overall_B)\n",
    "\n",
    "    # Now filter out bins with period p5 and p6 and compute again.\n",
    "    modelA_errors_filtered = [d for (k,d) in zip(modelA_map.keys(), modelA_errors) if k[0] not in {\"p5\",\"p6\"}]\n",
    "    modelB_errors_filtered = [d for (k,d) in zip(modelB_map.keys(), modelB_errors) if k[0] not in {\"p5\",\"p6\"}]\n",
    "    # Note: Because the errors lists were appended in the loop (in order of looping), an alternative is to loop over keys.\n",
    "    # Here we re-loop over the keys:\n",
    "    modelA_errors_filtered = []\n",
    "    for (pb, eb), (rfA, scA, errA) in modelA_map.items():\n",
    "        if pb not in {\"p5\", \"p6\"} and errA is not None:\n",
    "            modelA_errors_filtered.append(errA)\n",
    "    modelB_errors_filtered = []\n",
    "    for (pb, eb), (rfB, scB, errB) in modelB_map.items():\n",
    "        if pb not in {\"p5\", \"p6\"} and errB is not None:\n",
    "            modelB_errors_filtered.append(errB)\n",
    "\n",
    "    if modelA_errors_filtered:\n",
    "        overall_A_filtered = {\"MAE\": np.mean([d[\"MAE\"] for d in modelA_errors_filtered]),\n",
    "                              \"RMSE\": np.mean([d[\"RMSE\"] for d in modelA_errors_filtered]),\n",
    "                              \"MAFE\": np.mean([d[\"MAFE\"] for d in modelA_errors_filtered]),\n",
    "                              \"R2\": np.mean([d[\"R2\"] for d in modelA_errors_filtered])}\n",
    "        print(\"\\nOverall aggregated errors for Model A (bins excluding p5 and p6):\")\n",
    "        print(overall_A_filtered)\n",
    "    if modelB_errors_filtered:\n",
    "        overall_B_filtered = {\"MAE\": np.mean([d[\"MAE\"] for d in modelB_errors_filtered]),\n",
    "                              \"RMSE\": np.mean([d[\"RMSE\"] for d in modelB_errors_filtered]),\n",
    "                              \"MAFE\": np.mean([d[\"MAFE\"] for d in modelB_errors_filtered]),\n",
    "                              \"R2\": np.mean([d[\"R2\"] for d in modelB_errors_filtered])}\n",
    "        print(\"\\nOverall aggregated errors for Model B (bins excluding p5 and p6):\")\n",
    "        print(overall_B_filtered)\n",
    "\n",
    "    # Define a single system dictionary (using user-provided values)\n",
    "    user_system = {\n",
    "        \"star_m\": 0.93,\n",
    "        \"inn_m\": 3.49494,\n",
    "        \"inn_p\": 14.91095177,\n",
    "        \"inn_ecc\": np.log10(1e-9) if 0.0<=1e-12 else 0.0,\n",
    "        \"inn_inc\": 89.98,\n",
    "        \"inn_omega\": 0.0,\n",
    "        \"AmpP1\": 38.20506441,\n",
    "        \"DomP1\": 17.64122894,\n",
    "        \"out_m\": 16.4,\n",
    "        \"out_p\": 30,\n",
    "        \"out_ecc\": np.log10(1e-9) if 0.0<=1e-12 else 0.0,\n",
    "        \"out_inc\": 90,\n",
    "        \"out_omega\": 0.0\n",
    "    }\n",
    "\n",
    "    #user_system = {\n",
    "     #   \"star_m\": 0.97,\n",
    "      #  \"inn_m\": 17.3,\n",
    "       # \"inn_p\": 7.64159,\n",
    "       # \"inn_ecc\": np.log10(1e-9) if 0.0<=1e-12 else 0.0,\n",
    "       # \"inn_inc\": 87.68,\n",
    "       # \"inn_omega\": 0.0,\n",
    "       # \"AmpP1\": 8.165742477,\n",
    "       # \"DomP1\": 34.84390735,\n",
    "       # \"out_m\": 16.4,\n",
    "       # \"out_p\": 14.85888,\n",
    "       # \"out_ecc\": np.log10(1e-9) if 0.0<=1e-12 else 0.0,\n",
    "       # \"out_inc\": 88.07,\n",
    "       # \"out_omega\": 0.0\n",
    "    #}\n",
    "    #user_system[\"inn_ecc\"] = -9.0\n",
    "    #user_system[\"out_ecc\"] = -9.0\n",
    "    user_system[\"ratio\"] = user_system[\"out_p\"] / user_system[\"inn_p\"]\n",
    "\n",
    "    # Run the single-layer pipeline on the user system\n",
    "    combos_18 = run_singlelayer_pipeline_for_system(user_system, modelB_map, modelA_map)\n",
    "\n",
    "    print(\"\\n=== RESULTS for the single user-provided system ===\")\n",
    "    bayes_results = {}\n",
    "    for co in combos_18:\n",
    "        pbin, ebin = co[\"bins\"]\n",
    "        sol_list = co[\"solutions\"]\n",
    "        lnZ = co[\"lnZ\"]\n",
    "        bayes_results[(pbin, ebin)] = lnZ\n",
    "        print(f\"Bin=({pbin},{ebin}), #solutions={len(sol_list)}, lnZ = {lnZ:.6f}\")\n",
    "        for iSol, sol_ in enumerate(sol_list):\n",
    "            fd = sol_[\"final_param_dict\"]\n",
    "            p_val, p_err = sol_[\"p_mean\"]\n",
    "            m_val, m_err = sol_[\"mass_mean\"]\n",
    "            e_log, e_std = sol_[\"e_mean\"]\n",
    "            e_lin = 10**(e_log)\n",
    "            e_err_lin = 10**(e_log + e_std)- 10**(e_log) if e_lin>1e-12 else 0.0\n",
    "            a_val = fd[\"predAmp\"]\n",
    "            f_val = fd[\"predDomP\"]\n",
    "            sse_  = sol_[\"SSE\"]\n",
    "            print(f\"  solution {iSol}: out_p={p_val:.6f} \u00b1 {p_err:.6f}, \"\n",
    "                  f\"out_m={m_val:.6f} \u00b1 {m_err:.6f}, e_lin={e_lin:.6f} \u00b1 {e_err_lin:.6f}, \"\n",
    "                  f\"PredAmp={a_val:.6f}, PredDomP={f_val:.6f}, SSE={sse_:.6f}\")\n",
    "    # --- NEW: Compute and print Bayes factor differences (\u0394lnZ) ---\n",
    "    if bayes_results:\n",
    "        best_lnZ = max(bayes_results.values())\n",
    "        print(\"\\n=== Bayes Factor (\u0394lnZ) Summary ===\")\n",
    "        for bin_key, lnZ in bayes_results.items():\n",
    "            delta_lnZ = lnZ - best_lnZ\n",
    "            print(f\"Bin {bin_key}: lnZ = {lnZ:.6f}, \u0394lnZ = {delta_lnZ:.6f}\")\n",
    "    # --- END NEW ---\n",
    "\n",
    "    print(\"\\nDone. You have the MCMC solutions for the single system.\")\n",
    "    print(\"No final difference plots are produced since we are only analyzing one system.\")\n",
    "    print(\"If you need additional plots, you can adapt the code above accordingly.\")\n",
    "\n",
    "    # --- NEW: Save final MCMC solution inputs and error bars to a list ---\n",
    "    global final_solution_inputs\n",
    "    final_solution_inputs = []\n",
    "    for co in combos_18:\n",
    "        for sol in co[\"solutions\"]:\n",
    "            sol_dict = {}\n",
    "            # Outer period and error\n",
    "            sol_dict[\"outer_period\"] = sol[\"p_mean\"][0]\n",
    "            sol_dict[\"outer_period_err\"] = sol[\"p_mean\"][1]\n",
    "            # Outer mass and error\n",
    "            sol_dict[\"outer_mass\"] = sol[\"mass_mean\"][0]\n",
    "            sol_dict[\"outer_mass_err\"] = sol[\"mass_mean\"][1]\n",
    "            # Outer eccentricity (converted from log to linear) and error\n",
    "            e_log = sol[\"e_mean\"][0]\n",
    "            e_std = sol[\"e_mean\"][1]\n",
    "            sol_dict[\"outer_ecc\"] = 10**(e_log)\n",
    "            sol_dict[\"outer_ecc_err\"] = (10**(e_log + e_std) - 10**(e_log))\n",
    "            # SSE\n",
    "            sol_dict[\"SSE\"] = sol[\"SSE\"]\n",
    "            # Predicted dominant period and amplitude with errors\n",
    "            sol_dict[\"pred_dom_period\"] = sol[\"ttvDomP\"][0]\n",
    "            sol_dict[\"pred_dom_period_err\"] = sol[\"ttvDomP\"][1]\n",
    "            sol_dict[\"pred_dom_amp\"] = sol[\"ttvAmp\"][0]\n",
    "            sol_dict[\"pred_dom_amp_err\"] = sol[\"ttvAmp\"][1]\n",
    "            # True values from the system dictionary\n",
    "            sol_dict[\"trueAmp\"] = user_system[\"AmpP1\"]\n",
    "            sol_dict[\"trueDomP\"] = user_system[\"DomP1\"]\n",
    "            # Include the rest of the parameters from final_param_dict\n",
    "            sol_dict.update(sol[\"final_param_dict\"])\n",
    "            final_solution_inputs.append(sol_dict)\n",
    "    print(\"\\nFinal MCMC solution inputs and error bars:\")\n",
    "    for sol in final_solution_inputs:\n",
    "        print(sol)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run as Script\n",
    "The notebook can also be executed as a standalone script; this final section handles argument parsing and kicks off the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Overview\n",
    "The previous code cell defines helper functions for clustering, model training, and running MCMC.\n",
    "Key functions include:\n",
    "- `build_modelB_for_bin` and `build_modelA_for_bin` for training Random Forest regressors on different period and eccentricity bins.\n",
    "- `do_mcmc_3param_layer1` which performs the MCMC sampling with custom priors.\n",
    "- `run_singlelayer_pipeline_for_system` that orchestrates the entire workflow for one planetary system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccae9ae-222d-4faf-89f3-3af11d31223d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install emcee corner plotly tqdm hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the Solutions\n",
    "After the MCMC run we sort the resulting parameter sets by sum of squared errors (SSE) to inspect the best fits first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d7c85-86eb-4c4a-8d6c-25cc89eb22af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- NEW: Sort final MCMC solution inputs by SSE (smallest to largest) ---\n",
    "sorted_solution_inputs = sorted(final_solution_inputs, key=lambda x: x['SSE'])\n",
    "\n",
    "print(\"\\nSorted MCMC solutions by SSE (smallest to largest):\")\n",
    "for sol in sorted_solution_inputs:\n",
    "    # Displaying the relevant information for each solution\n",
    "    print(f\"System with SSE = {sol['SSE']:.6f}\")\n",
    "    print(f\"  Outer Period = {sol['outer_period']:.6f} \u00b1 {sol['outer_period_err']:.6f}\")\n",
    "    print(f\"  Outer Mass = {sol['outer_mass']:.6f} \u00b1 {sol['outer_mass_err']:.6f}\")\n",
    "    print(f\"  Outer Eccentricity (linear) = {sol['outer_ecc']:.6f} \u00b1 {sol['outer_ecc_err']:.6f}\")\n",
    "    print(f\"  Predicted TTV Amp = {sol['pred_dom_amp']:.6f} \u00b1 {sol['pred_dom_amp_err']:.6f}\")\n",
    "    print(f\"  Predicted TTV Dominant Period = {sol['pred_dom_period']:.6f} \u00b1 {sol['pred_dom_period_err']:.6f}\")\n",
    "    print(f\"  True TTV Amp = {sol['trueAmp']:.6f}\")\n",
    "    print(f\"  True TTV Dominant Period = {sol['trueDomP']:.6f}\")\n",
    "    print(f\"  Outer Mass (True) = {sol['out_m']:.6f}\")\n",
    "    print(f\"  Outer Period (True) = {sol['out_p']:.6f}\")\n",
    "    print(f\"  Outer Eccentricity (True) = {sol['out_ecc']:.6f}\")\n",
    "    print(f\"  Star Mass (True) = {sol['star_m']:.6f}\")\n",
    "    print(f\"  Inner Period (True) = {sol['inn_p']:.6f}\")\n",
    "    print(f\"  Inner Mass (True) = {sol['inn_m']:.6f}\")\n",
    "    print(f\"  Inner Eccentricity (True) = {sol['inn_ecc']:.6f}\")\n",
    "    print(f\"  Inner Inclination (True) = {sol['inn_inc']:.6f}\")\n",
    "    print(f\"  Inner Omega (True) = {sol['inn_omega']:.6f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by SSE\n",
    "Solutions with SSE below the 40th percentile are kept for further inspection. This helps focus on high quality fits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27a69c5-8624-4ca9-ab70-67d2b8071dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Filter solutions with SSE below the 40th percentile ---\n",
    "import numpy as np\n",
    "\n",
    "# Extract all SSE values from the final solutions list\n",
    "sse_values = [sol['SSE'] for sol in final_solution_inputs]\n",
    "# Compute the 40th percentile for SSE\n",
    "percentile_40 = np.percentile(sse_values, 40)\n",
    "\n",
    "# Create a duplicate of the final_solution_inputs list with only solutions having SSE below the 40th percentile\n",
    "filtered_solution_inputs = [sol.copy() for sol in final_solution_inputs if sol['SSE'] < percentile_40]\n",
    "\n",
    "print(\"\\nMCMC solutions with SSE below 40th percentile (SSE < {:.6f}):\".format(percentile_40))\n",
    "for sol in filtered_solution_inputs:\n",
    "    print(f\"SSE = {sol['SSE']:.6f}\")\n",
    "    print(f\"  Outer Period = {sol['outer_period']:.6f} \u00b1 {sol['outer_period_err']:.6f}\")\n",
    "    print(f\"  Outer Mass = {sol['outer_mass']:.6f} \u00b1 {sol['outer_mass_err']:.6f}\")\n",
    "    print(f\"  Outer Eccentricity (linear) = {sol['outer_ecc']:.6f} \u00b1 {sol['outer_ecc_err']:.6f}\")\n",
    "    print(f\"  Predicted TTV Amp = {sol['pred_dom_amp']:.6f} \u00b1 {sol['pred_dom_amp_err']:.6f}\")\n",
    "    print(f\"  Predicted TTV Dominant Period = {sol['pred_dom_period']:.6f} \u00b1 {sol['pred_dom_period_err']:.6f}\")\n",
    "    print(f\"  True TTV Amp = {sol['trueAmp']:.6f}\")\n",
    "    print(f\"  True TTV Dominant Period = {sol['trueDomP']:.6f}\")\n",
    "    print(f\"  Outer Mass (True) = {sol['out_m']:.6f}\")\n",
    "    print(f\"  Outer Period (True) = {sol['out_p']:.6f}\")\n",
    "    print(f\"  Outer Eccentricity (True) = {sol['out_ecc']:.6f}\")\n",
    "    print(f\"  Star Mass (True) = {sol['star_m']:.6f}\")\n",
    "    print(f\"  Inner Period (True) = {sol['inn_p']:.6f}\")\n",
    "    print(f\"  Inner Mass (True) = {sol['inn_m']:.6f}\")\n",
    "    print(f\"  Inner Eccentricity (True) = {sol['inn_ecc']:.6f}\")\n",
    "    print(f\"  Inner Inclination (True) = {sol['inn_inc']:.6f}\")\n",
    "    print(f\"  Inner Omega (True) = {sol['inn_omega']:.6f}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing TTV Periods\n",
    "The final section demonstrates how to use Lomb--Scargle periodograms to analyse the transit timing variation curves and relate them back to the orbital periods inferred from MCMC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7939c87-783d-434b-adc1-da6b51a286c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from astropy.timeseries import LombScargle\n",
    "from scipy.signal import find_peaks\n",
    "import math\n",
    "\n",
    "# ---------------------------\n",
    "# Define the inner period (exact) in days\n",
    "inner_period =8.30864937\n",
    "\n",
    "\n",
    "# --- NEW: Use final MCMC solution inputs if available ---\n",
    "try:\n",
    "    filtered_solution_inputs\n",
    "except NameError:\n",
    "    # If final_solution_inputs is not defined, use default hard-coded values.\n",
    "    outer_periods = [57.890833, 40.0, 50.0]         # Replace with your values\n",
    "    outer_period_errors = [0.890964, 0.7, 1.0]        # Replace with your uncertainties\n",
    "    print(\"Using hard-coded outer period values.\")\n",
    "else:\n",
    "    # Extract the outer period and uncertainty from each solution\n",
    "    outer_periods = [sol[\"outer_period\"] for sol in filtered_solution_inputs]\n",
    "    outer_period_errors = [sol[\"outer_period_err\"] for sol in filtered_solution_inputs]\n",
    "    print(\"Using outer period values from final_solution_inputs:\")\n",
    "    for op, err in zip(outer_periods, outer_period_errors):\n",
    "        print(f\"Outer Period = {op:.3f} \u00b1 {err:.3f} (in days)\")\n",
    "\n",
    "# Compute the synodic period and its propagated uncertainty for each outer period.\n",
    "# Formula (in days):\n",
    "#   T_syn = 1/((1/inner_period) - (1/outer_period))\n",
    "#   dT_syn/dT_outer = (1/T_outer^2) / (1/inner_period - 1/outer_period)^2\n",
    "# Then convert to epochs by dividing by inner_period.\n",
    "synodic_periods = []\n",
    "synodic_errors = []\n",
    "\n",
    "for T_outer, sigma_outer in zip(outer_periods, outer_period_errors):\n",
    "    # Compute the synodic period in days\n",
    "    denominator = (1.0 / inner_period) - (1.0 / T_outer)\n",
    "    if denominator == 0:\n",
    "        raise ValueError(\"Denominator in synodic period calculation is zero.\")\n",
    "    T_syn = 1.0 / denominator\n",
    "\n",
    "    # Propagate the error (inner_period assumed exact)\n",
    "    sigma_syn = sigma_outer / (T_outer**2 * denominator**2)\n",
    "    \n",
    "    # Convert synodic period and its uncertainty from days to epochs\n",
    "    T_syn_epochs = T_syn / inner_period\n",
    "    sigma_syn_epochs = sigma_syn / inner_period\n",
    "    \n",
    "    synodic_periods.append(T_syn_epochs)\n",
    "    synodic_errors.append(sigma_syn_epochs)\n",
    "\n",
    "    print(f\"Outer Period = {T_outer:.3f} \u00b1 {sigma_outer:.3f} days -> Synodic Period = {T_syn_epochs:.3f} \u00b1 {sigma_syn_epochs:.3f} epochs\")\n",
    "\n",
    "# ---------------------------\n",
    "# Load the Kepler-46 TTV curve from a CSV file.\n",
    "# The CSV must have columns: \"Epoch\" and \"O-C (days)\"\n",
    "ttv_df = pd.read_csv(\"F:/Kepler_1710_b.csv\")\n",
    "if not {\"Epoch\", \"O-C (days)\"}.issubset(ttv_df.columns):\n",
    "    raise ValueError(\"CSV file must contain columns 'Epoch' and 'O-C (days)'.\")\n",
    "\n",
    "# Extract the arrays from the CSV\n",
    "all_epochs = ttv_df[\"Epoch\"].values\n",
    "all_oc = ttv_df[\"O-C (days)\"].values\n",
    "\n",
    "# ----- SUBTRACTION STEP (exactly as in your provided code, but only one iteration) -----\n",
    "# Convert O\u2013C from days to minutes\n",
    "ttv_f = all_oc * 24.0 * 60.0\n",
    "epochs_f = all_epochs  # Use all data; no error filtering\n",
    "\n",
    "# Mean-center the TTV data (in minutes)\n",
    "y_data = ttv_f - np.mean(ttv_f)\n",
    "\n",
    "# Perform a single iteration of Lomb\u2013Scargle to subtract the dominant periodic signal\n",
    "ls_current = LombScargle(epochs_f, y_data)\n",
    "frequency, power = ls_current.autopower(\n",
    "    minimum_frequency=1.0/20.0,\n",
    "    maximum_frequency=0.5,\n",
    "    samples_per_peak=10\n",
    ")\n",
    "periods_ls = 1.0 / frequency  # Periods in epochs\n",
    "\n",
    "# Limit to periods less than 40 epochs\n",
    "valid = periods_ls < 100\n",
    "if not np.any(valid):\n",
    "    print(\"No valid periods found in subtraction step. Using original signal.\")\n",
    "    cleaned_signal = y_data\n",
    "else:\n",
    "    periods_valid = periods_ls[valid]\n",
    "    power_valid = power[valid]\n",
    "    idx_max = np.argmax(power_valid)\n",
    "    dom_period = periods_valid[idx_max]\n",
    "    dom_power = power_valid[idx_max]\n",
    "    fap = ls_current.false_alarm_probability(dom_power)\n",
    "    print(\"\\n--- Subtraction Step ---\")\n",
    "    print(f\"Dominant Period (epochs) = {dom_period:.3f}\")\n",
    "    print(f\"Power                  = {dom_power:.4f}\")\n",
    "    print(f\"FAP                    = {fap:.4g}\")\n",
    "    \n",
    "    # Subtract the best-fit sinusoid for the dominant period\n",
    "    model = ls_current.model(epochs_f, frequency=1.0/dom_period)\n",
    "    cleaned_signal = y_data - model\n",
    "\n",
    "# Use the full epochs and the cleaned signal (still in minutes) for further analysis.\n",
    "time_for_analysis = epochs_f\n",
    "\n",
    "# ---------------------------\n",
    "# Compute Lomb\u2013Scargle Periodogram using the cleaned signal.\n",
    "# Frequency is in cycles per epoch, so periods are in epochs.\n",
    "ls = LombScargle(time_for_analysis, cleaned_signal)\n",
    "frequency, power = ls.autopower(minimum_frequency=0.01, maximum_frequency=1.0, samples_per_peak=20)\n",
    "periods = 1.0 / frequency  # Periods in epochs\n",
    "\n",
    "# Plot the overall periodogram (for reference)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(periods, power, color='black', label=\"Lomb\u2013Scargle Power\")\n",
    "plt.xlabel(\"Period (epochs)\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.title(\"Lomb\u2013Scargle Periodogram for Kepler-46 TTV (Cleaned)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# For each computed synodic period (in epochs), plot the periodogram with markers and search for peaks.\n",
    "for idx, (syn, syn_err) in enumerate(zip(synodic_periods, synodic_errors), start=1):\n",
    "    print(f\"\\n--- Synodic Period Set {idx}: {syn:.3f} \u00b1 {syn_err:.3f} epochs ---\")\n",
    "    \n",
    "    # Plot 1: Full periodogram with synodic period marker and shaded error range\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(periods, power, color='black', label=\"Lomb\u2013Scargle Power\")\n",
    "    plt.axvline(x=syn, color='red', linestyle='--', label=f\"Synodic Period ({syn:.2f} epochs)\")\n",
    "    plt.axvspan(syn - syn_err, syn + syn_err, color='red', alpha=0.3, label=\"Synodic Error Range\")\n",
    "    plt.xlabel(\"Period (epochs)\")\n",
    "    plt.ylabel(\"Power\")\n",
    "    plt.title(f\"Full Periodogram with Synodic Marker (Set {idx})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(periods, power, color='black', label=\"Lomb\u2013Scargle Power\")\n",
    "    plt.axvline(x=syn, color='red', linestyle='--', label=f\"Synodic Period ({syn:.2f} epochs)\")\n",
    "    plt.axvspan(syn - syn_err, syn + syn_err, color='red', alpha=0.3, label=\"Synodic Error Range\")\n",
    "    plt.xlabel(\"Period (epochs)\")\n",
    "    plt.xlim(0.5 * syn, 1.5 * syn)\n",
    "    plt.ylabel(\"Power\")\n",
    "    plt.title(f\"Periodogram with Synodic Marker (Set {idx})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Search for peaks in the error range [syn - syn_err, syn + syn_err]\n",
    "    region_mask = (periods >= (syn - syn_err)) & (periods <= (syn + syn_err))\n",
    "    indices = np.where(region_mask)[0]\n",
    "    \n",
    "    if len(indices) == 0:\n",
    "        print(\"No periodogram points found in the synodic error region.\")\n",
    "        continue\n",
    "    \n",
    "    # Find peaks within the region (using scipy.signal.find_peaks)\n",
    "    region_power = power[indices]\n",
    "    peak_rel_indices, _ = find_peaks(region_power)\n",
    "    \n",
    "    if len(peak_rel_indices) == 0:\n",
    "        # If no peaks detected, use the highest power point in the region.\n",
    "        best_rel_idx = np.argmax(region_power)\n",
    "        global_idx = indices[best_rel_idx]\n",
    "        peak_period = periods[global_idx]\n",
    "        peak_power = power[global_idx]\n",
    "        fap = ls.false_alarm_probability(peak_power)\n",
    "        print(f\"No peaks detected in the region. Highest point: Period = {peak_period:.3f} epochs, Power = {peak_power:.4f}, FAP = {fap:.4g}\")\n",
    "    else:\n",
    "        # For each detected peak, compute its period, power, and FAP.\n",
    "        print(\"Detected peaks in the synodic error region:\")\n",
    "        for rel_idx in peak_rel_indices:\n",
    "            global_idx = indices[rel_idx]\n",
    "            peak_period = periods[global_idx]\n",
    "            peak_power = power[global_idx]\n",
    "            fap = ls.false_alarm_probability(peak_power)\n",
    "            print(f\"  Peak at {peak_period:.3f} epochs with Power = {peak_power:.4f} and FAP = {fap:.4g}\")\n",
    "\n",
    "# --- Compute the best peak (maximum power) in the error region for each synodic period ---\n",
    "best_peak_power_list = []\n",
    "best_fap_list = []\n",
    "\n",
    "for syn, syn_err in zip(synodic_periods, synodic_errors):\n",
    "    # Define the error region mask.\n",
    "    region_mask = (periods >= (syn - syn_err)) & (periods <= (syn + syn_err))\n",
    "    indices = np.where(region_mask)[0]\n",
    "    if len(indices) == 0:\n",
    "        best_peak_power = None\n",
    "        best_fap = None\n",
    "    else:\n",
    "        region_power = power[indices]\n",
    "        # Identify peaks within the region.\n",
    "        peak_rel_indices, _ = find_peaks(region_power)\n",
    "        if len(peak_rel_indices) == 0:\n",
    "            # No peaks detected: choose the maximum power point in the region.\n",
    "            best_idx = indices[np.argmax(region_power)]\n",
    "        else:\n",
    "            # Choose the peak with the maximum power.\n",
    "            candidate_indices = indices[peak_rel_indices]\n",
    "            best_idx = candidate_indices[np.argmax(power[candidate_indices])]\n",
    "        best_peak_power = power[best_idx]\n",
    "        best_fap = ls.false_alarm_probability(best_peak_power)\n",
    "    best_peak_power_list.append(best_peak_power)\n",
    "    best_fap_list.append(best_fap)\n",
    "\n",
    "# --- First grouped figure: All synodic period sets with Power and FAP info in a text box ---\n",
    "\n",
    "n_syn = len(synodic_periods)\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_syn / n_cols)\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))\n",
    "axs = axs.flatten()  # Flatten for simple iteration\n",
    "\n",
    "for i, (syn, syn_err, best_power, best_fap) in enumerate(zip(synodic_periods, synodic_errors, best_peak_power_list, best_fap_list)):\n",
    "    ax = axs[i]\n",
    "    # Plot the full periodogram (zoomed in).\n",
    "    ax.plot(periods, power, color='black', label=\"Lomb\u2013Scargle Power\")\n",
    "    ax.axvline(x=syn, color='red', linestyle='--', label=f\"Synodic = {syn:.2f}\")\n",
    "    ax.axvspan(syn - syn_err, syn + syn_err, color='red', alpha=0.3, label=\"Error Range\")\n",
    "    ax.set_xlabel(\"Period (epochs)\")\n",
    "    ax.set_ylabel(\"Power\")\n",
    "    ax.set_title(f\"Set {i+1}: {syn:.2f} epochs\")\n",
    "    ax.set_xlim(0.5 * syn, 1.5 * syn)\n",
    "    ax.legend(fontsize='small')\n",
    "    \n",
    "    # Create a text box with the best peak power and FAP.\n",
    "    if best_power is not None:\n",
    "        text_str = f\"Power: {best_power:.4f}\\nFAP: {best_fap:.4g}\"\n",
    "    else:\n",
    "        text_str = \"No peak found\"\n",
    "    ax.text(0.05, 0.95, text_str, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# Remove any unused subplots.\n",
    "for j in range(i+1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Second grouped figure: Only synodic period sets with FAP below 0.5 ---\n",
    "filtered_indices = [i for i, fap in enumerate(best_fap_list) if (fap is not None and fap < 0.5)]\n",
    "\n",
    "if len(filtered_indices) > 0:\n",
    "    n_filtered = len(filtered_indices)\n",
    "    n_cols_f = 3\n",
    "    n_rows_f = math.ceil(n_filtered / n_cols_f)\n",
    "    \n",
    "    fig2, axs2 = plt.subplots(n_rows_f, n_cols_f, figsize=(6 * n_cols_f, 4 * n_rows_f))\n",
    "    axs2 = axs2.flatten()\n",
    "    \n",
    "    for idx, i in enumerate(filtered_indices):\n",
    "        syn = synodic_periods[i]\n",
    "        syn_err = synodic_errors[i]\n",
    "        best_power = best_peak_power_list[i]\n",
    "        best_fap = best_fap_list[i]\n",
    "        ax = axs2[idx]\n",
    "        ax.plot(periods, power, color='black', label=\"Lomb\u2013Scargle Power\")\n",
    "        ax.axvline(x=syn, color='red', linestyle='--', label=f\"Synodic = {syn:.2f}\")\n",
    "        ax.axvspan(syn - syn_err, syn + syn_err, color='red', alpha=0.3, label=\"Error Range\")\n",
    "        ax.set_xlabel(\"Period (epochs)\")\n",
    "        ax.set_ylabel(\"Power\")\n",
    "        ax.set_title(f\"Set {i+1}: {syn:.2f} epochs\")\n",
    "        ax.set_xlim(0.5 * syn, 1.5 * syn)\n",
    "        ax.legend(fontsize='small')\n",
    "        if best_power is not None:\n",
    "            text_str = f\"Power: {best_power:.4f}\\nFAP: {best_fap:.4g}\"\n",
    "        else:\n",
    "            text_str = \"No peak found\"\n",
    "        ax.text(0.05, 0.95, text_str, transform=ax.transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # Remove any extra subplots.\n",
    "    for j in range(idx+1, len(axs2)):\n",
    "        fig2.delaxes(axs2[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No synodic period sets with FAP below 0.5.\")\n",
    "# --- Print summary parameters for all synodic period sets ---\n",
    "print(\"\\n--- Summary of Synodic Period Sets and Parameters ---\")\n",
    "for i, (T_outer, sigma_outer, syn, syn_err) in enumerate(zip(outer_periods, outer_period_errors, synodic_periods, synodic_errors), start=1):\n",
    "    # If filtered_solution_inputs is defined, attempt to extract additional parameters\n",
    "    if 'filtered_solution_inputs' in globals() and len(filtered_solution_inputs) >= i:\n",
    "        sol = filtered_solution_inputs[i-1]\n",
    "        outer_mass = sol.get(\"outer_mass\", \"N/A\")\n",
    "        outer_ecc  = sol.get(\"outer_ecc\", \"N/A\")\n",
    "        SSE = sol.get(\"SSE\", \"N/A\")\n",
    "        # You can add more parameters here if needed, e.g.:\n",
    "        # paramX = sol.get(\"paramX\", \"N/A\")\n",
    "    else:\n",
    "        outer_mass = \"N/A\"\n",
    "        outer_ecc  = \"N/A\"\n",
    "        SSE = \"N/A\"\n",
    "    print(f\"Set {i}: Outer Period = {T_outer:.3f} \u00b1 {sigma_outer:.3f} days, \"\n",
    "          f\"Synodic Period = {syn:.3f} \u00b1 {syn_err:.3f} epochs, \"\n",
    "          f\"Outer Mass = {outer_mass}, Outer Ecc = {outer_ecc}, SSE = {SSE}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# Constants for conversion\n",
    "MEARTH_TO_MSUN = 3.003e-6  # Earth mass to solar mass conversion factor\n",
    "\n",
    "# Laplace coefficient b_{1/2}^{(0)}(\u03b1)\n",
    "def laplace_b(alpha):\n",
    "    integrand = lambda psi: (1 - 2 * alpha * np.cos(psi) + alpha**2)**(-0.5)\n",
    "    result, _ = quad(integrand, 0, 2 * np.pi, limit=1000)\n",
    "    return result / np.pi\n",
    "\n",
    "# Compute f(\u03b1)\n",
    "def f_of_alpha(alpha, eps=1e-5):\n",
    "    b_plus = laplace_b(alpha + eps)\n",
    "    b_minus = laplace_b(alpha - eps)\n",
    "    d_b_d_alpha = (b_plus - b_minus) / (2 * eps)\n",
    "    b_val = laplace_b(alpha)\n",
    "    return alpha * d_b_d_alpha + 2 * alpha * b_val\n",
    "\n",
    "# Known Parameters\n",
    "inner_period = final_solution_inputs     # P_transit in days\n",
    "V_syn = 20.785 / (24*60)    # TTV amplitude in days (converted from minutes)\n",
    "\n",
    "print(\"\\nEstimated Outer Mass (perturber) from TTV amplitude for each solution:\")\n",
    "\n",
    "# Only predict outer mass for sets with FAP < 0.5.\n",
    "for i, sol in enumerate(filtered_solution_inputs):\n",
    "    if best_fap_list[i] is None or best_fap_list[i] >= 0.5:\n",
    "        print(f\"Skipping outer mass prediction for set {i+1} due to high FAP ({best_fap_list[i]:.4g}).\")\n",
    "        continue\n",
    "\n",
    "    M_star = sol[\"star_m\"]                # Stellar mass in solar masses\n",
    "    m_inner = sol[\"inn_m\"] * MEARTH_TO_MSUN  # Convert inner mass from Mearth to Msun\n",
    "    P_pert = sol[\"outer_period\"]          # Outer planet period in days\n",
    "\n",
    "    # Compute \u03b1\n",
    "    alpha = (inner_period / P_pert) ** (2.0 / 3.0)\n",
    "\n",
    "    # Calculate f(\u03b1)\n",
    "    f_val = f_of_alpha(alpha)\n",
    "\n",
    "    # Correct mass calculation formula:\n",
    "    m_pert_est = (V_syn * np.pi * (M_star + m_inner)) / (inner_period * alpha * f_val)\n",
    "\n",
    "    # Output results clearly with units\n",
    "    print(f\"SSE = {sol['SSE']:.6f}, Outer Period = {P_pert:.3f} days, TTV Amp = {V_syn*1440:.3f} min\")\n",
    "    print(f\"  \u03b1 = {alpha:.4f}, f(\u03b1) = {f_val:.4e}, Estimated Outer Mass = {m_pert_est:.6f} M_sun\")\n",
    "    print(f\"    (Equivalent to {m_pert_est/MEARTH_TO_MSUN:.2f} Earth masses)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
